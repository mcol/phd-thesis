%Started on 5 February 2007
%Mar: 31
%Apr:  1,  2, 18, 28
%May:  6,  7,  8

%
% Chapter: Conclusions
%
\label{ch:Conclusions}

In this chapter we summarise the results obtained and 
present some observations derived from the experience
gathered during this research.
Our focus here is to draw together the original aspects of this research
and present possible directions for future research.

In Chapter~\ref{ch:Ipm} we analysed the symmetric neighbourhood
of the central path, which expresses both a lower and an upper bound
on the complementarity pairs.
The analysis presented shows that the presence of an upper bound does not
adversely affect the theoretical properties of a feasible algorithm
based on this neighbourhood.
As this neighbourhood is at the heart of the successful multiple
centrality correctors technique, we believe that it properly describes
the properties of a well-centered iterate for a practical algorithm.

In Chapter~\ref{ch:Correctors} we revisited the 
technique of multiple centrality correctors \cite{Gondzio96} 
and added a new degree of freedom to it. 
Instead of computing the corrected direction from 
$\Delta w = \Delta_p w + \Delta_c w$ where 
$\Delta_p w$ and $\Delta_c w$ are the predictor and corrector terms, 
we allow a choice of weight 
$\omega \in (0,1]$ for the corrector term and compute 
$\Delta^\omega w = \Delta_p w + \omega \Delta_c w$.
We combined this modification with the use of a symmetric neighbourhood
of the central path. 

We have compared our algorithm against the recently introduced 
Krylov subspace scheme of Mehrotra and Li \cite{MehrotraLi}.
The two approaches have similarities: they look for a set of attractive 
independent terms from which the final direction is constructed. 
Mehrotra and Li's approach uses the first few elements from the basis
of the Krylov space and solves an auxiliary linear program to find an
optimal combination of all available direction terms.
The weighted correctors technique generates direction terms using 
centrality correctors of \cite{Gondzio96}, evaluates the weight
for each corrector independently, and it can detect when the use
of an additional corrector term would not be beneficial.
Eventually, after adding $k$ corrector terms, 
the directions used in our approach have form
\[
  \Delta w = \Delta_a w + \omega_1\Delta_1 w + \ldots + \omega_k\Delta_k w,
\]
and the affine-scaling term $\Delta_a$ contributes to it without any
reduction. Hence, the larger the stepsize, the more progress we make
towards the optimizer.

The extensive computational results presented for different 
classes of problems demonstrate the potential of the
weighted correctors technique, particularly for large-scale problems.
The comparison presented in Section~\ref{ML-tests} 
shows some advantage
of our scheme over that of \cite{MehrotraLi}. Indeed, with the same 
number of direction terms allowed, our scheme outperforms Krylov subspace 
correctors by a wide margin.

One of the advantages of generating correctors in a recursive way
consists in the possibility of stopping the correction phase
if the use of the corrector does not offer sufficient improvement.
However, we realise that using a small linear programming subproblem
to produce the optimal weighting of search directions can only improve
our results in terms of stepsizes and number of iterations.
It was suggested by Nick Gould to implement a mixed strategy;
by the multiple centrality correctors heuristic we can dynamically
find the number of correctors needed at each iteration; then we find
the weights by solving a subproblem.
This suggestion leads to another possibility: the scheme of
Mehrotra and Li should have a heuristic way to decide how many
Krylov vectors they want to produce, rather than always generating
a fixed number of them.

Given that the weighted correctors scheme follows the usual 
predictor--corrector framework,
we expect it to be easier to implement in other interior point software.
In particular, such a technique should be used for large-scale problems
for which the reduction in number of iterations repays the
increased cost of each iteration. 

As we have seen in Chapter~\ref{ch:Correctors}, many attempts
have been made to find new and original search directions.
We believe that the direction generated from the Newton system,
possibly complemented by Mehrotra's second-order correction,
are only one of the possibility of exploring the solution space.

From the study of subspace searches explored in
Section~\ref{sec:SubspaceSearches}, it is clear that the more 
directions we consider, the better the final search direction 
we get. Therefore, if we had a cheap way of generating search
directions (rather than from solving a system of linear equations),
then these should be employed.
In this respect, Mehrotra and Li \cite{MehrotraLi} 
mention employing previous search directions alongside the usual ones. 
The use of these incurs an increased memory usage 
in order to store them, but no additional computational cost.
However, it does not seem that they were actually employed in
their implementation.
This opens some questions on what constitutes a valid
previous direction (only affine scaling, the final composite direction
or something else).

The analysis of Jarre and Wechs discussed in Section~\ref{sec:JarreWechs}
makes it very clear that the choice of the target $t$ in
the right-hand side of the Newton system is the driving
tool in finding effective search directions.
In our reimplementation of multiple centrality correctors, we
have pushed the target vector of complementary points further
in the infeasible space with the aim to generate a better
correction to the current iterate.

In Chapter~\ref{ch:Warmstart}
we introduced a technique that exploits the near-optimal solution
to a stochastic linear program corresponding to a 
reduced scenario tree to warm-start a much larger problem 
that encompasses the complete scenario tree.
Our way of reducing the dimension of the scenario tree 
is based on the assumption that we have no knowledge 
of the underlying stochastic process. 
Therefore we developed an ad-hoc measure of distance 
between the scenarios, and we proposed to select for the 
reduced tree those that minimize the distance to 
a selection of representative scenarios. 
Other possibilities can be devised, and may be 
the subject of future research.

We observed that the iterate generated from the reduced problem
provides an advanced starting point for the solution of the complete problem,
in general resulting in a decrease of the number of iterations needed.
As the computational cost of generating such an iterate is negligible,
this produces consistent savings in computational time.

If the iterate produced by a reduced tree is not good enough 
in the sense that the warm-start strategy fails, then another one 
can be produced by generating a modified reduced tree (more 
bushy for example).
This second tree will provide a better approximation to the
complete tree, and the corresponding warm-start point produces
smaller infeasibilities. Hence, the chances for a successful 
warm-start increase.
This leads to the idea of allowing a multi-start procedure, in
which a series of reduced trees of increasing size are generated,
and the solution to one of them is used to construct an 
advanced starting point for the next instance.

In our analysis we assumed not to have any information about the
undelying stochastic process that governs the uncertainty.
However, it is clear that such information would provide additional
elements on which the choice of the reduced tree can be based.
In such a situation, two trees would be generated: the complete one,
and an optimally reduced one.
As we have seen, in constructing the warm-start point we need
to know the mapping of nodes between the reduced and complete trees.
Therefore, the tree-reduction process should provide this information
as well.

The relative youth of interior point methods means that there is still
a lot to learn and try, particularly in practical implementations.
An interesting avenue of research, according to the author, is the development
of specialised techniques to exploit the problem structure.
This means that the development and diffusion of structure-exploiting
codes and of structure-aware modelling languages may become a necessary
requirement for a new generation of interior point codes.
In this sense, also theoretical developments in these aspects are 
wanted and necessary.
