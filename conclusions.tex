%Started on 5 February 2007
%Mar: 31
%Apr:  1,  2, 18, 28

%
% Chapter: Conclusions
%
\label{ch:Conclusions}


In Chapter~\ref{ch:Correctors} we have revisited the 
technique of multiple centrality correctors \cite{Gondzio96} 
and added a new degree of freedom to it. 
Instead of computing the corrected direction from 
$\Delta w = \Delta_p w + \Delta_c w$ where 
$\Delta_p w$ and $\Delta_c w$ are the predictor and corrector terms, 
we allow a choice of weight 
$\omega \in (0,1]$ for the corrector term and compute 
$\Delta w = \Delta_p w + \omega \Delta_c w$.
We combined this modification with the use of a symmetric neighbourhood
of the central path. 

The extensive computational results presented for different 
classes of problems demonstrate the potential of the new scheme. 
We have compared our algorithm against the recently introduced 
Krylov subspace scheme \cite{MehrotraLi}.
The two approaches have similarities: they look for a set of attractive 
independent terms from which the final direction is constructed. 
Mehrotra and Li's approach uses the first few elements from the basis
of the Krylov space; the weighted correctors technique
generates direction terms using 
centrality correctors of \cite{Gondzio96}. Mehrotra and Li's approach 
solves an auxiliary linear program to find an optimal combination 
of all available direction terms; our approach repeatedly chooses 
the best weight for each newly constructed corrector term (and switches 
off if the use of the corrector does not offer sufficient improvement). 
Eventually, after adding $k$ corrector terms, 
the directions used in our approach have form
\[
  \Delta w = \Delta_a w + \omega_1\Delta_1 w + \ldots + \omega_k\Delta_k w,
\]
and the affine-scaling term $\Delta_a$ contributes to it without any
reduction. Hence, the larger the stepsize, the more progress we make
towards the optimizer.

The computational comparison presented in Section~\ref{ML-tests} 
shows some advantage
of our scheme over that of \cite{MehrotraLi}. Indeed, with the same 
number of direction terms allowed, our scheme outperforms Krylov subspace 
correctors by a wide margin.

Besides, our scheme follows the usual predictor--corrector framework,
and thus we expect it to be easier to implement in other 
interior point software.
However, we realise that using a small linear programming subproblem
to produce the optimal weighting of search directions can only improve
our results in terms of stepsizes and number of iterations.
It was suggested by Nick Gould to implement a mixed strategy;
by the multiple centrality correctors heuristic we can dynamically
find the number of correctors needed at each iteration; then we find
the weights by solving a subproblem.
This suggestion leads to another possibility: in the scheme of
Mehrotra and Li, they should have a heuristic way to decide how many
Krylov vectors they want to produce, rather than always generating
a fixed number of them.

The relative youth of interior point methods means that there is still
a lot to learn and try, particularly in practical implementations.
We present here some observations derived from the experience
gathered during this research.

As we have seen in Chapter~\ref{ch:Correctors}, many attempts
have been made to find new and original search directions.
We believe that the direction generated from the Newton system,
possibly complemented by Mehrotra's second-order correction,
are only one of the possibility of exploring the solution space.

From the study of subspace searches explored in
Section~\ref{sec:SubspaceSearches}, it is clear that the more 
directions we consider, the better the final search direction 
we get. Therefore, if we had a cheap way of generating search
directions (rather than from solving a system of linear equations),
then these should be employed.
In this respect, Mehrotra and Li \cite{MehrotraLi} 
mention employing previous search directions alongside the usual ones. 
The use of these incurs an increased memory usage 
in order to store them, but no additional computational cost.
However, it does not seem that they were actually employed in
their implementation.
This opens some questions on what constitutes a valid
previous direction (only affine scaling, the final composite direction
or something else).

The analysis of Jarre and Wechs discussed in Section~\ref{sec:JarreWechs}
makes it very clear that the choice of the target $t$ in
the right-hand side of the Newton system is the driving
tool in finding effective search directions.
In our reimplementation of multiple centrality correctors, we
have pushed the target vector of complementary points further
in the infeasible space with the aim to generate a better
correction to the current iterate.

In Chapter~\ref{ch:Warmstart}
we have introduced a technique that exploits the near-optimal solution
to a stochastic linear program corresponding to a 
reduced scenario tree to warm-start a much larger problem 
that encompasses the complete scenario tree.
Our way of reducing the dimension of the scenario tree 
is based on the assumption that we have no knowledge 
of the underlying stochastic process. 
Therefore we developed an ad-hoc measure of distance 
between the scenarios, and we proposed to select for the 
reduced tree those that minimize the distance to 
a selection of representative scenarios. 
Other possibilities can be devised, and may be 
the subject of future research.

We observed that the iterate generated from the reduced problem
provides an advanced starting point for the solution of the complete problem,
in general resulting in a decrease of the number of iterations needed.
As the computational cost of generating such an iterate is negligible,
this produces consistent savings in computational time.

If the iterate produced by a reduced tree is not good enough 
in the sense that the warm-start strategy fails, then another one 
can be produced by generating a modified reduced tree (more 
bushy for example).
This second tree will provide a better approximation to the
complete tree, and the corresponding warm-start point produces
smaller infeasibilities. Hence, the chances for a successful 
warm-start increase.
This leads to the idea of allowing a multi-start procedure, in
which a series of reduced trees of increasing size are generated,
and the solution to one of them is used to construct an 
advanced starting point for the next instance.

In our analysis we assumed not to have any information about the
undelying stochastic process that governs the uncertainty.
However, it is clear that such information would provide additional
elements on which the choice of the reduced tree can be based.
In such a situation, two trees would be generated: the complete one,
and an optimally reduced one.
As we have seen, in constructing the warm-start point we need
to know the mapping of nodes between the reduced and complete trees.
Therefore, the tree-reduction process should provide this information
as well.

A big avenue of research, according to the author, is the development
of specialised techniques to exploit the problem structure.
This means that the development and diffusion of structure-exploiting
codes and of structure-aware modelling languages may become a necessary
requirement for a new generation of interior point codes.

In this sense, also theoretical developments in these aspects are 
wanted and necessary.
