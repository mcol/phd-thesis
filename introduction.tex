%Started on 30th August 2006
%Aug: 31
%Sep:  1,  8

%
% Chapter: Background and introduction.
%
\label{ch:Introduction}

In this chapter we present the background and motivations for
this research.

%
% Section
%
\section{Linear programming}

\fb{
An historical account of linear programming is given in \cite{Schrijver86}.
}

\fb{
The simplex method as an active set method.
}

\fb{
Maybe something on the difficulty of LP? And something that motivates
the fact that a solution has to be at a vertex.
}

The simplex method reaches a solution by visiting a sequence of 
vertices of the polyhedron, moving from one vertex to an adjacent 
one characterized by a better objective function value. The polyhedron 
corresponding to a linear system of $m$ constraints in $n$ variables 
($m < n$) has a number of vertices equal to
\[
\binom{n}{m} = \frac{n!}{m!(n-m)!}.
\]

\fb{
Klee and Minty's example of bad behaviour of the simplex method
(when Dantzig's pivoting rule is used).
}

In real-life problems, this situation never arises, and not all of 
these vertices will have to be visited. Besides, given the monotonic
way of choosing the next vertex, in the non-degenerate case the set 
of possible vertices decreases after each iteration. Degeneracy
complicates things because at a degenerate vertex, the simplex method
may try different changes of basis without actually moving away
from the vertex.

\fb{
Discussion on the complexity of linear programming. How the simplex
method is exponential.
}

\fb{
The search for a polynomial algorithm and the ellipsoid method.

In Khachiyan's ellipsoid method, the polyhedron is inscribed in a sequence
of ellipsoids of decreasing size. If the problem has a solution, the 
centers of these ellipsoids converge to the optimal solution; otherwise,
their size decreases to zero.

The ellipsoid method finds a solution in $\bigO(n^2L)$ iterations,
thus has polynomial complexity. Since this worst-case bound
is generally attained, its practical performance is not competitive
with other solution methods. Besides, other problems relate to round-off
errors and dense matrix computation.

However, the ellipsoid method is often used in the contex of
combinatorial optimization as an analytic tool to prove complexity
results for algorithms.
}

We know polynomial-time algorithms for linear programming, 
namely the ellipsoid method (see \cite[ch.~13]{Schrijver86} 
and \cite[ch.~I.6]{ip:NemhauserWolsey88}) and interior point 
methods (\cite{ipm:Wright97}). Despite being an exponential 
algorithm, the simplex method shows polynomial complexity in 
the average time, and is therefore widely adopted in the 
solution of linear programming problems.


%
% Section
%
\section{Interior point methods}

Since their introduction following Karmarkar's groundbreaking paper
\cite{Karmarkar}, interior point methods (IPMs for short) have attracted 
the interest of a growing number of researchers.

\fb{
Karmarkar's algorithm is a primal algorithm based on a projection
in some transformed space and steepest descent. It uses a potential
function to measure the progress. It has $\bigO(nL)$ complexity.

}

Over the last 20 years, an impressive wealth of theoretical research
has been published, and computational developments have brought life
to a field, that of Linear Programming, that seemed not to attract much
attention anymore.

\fb{
Cite the reports on how much the simplex method has improved as a 
consequence of that.
}

Interior point methods are well-suited to solving very
large scale optimization problems. Their theory is well understood
\cite{ipm:Wright97} and the techniques used in their implementation 
are documented in extensive literature (see, for example, 
\cite{AndersenGondzioMeszarosXu,GondzioTerlaky} and the references therein).

\fb{
Cases where the simplex method cannot be beaten: hyper-sparsity.
}

\fb{
Contrary to active set algorithms, interior point methods reach a
solution only asymptotically. Mention crossover strategies and finite
termination.
Once a solution with a prescribed optimality tolerance has been found,
such a point can be projected upon a face of the polyhedron.
}

%
% Section
%
\section{Other stuff}

A generic optimization algorithm can be summarised in the following
scheme:

\bt{
\begin{description}
\item[Given] an initial iterate;
\item[Repeat] for $k=0,1,2,\ldots$ 
  \begin{itemize}
  \item Determine a search direction.

  \item Determine how far to move along it.

  \item Move to the next point.
  \end{itemize}

\item[Until] some termination criteria are met.
\end{description}
}

Each of the point of this simplified algorithm has to be specialised
to the specific algorithm we are talking about.

%
% Section
%
\section{Motivation of this thesis}

Optimization algorithms are extremely important in real-life 
applications. Theoretical advances are necessary for the 
understanding of the current state and the opening of new avenues 
of research. 

However, theory per se has rarely a direct impact on the lives 
of those who use optimization as a tool to solver their problems.
It is therefore necessary that the understanding gathered from
theoretical studies is then transformed into actual practical
tools. This often requires the implementation of computer programs.

The process of creating computationally efficient methods from
theoretical studies is not as direct as it might sound. It usually
involves relaxing many of the theoretical assumptions, and ensuring
other properties.

Therefore we will put great effort in accompanying theoretical
results with the corresponding computational considerations. While
in a few cases these can be treated simultaneously, generally that
will not be possible. There are a few reasons for this:
\begin{itemize}
\item Theoretical assumptions may not be realistic: this is the case
when a condition stated in a theorem is not realistically satisfiable 
in practice (for examples, bounds on some quantities). 
\item Theoretical assumptions may be too restrictive: this happens
when the theory predicts a certain behaviour under some conditions
but actually in practice it happens anyway, or provides a worst-case 
result which may be far from the average one.
\item Theoretical requirement is computationally expensive: this 
may happen when the satisfaction of a certain condition is not 
computationally viable, and workarounds are usually employed.
\end{itemize}

Besides this, the practical implementation of an algorithm happens
in a context that is not amenable to theoretical analysis for the
following reasons:
\begin{itemize}
\item Finite precision of floating-point arithmetic;
\item Heuristic choices;
\item 
\end{itemize}

%
%
\section{Main references}

The original results presented in this thesis are mainly based on two
papers that have been submitted for publication.

The first \cite{ColomboGondzio05} is a joint work with Jacek Gondzio.
The main objective of this paper was to analyze the efficiency of
corrector directions in the light of the theoretical studies of Cartis
\cite{Cartis04,Cartis05}. It concentrates on ensuring that a corrector
direction computed at the current iterate is not rejected because it
produces a short stepsize. Such a behaviour usually is manifested when
the point is badly centered or highly infeasible.

The second \cite{ColomboGondzioGrothey06} is a joint work with
Jacek Gondzio and Andreas Grothey. It aims at developing an
efficient way of constructing a starting point for structured 
large-scale stochastic linear programs.
