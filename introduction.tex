%Started on 30th August 2006
%Aug: 31
%Sep:  1,  8
% 2007
%Jan: 15, 22, 26, 29, 31
%Feb:  1,  2,  4,  5,  6,  9, 11, 21
%Mar:  8, 12, 17, 19, 20, 21, 27, 28, 29, 30, 31
%Apr:  1,  2, 16
%May:  2,  8, 12

%
% Chapter: Background and introduction.
%
\label{ch:Introduction}

In this chapter we present the background and scope of
this research. In particular we introduce the field of linear programming,
discuss its relevance, and compare some solution methods, specifically
with regard to their computational complexity.
Further, we motivate our research and provide an outline of the chapters
of this thesis.

%
% Section
%
\section{Linear programming}

{\em Linear programming} is a relatively new discipline in the mathematical
spectrum.
It was developed as mathematical models were being introduced for
economic and military planning in the years immediately following
the end of World War II.
The realisation of its usefulness came simultaneously with the development
of a solution method, the simplex method.
The introduction of the first computer calculators was crucial to
the blossoming and increase of this newly born area of study. 
Historical accounts of the birth and development of linear programming 
can be drawn from many sources, such as \cite[Chapter~2]{Dantzig63} and 
\cite{Schrijver86}. Dantzig's personal recollections are also
in \cite{Dantzig02}.

A broad definition of linear programming has been given by 
Dantzig \cite{Dantzig02}:
\begin{quotation}
Linear programming can be viewed as part of the great revolutionary 
development which has given mankind the ability to state general goals
and to lay out a path of detailed decisions to take in order to ``best''
achieve its goals when faced with practical situations of great complexity.
\end{quotation}

Further, Dantzig \cite{Dantzig02} mentions the essential 
components of linear programming:
\begin{quotation}
Our tools for doing this are ways to formulate real-world problems in
detailed mathematical terms (models), techniques for solving the models
(algorithms), and engines for executing the steps of algorithms
(computers and software).
\end{quotation}

An {\em optimization problem} can be described in terms of decision variables, 
a set of constraints, and an objective function. 
The investigation of optimization problems stems from the natural
desire to solve a problem in the ``best possible way''.
It is interesting to note that while the need for an objective function 
is obvious now, it was not clear when the first problems were modelled: the 
set of feasible solution used to be investigated with some ad-hoc criteria, 
instead of being guided by the optimization of some quantity 
\cite{Dantzig02}.

A {\em linear programming problem} is an optimization problem in which
the objective function and constraints are linear. 
Linear programming problems arise directly from real-life problems
(for example in economics, finance, logistics,
and other areas), or as approximations to
more complicated formulations, as most real-life relationships are
nonlinear. Another important source of linear programs is the 
continuous relaxation of integer programming problems \cite{Schrijver86}.

Among the class of convex optimization problems, linear programming
has a peculiar feature which is described by the following Theorem.

\begin{theorem}[Fundamental theorem of linear programming]
\label{th:FundamentalLP}
For a consistent linear programming problem with a
feasible domain $\mathcal{P}$, the optimal objective value is either
unbounded or is achievable at least at one extreme point of $\mathcal{P}$.
\end{theorem}

The set of linear constraints defines a {\em polyhedron} that constitutes
the {\em feasible region}.
According to Theorem~\ref{th:FundamentalLP}, in looking for a solution 
we can restrict our attention to the vertices of this polyhedron.
The polyhedron corresponding to a linear system of $m$ constraints 
in $n$ variables ($m < n$) has a number of vertices equal to
\be \label{eq:NumberVertices}
\binom{n}{m} = \frac{n!}{m!(n-m)!} \ge \left( \frac{n}{m} \right)^m.
\ee
This number is an overestimate, as not all of these choices correspond
to feasible points.

The fact that the number of vertices is finite guarantees termination 
of any algorithm that explores all vertices.
However this number is exponential, as can be clearly seen by further
manipulating (\ref{eq:NumberVertices}):
\[
\left( \frac{n}{m} \right)^m \ge 2^m 
\quad \mbox{for } n \ge 2m.
\]
This observation gives rise to the need of defining an algorithm
that uses an intelligent way to discover an optimal vertex 
among the multitude of non-optimal ones.

An important feature of any algorithm is its efficiency, that is 
how much effort is needed for the algorithm to provide an answer
for some given input.
The concept of {\em computational complexity} was introduced in the 70s,
as the greater availability of computing machines required a deeper insight
on the computational performance of different algorithms.
The computational complexity of an algorithm can be used as a measure
of the growth in the computational effort
as a function of the size of the problem. Therefore, it provides 
a worst-case measure.
The formal notion of {\em efficiency} is that a problem has an 
algorithm running in time proportional to a polynomial function 
of its input size. That is, we consider an algorithm efficient
 if it runs in time $\bigO(n^k)$ on any input of size $n$, 
for some constant $k>0$.

An exhaustive presentation of this topic exceeds the aims of this thesis.
We refer the reader to available introductions in the area, 
\cite[Chapter~2]{Schrijver86} among others.

Complexity proofs rely on two assumptions that are necessary 
simplifications:
\begin{enumerate}
\item Computations are performed in exact arithmetic;
\item The numerical data $(A, b, c)$ of a problem instance is rational.
\end{enumerate}

Computational complexity is measured by the number of elementary operations
required to perform the algorithmic steps until termination. It often depends 
on the size of the binary representation of the input,
usually denoted by $L$.

Many algorithms have been proposed for solving a variety of optimization
problems. However, despite their diversity, they are based on the same 
general framework which is summarised in the steps of 
Algorithm~\ref{alg:GenericOptimization}.

\renewcommand{\algorithmicrequire}{\textbf{Given:}}
\renewcommand{\algorithmicrepeat}{\textbf{Repeat:}}
\renewcommand{\algorithmicuntil}{\textbf{Until}}
\begin{algorithm}[ht]
  \caption{Generic optimization algorithm}
    \begin{algorithmic}  \label{alg:GenericOptimization}
      \REQUIRE An initial iterate $w$;
      \smallskip
      \REPEAT
         \STATE Determine a search direction $\Delta w$.
         \smallskip
         \STATE Compute the distance $\alpha$ of how far to move along
	        the search direction.
         \smallskip
         \STATE Move to the next point $w + \alpha\Delta w$.
         \smallskip
      \UNTIL Some termination criteria are met.
  \end{algorithmic}
\end{algorithm}

Each element of this generic framework
(starting point, search direction, stepsize, termination criteria)
has to be accurately specialised in order to define a specific algorithm.
In what follows, we will introduce the main ideas behind three 
different solution methods for linear programming: the simplex method,
the ellipsoid method, and the class of interior point methods.

%
%
\subsection{The simplex method}

The simplex method was introduced in 1947 by George Dantzig 
\cite{Dantzig63}. The
introduction of the simplex method happened simultaneously with
the realisation of linear programming as an efficient modelling tool
for practical decision making. 

The simplex method exploits the insight provided by the fundamental 
theorem of linear programming (\ref{th:FundamentalLP}), which states that
the optimal solution, if it exists, is at one of the vertices
of the feasible polytope.
Thus it reaches a solution by visiting a sequence of 
vertices of the polyhedron, moving from each subsequent vertex to an adjacent 
one characterised by a better objective function value
(in the non-degenerate case). 
Since the number of vertices is finite, termination is guaranteed.

Moreover, given the monotonic
method of choosing the next vertex, the set of possible vertices 
decreases after each iteration, in the non-degenerate case.
Degeneracy
occurs when a vertex in $\R^m$ is defined by $p > m$ constraints,
and a step of length zero may be produced.
In such a case, the simplex method does not actually move away
from the current vertex, and thus no improvement in the objective
function value can be achieved.

%The simplex method can be seen as an active set method, as at each
%iteration only a set of constraints are active (those that define
%the vertex).

In terms of practical efficiency, the simplex algorithm has 
long been considered the undisputed method for solving linear programming
problems.
However, the simplex method has exponential complexity. It is possible that all
the vertices of the feasible polyhedron have to be visited
before an optimal solution is reached.
Klee and Minty \cite{KleeMinty} were the first to provide an example 
of pathological behaviour of the simplex method.
In their example, a linear program with $n$ variables 
and $2n$ inequalities,
the simplex method visits each of the $2^n$ vertices.
%The same example does not cause this pathological behaviour if different
%pivoting rules are implemented (see \cite[Chapter~4]{lp:Chvatal}).

However, no cases of exponential number of iterations have been encountered 
in real-life problems, and usually only a fraction of the vertices
are actually traversed before the optimal one is found.
Moreover, in most cases the simplex algorithm shows 
polynomial behaviour, being linear in $m$ and sublinear in $n$
\cite[p.94]{FangPuthenpura93}.
A survey on the efficiency of the simplex method is done by Shamir 
\cite{Shamir87}, where a probabilistic analysis (as opposed to
worst-case analysis) is also presented.

The gap between the observed and theoretical worst-case performances
of the simplex method is still unexplained.
Given this theoretical drawback, a great deal of effort 
has been put into finding an
algorithm for linear programming which is 
characterised by a polynomial-time bound.

%
%
\subsection{The ellipsoid method}

In 1979 a breakthrough occurred, as Khachiyan showed how to adapt 
the ellipsoid method for convex programming to the linear programming case,
and determined the computational complexity of linear programming.

In Khachiyan's ellipsoid method, the feasible polyhedron is inscribed in 
a sequence of ellipsoids of decreasing size. 
The first ellipsoid has to be large enough to include a feasible 
solution to the constraints; the volume of the successive ellipsoids 
shrinks geometrically. Therefore it generates improving iterates
in the sense that the region in which the solution lies is 
reduced at each iteration in a monotonic fashion.
The algorithm either finds a solution, as the centres of the ellipsoids
converge to the optimal point, or states that no solution exists.
More details on the ellipsoid method can be found in 
\cite[Chapter~13]{Schrijver86} and \cite[Chapter~I.6]{ip:NemhauserWolsey88},
for example.

The exciting property of the ellipsoid method is that it finds a 
solution in $\bigO(n^2L)$ iterations, and thus has polynomial complexity. 
However, since the ellipsoid algorithm generally attains
this worst-case bound \cite{GoldfarbTodd82},
its practical performance is not competitive
with other solution methods. Besides, it displays other drawbacks
related to large round-off errors and a need for dense matrix computation.
Nevertheless, the ellipsoid method is often used in the context of
combinatorial optimization as an analytic tool to prove complexity
results for algorithms \cite{ip:NemhauserWolsey88}.

%
%
\subsection{Interior point methods}

{\em Interior point methods} were being developed in the 60s and the 
beginning of the 70s as methods to solve nonlinear programming problems 
with inequality constraints. 
However, they fell from favour and received less and less attention
because of their inefficiency and the presence of strong competitors
such as sequential quadratic programming \cite{MWright92}.

Since their reintroduction, this time to solve linear programs, 
following Karmarkar's groundbreaking paper
\cite{Karmarkar}, interior point methods have attracted 
the interest of a growing number of researchers.
This algorithm was also proved to have polynomial complexity: 
indeed, it converges in $\bigO(nL)$ iterations. As opposed to
Khachiyan's ellipsoid method, in practice Karmarkar's algorithm actually
performs much better than its worst-case bound states.

The main idea behind interior point methods is fundamentally different 
to the one that inspires the simplex algorithm. Here, the optimal vertex 
is approached by moving through the interior of the feasible region.
This is done by creating a family of parametrised approximate solutions
that asymptotically converge to the exact solution.
Therefore, by embedding the linear problem in a nonlinear context,
an interior point method escapes the ``curse of dimensionality''
characteristic of dealing with the combinatorial features of the 
linear programming problem.
For details on Karmarkar's algorithm, we refer to
\cite[Chapter~6]{FangPuthenpura93}.

Karmarkar \cite{Karmarkar} explained the advantage of an
interior point approach as follows:
\begin{quotation}
In the simplex method, the current solution is modified by introducing
a nonzero coefficient for one of the columns in the constraint
matrix. Our method allows the current solution to be modified by
introducing several columns at once.
\end{quotation}

%Karmarkar's algorithm is a primal algorithm based on a projection
%in some transformed space and steepest descent. It uses a potential
%function to measure the progress.

Karmarkar announced that his method was extremely successful in practice, 
claiming to beat the simplex method by a large margin (50 times,
as reported in \cite{MWright92}).
A variant of Karmarkar's original algorithm was then proposed and 
implemented by Adler, Resende, Veiga and Karmarkar 
\cite{AdlerResendeVeigaKarmarkar89}.
Since then, the theoretical understanding has considerably improved,
many algorithmic variants have been proposed and several of
them have shown to be computationally viable alternatives to the
simplex method.

Over the last 20 years, an impressive wealth of theoretical research
has been published, and computational developments have brought life
to the field of linear programming, which had not seemed to attract much
attention anymore.
Among the positive consequences of the renewed interest in linear
programming are the improvements to the implementations of 
simplex-based solvers \cite{Bixby94,Bixby02}.

There are classes of problems that are best solved with the simplex
method, and others for which an interior point method is preferred.
Size, structure and sparsity play a major role in the choice of
algorithm for computations.
As a rule of thumb, with the increase of problem dimension, 
interior point methods become more effective.
However, this does not hold in the hyper-sparse case, where the
simplex method is virtually unbeatable \cite{Bixby02,HallMcKinnon05}, 
and for network problems,
where the specialised network simplex method can exploit the
structure in an extremely efficient manner \cite{ip:NemhauserWolsey88}.

Interior point methods are well-suited to solving very
large scale optimization problems. Their theory is well understood
\cite{RoosTerlakyVial,ipm:Wright97,Ye97} 
and the techniques used in their implementation 
are documented in extensive literature (see, for example, 
\cite{AndersenGondzioMeszarosXu,GondzioTerlaky,LustigMarstenShanno94} 
and the references therein).
They can be applied to a wide range of situations with no need
of major changes. In particular, they have been successfully applied to
complementarity problems, quadratic programming,
convex nonlinear programming, second-order cone programming and
semidefinite programming.


%
% Section
%
\section{Motivation and scope of this research}

Optimization algorithms are extremely important in real-life 
applications. Theoretical advances are necessary for a deeper
understanding of the the available techniques and the opening of new avenues 
of research. 
However, theory per se rarely has a direct impact on the lives 
of those who rely on optimization as a tool to solve their problems.
It is therefore necessary that the insight gathered from
theoretical studies is then transformed into effective practical
tools. This usually requires the implementation of computer programs
with the aims of accuracy, speed, and reliability.

The process of creating computationally efficient methods from
theoretical studies is not as direct as it might sound, but is somewhat
an art in itself. It often involves relaxing many of the theoretical 
assumptions, while ensuring other properties and conditions.
Therefore we will put great effort in accompanying theoretical
results with the corresponding computational considerations. While
in a few cases these can be treated simultaneously, generally that
will not be possible. There are a few reasons for this:
\begin{itemize}
\item Theoretical assumptions may not be realistic. This is the case
when a condition stated in a theorem is not realistically satisfiable 
in practice (for example, bounds on some quantities). 
\item Theoretical requirements may be computationally expensive. This 
happens when checking the satisfaction of a certain condition is not 
viable in practice.
\item Theoretical assumptions may be too restrictive. This occurs
when the theory predicts the conditions under which a certain behaviour
happens, but in practice the same behaviour happens under more relaxed
conditions.
\item Theoretical analysis provides a worst-case result which may be 
very far from the average one.
\end{itemize}

On the other hand, the practical implementation of an optimization
algorithm happens in a context that is not amenable to theoretical 
analysis for the following reasons:
\begin{itemize}
\item Finite precision of floating-point arithmetic;
\item Dependence on the numerical inputs;
\item Heuristic algorithmic choices.
\end{itemize}

For these reasons, we will take a very pragmatic view of the
field of optimization. 
The approach that permeates this research and 
the presentation of its results is markedly computationally oriented. 
We recognise the importance and validity of theoretical developments, 
and we appreciate the insights that such results provide.
Hence, we will discuss the theoretical foundations of 
interior point methods for linear programming and contribute to it.
However, as we are keen on improving existing computer implementations of 
interior point methods, 
our main drive will be the development of implementable techniques
for interior-point-based linear programming solvers.

The contributions of this research are threefold.

First, we introduce and formalise the concept of 
{\em symmetric neighbourhood} as the
driving tool for the analysis and understanding of the good behaviour
of some practical algorithms. 
The practical success of an interior point code is negatively
affected by the presence of unbalanced complementary products.
In this sense, 
the quality of centrality (understood in a simplified way 
as complementarity) in a practical algorithm is not 
conveniently characterised by either of the two neighbourhoods 
$\Nhood_2$ or $\Nhood_{-\infty}$ which are usually employed in 
theoretical developments of interior point methods.
The symmetric neighbourhood $\Nhood_s$ is a variation on the more common
$\Nhood_{-\infty}$ neighbourhood in which the complementarity pairs
are bounded both from above and below by a quantity 
that depends on the barrier parameter $\mu$ but does
not depend directly on $n$.
Within the $\Nhood_s$ neighbourhood, the spread among 
complementarity pairs is kept
under control and is reduced with the barrier parameter $\mu$.
This has beneficial consequences on the quality of the search
directions generated by Newton's method. We show that
a long-step feasible algorithm based on the symmetric neighbourhood
is globally linearly convergent and
has the same computational complexity as an algorithm based
on the wide $\Nhood_{-\infty}$ neighbourhood, as it
converges in $\bigO(nL)$ iterations.
Also, the symmetric neighbourhood plays a central role in the development 
of the two computational techniques that we introduce and study in this work.

The most significant use of the notion of symmetric neighbourhood
is in the {\em weighted correctors technique},
which is our second contribution.
The weighting mechanism, which is an adaptive technique to judge the
most effective way of including a corrector in the search direction, 
works particularly well in conjunction with multiple centrality
correctors.
The main objective of this research is to analyse the efficiency of
corrector directions in the light of the theoretical studies of Cartis
\cite{Cartis04,Cartis05}. It concentrates on ensuring that a corrector
direction computed at the current iterate is not rejected because it
produces a short stepsize. Such behaviour is usually manifested when
the point is badly centered or highly infeasible.
For this reason, ensuring that an appropriate (centrality) corrector
is accepted and produces an increase in the stepsize is crucial
for moving the iterate to a more favourable position.
From the computational point of view, we have to consider that
a failed iteration comes with the high cost of having factorised
the matrix for no real improvement.
We argue that following a quadratic trajectory in generating a
corrector direction is not always desirable. In particular, as the stepsizes 
in the primal and dual spaces are often much smaller than 1,
trying to correct for the whole linearisation error,
as done to generate Mehrotra's second-order corrector, is too
demanding and not always necessary.
In generating a corrector direction, we propose to use 
the difference between the achievable complementarity products 
and a carefully chosen target vector of complementary products.
Moreover, it is unrealistic to expect that a corrector direction $\Delta_c w$
can absorb the error made by a predictor direction $\Delta_p w$;
however, this direction can be used to find a new composite
direction $\Delta^\omega w = \Delta_p w + \omega\Delta_c w$, where
$\omega \in [0,1]$.
We propose to measure the quality of the composite direction
by the length of the step that can be made along it, and choose
the $\omega$ that maximizes such step.
The introduction of the weighting mechanism, which determines
the contribution of each corrector direction to the overall
search direction, allows a better implementation
of the targeting procedure of the multiple centrality correctors technique
with improved performance.
We have implemented the weighted correctors strategy in the \HOPDM
interior point code, and have studied its performance with thorough
computational experience. 
From this testing we see that such a strategy 
produces considerable improvements, particularly for large-scale
problems.

Our third contribution is in the context of the generation of 
{\em warm-start iterates for stochastic linear programs}, 
and the practical value of the 
symmetric neighbourhood is also appreciable here.
In this setting we develop an efficient way of constructing 
an advanced starting point which greatly reduces the computational
effort of solving a problem instance.
Large-scale stochastic linear programs are very structured problems
that are generated from event trees which can be very big, particularly
in the multistage case.
We show that it is possible to exploit the inherent structure of 
the stochastic programming problem and obtain a computationally viable
warm-start point by solving a problem of much smaller
dimension that corresponds to a reduced event tree.
We produce a reduced tree by trying to capture the 
important information in the stochastic scenario space while keeping 
the dimension of the corresponding (reduced) deterministic equivalent small.
It is essential for the effectiveness of an interior point algorithm
that the iterates do not approach the boundary of the
feasible region too early.
This is particularly important in reoptimization, 
as the warm-start iterate should be able to quickly absorb
perturbations in the problem data.
This suggests solving the reduced problem with low accuracy.
As the size of the reduced problem is generally very small,
this phase of the algorithm is relatively inexpensive.
Once an approximate reduced-tree solution is found,
we then expand it to the full size of
the original problem, thus generating the warm-start point.
We present theoretical conditions that the warm-start iterate
should satisfy to guarantee a successful warm-start of the complete 
problem. 
We also show that from an iterate within the symmetric neighbourhood
of the central path, we can produce
a corrector direction that absorbs the perturbation and 
keeps the point inside a larger symmetric neighbourhood,
and move along such direction with full step.
The implementation within the \HOPDM and \OOPS interior point solvers 
shows remarkable advantages on a series of problem instances.

The original results presented in this thesis have been the base for two
papers that have been submitted for publication, jointly with
Jacek Gondzio \cite{ColomboGondzio05}, and with Jacek Gondzio and 
Andreas Grothey \cite{ColomboGondzioGrothey06}.

%
%
\subsection*{Outline of the thesis}

In Chapter~\ref{ch:Ipm} we introduce and formalise some basic
notions on linear programming and present the main theoretical results 
that are at the heart of many interior-point methods. 
We concentrate on the path-following class of
primal--dual interior point methods, as they provide the framework for 
most practical implementations.
Alongside reviewing the theoretical achievements known in the
literature, we introduce and analyse the concept of 
symmetric neighbourhood.

Chapter~\ref{ch:PracticalIpm} is dedicated to presenting
the main techniques adopted in computational implementations of
interior point methods.
We consider when
theoretical analysis and practical implementation diverge,
paying particular attention to two important strategies used 
in generating effective search directions: 
Mehrotra's predictor--corrector algorithm and the
multiple centrality correctors technique.
We also review the main results in the area of warm-start
approaches for interior point methods.

In Chapter~\ref{ch:Correctors} we continue the discussion on
Mehrotra's algorithm, presenting some recent insight on the
cases of failure of the corrector direction, and studying the use of
weighted correctors in the generation of search directions. This will
be compared to the subspace searches approach, which uses
an auxiliary linear subproblem to combine
a given set of directions. The advantages
and drawbacks of both strategies is presented together with
a rich computational study on the implementation of the
weighted correctors technique.

In Chapter~\ref{ch:Warmstart} we turn our attention to the field
of stochastic programming, explain its relevance and introduce some
basic concepts.
We present a warm-start technique
that exploits the inherent structure of stochastic linear programs.
Alongside some theoretical results, we show our computational
experience on some standard test problems and larger instances coming
from the telecommunication industry.

Finally, in Chapter~\ref{ch:Conclusions} we present our conclusions
and directions for future work.
