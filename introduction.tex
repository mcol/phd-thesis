%Started on 30th August 2006
%Aug: 31
%Sep:  1,  8
% 2007
%Jan: 15, 22, 26, 29, 31
%Feb:  1,  2,  4,  5,  6,  9, 11, 21

%
% Chapter: Background and introduction.
%
\label{ch:Introduction}

In this chapter we present the background and motivations for
this research. In particular we introduce the field of linear programming,
discuss its relevance, and compare some solution methods, in particular
with regard to their computational complexity.
Further, we motivate our research and provide an outline of the chapters
of this thesis.

%
% Section
%
\section{Linear programming}

{\em Linear programming} is a relatively new discipline in the mathematical
spectrum.
Linear programming was developed as models were being used for
economic and military planning in the years immediately following
the end of World War II.
The realisation of its usefulness came together with the development
of a solution method, the simplex method;
the introduction of the first computer calculators was crucial to
the blossoming and increase of this newly born area of study. 

Historical accounts on the birth and development of linear programming 
can be found in many sources, such as \cite[ch.~2]{Dantzig63} and 
\cite{Schrijver86}. Dantzig's personal recollection are also
in \cite{Dantzig02}.

A definition of Linear programming has been given by Dantzig \cite{Dantzig02}:
\begin{quote}
Linear programming can be viewed as part of the great revolutionary 
development which has given mankind the ability to state general goals
and to lay out a path of detailed decisions to take in order to ``best''
achieve its goals when faced with practical situations of great complexity.
\end{quote}

Further, Dantzig \cite{Dantzig02} mentions the essential 
components of linear programming:
\begin{quote}
Our tools for doing this are ways to formulate real-world problems in
detailed mathematical terms (models), techniques for solving the models
(algorithms), and engines for executing the steps of algorithms
(computers and software).
\end{quote}

An {\em optimization problem} can be described in terms of a set of variables, 
some constraints, and an objective function. 
The investigation of optimization problems stems from the natural
desire to solve a problem in the ``best possible way''.
It is interesting to note that while the need of an objective function 
is obvious to us, it was not when the first problems were modelled: the 
set of feasible solution used to be investigated with some ad-hoc criteria, 
instead of being guided by the optimization of some quantity as 
objective \cite{Dantzig02}.

A {\em linear programming problem} is an optimization problem for which
the objective function and the constraints are linear. 
Linear programming problems arise directly (for example in economics,
networks, scheduling or other applications), or as approximations to
more complicated formulations, as most real-life relationships are
nonlinear. Another important source of linear programs is the 
continuous relaxation of integer programming problems.

Among the class of convex optimization problems, linear programming
has a peculiar feature which is described by the following Theorem
(see, for example, \cite{FangPuthenpura93}).

\begin{theorem}[Fundamental theorem of linear programming]
\label{th:FundamentalLP}
For a consistent linear programming problem with a
feasible domain $\mathcal{P}$, the optimal objective value is either
unbounded or is achievable at least at one extreme point of $\mathcal{P}$.
\end{theorem}

The set of linear constraints defines a {\em polyhedron} that constitutes
the feasible region.
According to Theorem~\ref{th:FundamentalLP}, in looking for a solution 
we can restrict our attention to the vertices of this polyhedron.
The polyhedron corresponding to a linear system of $m$ constraints 
in $n$ variables ($m < n$) has a number of vertices equal to
\be \label{eq:NumberVertices}
\binom{n}{m} = \frac{n!}{m!(n-m)!}.
\ee
This number is an overestimate, as not all of these choices correspond
to feasible points.

The fact that the number of vertices is finite guarantees termination 
of any algorithm that explores all vertices.
However this number is exponential, as can be seen by further
manipulating (\ref{eq:NumberVertices}) \cite[ch.~5.2]{FangPuthenpura93}:
\[
\frac{n!}{m!(n-m)!} \ge \left( \frac{n}{m} \right)^m \ge 2^m 
\quad \mbox{for } n \ge 2m.
\]

This consideration gives rise to the need of defining an algorithm
that discovers an optimal vertex among the multitude of nonoptimal ones.

Many algorithms have been proposed to solve a variety of optimization
problems. However, despite their diversity, they are based on the same 
general framework summarised in the following scheme:

\bt{
\begin{description}
\item[Given] an initial iterate;
\item[Repeat] for $k=0,1,2,\ldots$ 
  \begin{itemize}
  \item Determine a search direction.

  \item Determine how far to move along it.

  \item Move to the next point.
  \end{itemize}

\item[Until] some termination criteria are met.
\end{description}
}

Each element of this simplified algorithm 
(starting point, search direction, stepsize, termination criteria)
has to be specialised
to the specific algorithm we are talking about.

%
%
\subsection{The simplex method}

The simplex method was introduced by George Dantzig in 1947.
We should note that the
introduction of the simplex method happened simultaneously with
the realisation of linear programming as an efficient modelling tool
for practical decision making. 

The simplex method exploits the insight provided by the fundamental 
theorem of linear programming (\ref{th:FundamentalLP}).
The simplex method reaches a solution by visiting a sequence of 
vertices of the polyhedron, moving from one vertex to an adjacent 
one characterised by a better objective function value
(in the non-degenerate case). 

Since the number of vertices is finite, then termination is guaranteed.
While it is possible that all of them get visited, in real-life problems, 
this situation never arises, and only a fraction of the vertices are
actually traversed (this discussion will be continued in 
Section~\ref{sec:ComplexityConsiderations}).

Moreover, given the monotonic
way of choosing the next vertex, in the non-degenerate case the set 
of possible vertices decreases after each iteration. Degeneracy
happens when a vertex in $\R^m$ is traversed by $p > m$ constraints.
In this case, there are $\binom{p}{m}$ different bases for the same
vertex. 
In such a case, the step length produced by the ratio test
is zero. Therefore, the simplex method
may try different changes of basis without actually moving away
from the vertex and thus not obtaining any improvement in the objective
function value.

%The simplex method can be seen as an active set method, as at each
%iteration only a set of constraints are active (those that define
%the vertex).

For the practical efficiency, the simplex algorithm has been considered
for a long time the undiscussed method for solving linear programming
problems.


%
%
\subsection{Complexity considerations}
\label{sec:ComplexityConsiderations}

The {\em computational complexity} of an algorithm can be used as a measure
of the growth in the computational effort required by the algorithm
as a function of the size of the problem. Therefore, it provides 
a worst-case measure.
The concept of computational complexity has been introduced in the 70s,
as the availability of computing machines required a deeper insight
on the computational performance of different algorithms.
An exhaustive presentation of this topic exceeds the aims of this thesis:
we refer the reader to available introductions on the area, 
\cite[ch.~2]{Schrijver86} among others.

Complexity proofs rely on two assumptions that are necessary 
simplifications:
\begin{enumerate}
\item Computations are performed in exact arithmetic;
\item The numerical data $(A, b, c)$ of a problem instance is rational.
\end{enumerate}

Computational complexity is measured by the number of elementary operations
required to perform the algorithmic steps until termination. It often depends 
on the size of the binary representation of the input ($L$).

The simplex method has exponential complexity: it is possible that all
the vertices of the feasible polyhedron have to be visited
before reaching an optimal solution.
Klee and Minty \cite{KleeMinty} were the first to provide an example 
of bad behaviour of the simplex method (when Dantzig's pivoting rule 
is used). In this example of $n$ variables and $2n$ inequalities,
the simplex method visits each of the $2^n$ vertices.
The same example does not cause this pathological behaviour if different
pivoting rules are implemented (see \cite[ch.~4]{lp:Chvatal}).

However, no cases of exponential number of iterations have been encountered 
in real-life situations. Moreover, in most cases the simplex algorithm shows 
to have a polynomial behaviour, being linear in $m$ and sublinear in $n$
\cite[p.94]{FangPuthenpura93}.
A survey on the efficiency of the simplex method is done by Shamir 
\cite{Shamir87}, where also a probabilistic analysis (as opposed to
worst-case analysis) is presented.

The gap between the observed and the theoretical worst-case performance
of the simplex method is still unexplained.
Given this theoretical drawback, there have been efforts to find an
algorithm for linear programming characterised by a polynomial-time bound.

In 1979 Khachiyan showed how to adapt the ellipsoid method for convex
programming to the linear programming case.
In Khachiyan's ellipsoid method, the feasible polyhedron is inscribed in 
a sequence of ellipsoids of decreasing size. 
The first ellipsoid has to be large enough to include a solution to the
system of inequalities $Ax < b$; the volume of the successive ellipsoids 
shrinks geometrically. Therefore it generates improving iterates
in the sense that the region in which the solution lies is 
reduced at each iteration in a monotonic fashion.
The algorithm either finds a solution, as the centres of the ellipsoids
converge to the optimal point, or states that no solution exists.
More details on the ellipsoid method can be found for example in 
\cite[ch.~13]{Schrijver86} and \cite[ch.~I.6]{ip:NemhauserWolsey88}.

The exciting property of the ellipsoid method is that it finds a 
solution in $\bigO(n^2L)$ iterations, thus has polynomial complexity. 
Khachiyan's contribution settled the question on the computational 
complexity of linear programming.
However, since this worst-case bound
is generally attained, its practical performance is not competitive
with other solution methods. Besides, it displays other drawbacks
related to large round-off errors and the need of dense matrix computation.
Nevertheless, the ellipsoid method is often used in the context of
combinatorial optimization as an analytic tool to prove complexity
results for algorithms \cite{ip:NemhauserWolsey88}.

%
%
\subsection{Interior point methods}

{\em Interior point methods} were being developed in the 60s and the 
beginning of the 70s as methods to solve nonlinear programming problems 
with inequality constraints. 
However, they fell from favour and attracted less and less attention
because of their inefficiency and the presence of strong competitors
such as sequential quadratic programming.

Since their reintroduction, this time to solve linear programs, 
following Karmarkar's groundbreaking paper
\cite{Karmarkar}, interior point methods (IPMs for short) have attracted 
the interest of a growing number of researchers.
Also this algorithm was proved to have polynomial complexity: 
indeed, it converges in $\bigO(nL)$ iterations. As opposed to
Khachiyan's ellipsoid method, Karmarkar's algorithm would actually
perform much better than what the bound states.

The main idea behind interior point methods is fundamentally different 
to the one that inspires the simplex algorithm: the optimal vertex 
is approached by moving through the interior of the feasible region.
This is done by creating a family of parametrised approximate solutions
that asymptotically converge to the exact solution.
Therefore, by embedding the linear problem in a nonlinear context,
an interior point method escapes the ``curse of dimensionality''
characteristic of the dealing with the combinatorial features of the 
linear programming problem.

Karmarkar \cite{Karmarkar} explained the advantage of an
interior point approach as follows:
\begin{quote}
In the simplex method, the current solution is modified by introducing
a nonzero coefficient for one of the columns in the constraint
matrix. Our method allows the current solution to be modified by
introducing several columns at once.
\end{quote}

%Karmarkar's algorithm is a primal algorithm based on a projection
%in some transformed space and steepest descent. It uses a potential
%function to measure the progress.

Karmarkar announced that his method was extremely successful in practice, 
claiming to beat the simplex method by a large margin (50 times,
as reported in \cite{MWright92}).
A variant of Karmarkar's original algorithm was then proposed and 
implemented by Adler-Karmarkar-Resende-Veiga 
(Math. Prog. 44, 1989).
Since then the theoretical understanding has considerably improved,
and many algorithmic variants have been proposed, and several of
them have shown to be a computationally viable alternative to the
simplex method.
For details on Karmarkar's algorithm, we refer to
\cite[ch.6]{FangPuthenpura93}.

Over the last 20 years, an impressive wealth of theoretical research
has been published, and computational developments have brought life
to a field, that of Linear Programming, that seemed not to attract much
attention anymore.
Among the positive consequences brought by the renewed interest in linear
programming, let us remind the improvements in the implementations of 
simplex-based solvers \cite{Bixby94,Bixby02}.

There are classes of problems that are best solved with the simplex
method, and others for which an interior point method is preferred.
Size, structure and sparsity play a major role as to which
algorithm should be chosen for computations.
As a rule of thumb, with the increase of the problem dimension, the 
more effective interior point methods become.
However, this does not hold in the hyper-sparse case, where the
simplex method is virtually unbeatable \cite{Bixby02,HallMcKinnon05}, 
and for network problems,
where the specialised network simplex method can exploit the
structure in an extremely efficient manner.

Interior point methods are well-suited to solving very
large scale optimization problems. Their theory is well understood
\cite{ipm:Wright97} and the techniques used in their implementation 
are documented in extensive literature (see, for example, 
\cite{AndersenGondzioMeszarosXu,GondzioTerlaky} and the references therein).
They can be applied to a wider range of situations with no need
of major changes. In particular, they have been successfully applied to
complementarity problems, quadratic programming and convex 
nonlinear programming.


%
% Section
%
\section{Motivation of this thesis}

Optimization algorithms are extremely important in real-life 
applications. Theoretical advances are necessary for the 
understanding of the current state and the opening of new avenues 
of research. 

However, theory per se has rarely a direct impact on the lives 
of those who use optimization as a tool to solve their problems.
It is therefore necessary that the understanding gathered from
theoretical studies is then transformed into effective practical
tools. This usually requires the implementation of computer programs
with the aims of accuracy, speed and reliability.

The process of creating computationally efficient methods from
theoretical studies is not as direct as it might sound, but is somewhat
of an art in itself. It often involves relaxing many of the theoretical 
assumptions, while ensuring other properties and conditions.
Therefore we will put great effort in accompanying theoretical
results with the corresponding computational considerations. While
in a few cases these can be treated simultaneously, generally that
will not be possible. There are a few reasons for this:
\begin{itemize}
\item Theoretical assumptions may not be realistic: this is the case
when a condition stated in a theorem is not realistically satisfiable 
in practice (for example, bounds on some quantities). 
\item Theoretical assumptions may be too restrictive: this happens
when the theory predicts a certain behaviour under some conditions
but actually in practice it happens anyway, or provides a worst-case 
result which may be far from the average one.
\item Theoretical requirements may be computationally expensive: this 
happens when the satisfaction of a certain condition is not 
computationally viable, in which case workarounds are usually employed.
\end{itemize}

On the other hand, the practical implementation of an optimization
algorithm happens in a context that is not amenable to theoretical 
analysis for the following reasons:
\begin{itemize}
\item Finite precision of floating-point arithmetic;
\item Heuristic choices;
\item Dependence on the numerical inputs;
\item 
\end{itemize}

%
%
\subsection{Scope of this thesis}

We introduce and formalise the concept of symmetric neighbourhood as the
driving tool for the analysis and understanding of the good behaviour
of some practical algorithms. 
%Also, the symmetric neighbourhood plays
%a central role in the development of the techniques that we propose here.
%
The use of the symmetric neighbourhood lets us simplify the presentation
of the multiple centrality correctors technique \cite{Gondzio96}, and
further motivate their use in a new and more adaptive way, thanks to the 
introduction of a weighting mechanism.
%
The practical value of the symmetric neighbourhood is appreciated also
in the context of generation of a warm-start iterate for stochastic
linear programs.

The original results presented in this thesis are mainly based on two
papers that have been submitted for publication.

The first \cite{ColomboGondzio05} is a joint work with Jacek Gondzio.
The main objective of this paper is to analyze the efficiency of
corrector directions in the light of the theoretical studies of Cartis
\cite{Cartis04,Cartis05}. It concentrates on ensuring that a corrector
direction computed at the current iterate is not rejected because it
produces a short stepsize. Such a behaviour usually is manifested when
the point is badly centered or highly infeasible.

The second \cite{ColomboGondzioGrothey06} is a joint work with
Jacek Gondzio and Andreas Grothey. It aims at developing an
efficient way of constructing a starting point for structured 
large-scale stochastic linear programs.
It shows that it is possible to obtain a computationally viable
warmstart point by solving a stochastic problem of much smaller
dimension.


%
%
\subsection{Outline of this thesis}

In Chapter~\ref{ch:Ipm} we introduce and derive the primal--dual
path-following algorithm for linear programming. This will be
the moment of introducing the main theoretical results that are at
the base of most interior-point methods, as well as concentrate
on those that are more used in practical implementations.

In Chapter~\ref{ch:PracticalIpm} we shift our focus towards 
the main techniques adopted in computer implementations of
interior point methods.
We will discuss two important strategies used in generating
effective search directions, paying particular attention
to the details for which there is a divergence between theoretical
analysis and practical implementation.

Chapter~\ref{ch:Correctors} is dedicated to studying the use of
weighted correctors in the generation of search directions. This will
be compared to the subspace searches approach, which tries something
similar but considering a given set of directions. The advantages
and drawbacks of both strategies will be discussed together with
a rich computational study.

In Chapter~\ref{ch:Warmstart} we present a warmstart technique
that exploits the inherent structure of a stochastic linear programs.
Alongside some theoretical results, we will show the computational
experience on some standard test problems and larger instances coming
from the telecommunication industry.

Finally, in Chapter~\ref{ch:Conclusions} we present our conclusions
and directions for future work.
