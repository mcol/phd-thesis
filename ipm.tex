%Started on 9th August 2006
%Aug: 11, 22, 23, 24, 25, 28, 29, 30, 31
%Sep:  1,  7,  8
% 2007
%Jan: 15, 16, 17, 22, 26, 29, 30
%Feb:  1,  2,  4,  5,  6,  7,  8,  9, 11, 21, 22
%Mar:  8,  9

%
% Chapter: Interior point methods
%
\label{ch:Ipm}

Interior point methods are well-suited to solving very
large scale optimization problems. Their theory is well understood
and a number of survey papers and academic books are available
\cite{Gonzaga92,MWright92,ipm:Wright97}.

This chapter is devoted to the derivation and analysis of primal--dual
path-following interior point methods. 
We present the elements that are at the base of this successful class
of algorithms, concentrating on their theoretical properties and
attractive features.


%
% Section
%
\section{Primal--dual path-following methods}
\label{sec:Derivation}

Consider the following primal--dual pair of linear programming problems 
in standard form
%
\begin{eqnarray} \label{eq:PrimalDualPair}
  \begin{array}{crlp{2cm}crl}
     & \mbox{ min } & c^T x     &  &     & \mbox{ max }  & b^T y \\
 (P) &\mbox{ s.t. } & Ax = b,   &  & (D) & \mbox{ s.t. } & A^T y + s = c, \\
     &              & x \geq 0; &  &     &   & y \mbox{ free,} \;\; s \geq 0,
  \end{array}
\end{eqnarray}
%
where $A \in \R^{m \times n}$, $x, s, c \in \R^{n}$ 
and $y, b \in \R^{m}$, $m<n$. We assume, without loss of generality,
that $A$ has full row rank, as linearly dependent rows can be
removed without changing the solution set.
This implies that a feasible $s \ge 0$ determines in a unique
way the value of $y$.
In fact, the $y$ variables can be eliminated thus producing the
symmetric combined primal--dual form studied by Todd and Ye \cite{ToddYe90}.

We define the sets of primal feasible points and of
primal interior points 
\[
\mathcal{P} = \{ x : Ax = b, \; x \ge 0 \}, \quad
\mathcal{P}^0 = \{ x \in \mathcal{P} : x > 0 \},
\]
and, similarly, the sets of dual feasible points and of
dual interior points
\[
\mathcal{D} = \{ (y,s) : A^T y + s = c, \; s \ge 0 \}, \quad
\mathcal{D}^0 = \{ (y,s) \in \mathcal{D} : s > 0 \}.
\]
The set of feasible primal--dual points is therefore
$\mathcal{F} = \mathcal{P} \times \mathcal{D}$, and the set of primal--dual
interior points is
\[
\mathcal{F}^0 = \{ (x,y,s) \in \mathcal{F} : (x,s) > 0 \}.
\]
A point $(x,y,s) \in \mathcal{F}^0$ is said to be {\em strictly feasible}
for the primal--dual pair (\ref{eq:PrimalDualPair}).

Using this notation, the primal--dual pair (\ref{eq:PrimalDualPair}) can 
be written as
\be \label{eq:PrimalDualPair2}
\min \; c^T x, \;\;  x    \in \mathcal{P}; \qquad
\max \; b^T y, \;\; (y,s) \in \mathcal{D},
\ee

We recall here some well-known results on the relationship between
problems $(P)$ and $(D)$.
These can be found in plenty of sources, for example 
\cite{lp:Chvatal,Schrijver86}.

\begin{lemma}[Weak duality]
Let $(x,y,s) \in \mathcal{F}$. Then $c^Tx \ge b^Ty$.
\end{lemma}

The weak duality property states that the primal and dual objective
values bound each other.
The difference $c^Tx - b^Ty$ is called {\em duality gap}.

Problem $(P)$ has a solution if and only if $\mathcal{P} \ne \emptyset$;
if also $\mathcal{D} \ne \emptyset$, then both problems admit an
optimal solution $(x^*, y^*, s^*)$, and the objective function 
values of both problems at that point coincide. This can be formalised
in the following lemma.

\begin{lemma}[Strong duality]
A point $x \in \mathcal{P}$ is an optimal solution if and only if
there exists a pair $(x,s) \in \mathcal{D}$ such that $c^Tx = b^Ty$.
\end{lemma}

If one of the sets $\mathcal{P}$ or $\mathcal{D}$ is empty, 
then the other is either unbounded or empty as well. 
In such a case, an optimal
solution for problem (\ref{eq:PrimalDualPair2}) does not exist,

In what follows, we make the standard assumption for the development
of interior point methods that $\mathcal{P}^0 \ne \emptyset$ and 
$\mathcal{D}^0 \ne \emptyset$. This is also referred to as the
{\em interior point assumption}. 
The interior point assumption corresponds to assuming that 
the primal--dual optimal face is 
bounded (this is mentioned in \cite{GonzagaCardia04} and also
in \cite[Lemma~2.2]{GulerRoosTerlakyVial}).
Cases when this assumption does not hold can be considered by allowing
the algorithm to accept infeasible iterates 
(see Section~\ref{sec:InfeasibleMethods}).

Optimality conditions let us recognise that a solution has been
found. They also provide insight on the development of algorithms 
for finding a solution.

The Karush-Kuhn-Tucker (KKT) conditions express first-order optimality 
conditions for the primal--dual pair (\ref{eq:PrimalDualPair}).
They can be written as
\be  \label{eq:KKT}
\begin{array}{rcl}
  Ax      &=& b \\
  A^Ty +s &=& c \\
  XSe     &=& 0 \\
  (x,s)   &\ge& 0,
\end{array}
\ee
where $X, S \in \R^{n\times n}$ are diagonal matrices with elements 
$x_i$ and $s_i$ respectively, and $e \in \R^n$ is a vector 
of ones. In other words, an optimal solution is characterised by 
primal feasibility, dual feasibility and complementarity.

Complementarity can be seen as a certificate for optimality 
in linear programming \cite{phd:Jansen,Schrijver86}.
For non-optimal feasible iterates, complementarity measures the distance of the
iterate to optimality:
\be  \label{eq:DualityComplementarityGap}
  c^Tx - b^T y = c^Tx - x^T A^T y = x^T(c - A^T y) = x^T s.
\ee
The quantity $x^T s$ is called {\em complementarity gap}:
when it is driven to zero, then a feasible solution is also optimal. 
It should be noted that the equality between duality gap and
complementarity gap of equation (\ref{eq:DualityComplementarityGap})
holds only for a feasible point.

%
%
\subsection{The barrier problem}

Many algorithms used in mathematical programming can be interpreted 
as path-following. Here we restrict our attention to the path described 
by the logarithmic barrier function in linear programming.
Given the linear program in standard form $(P)$,
it is possible to write the corresponding {\em barrier problem}:
\[
\begin{array}{crl}
         & \min        & c^Tx - \mu \sum_i \ln x_i \\
 (P_\mu) & \mbox{s.t.} & Ax = b, \\
         &             & x > 0.
\end{array}
\]

Problem $(P_\mu)$ denotes a family of problems parametrised by the 
quantity $\mu>0$ (typically small), called {\em barrier parameter} 
in the interior point literature. 
It is worth noting that such approach is 
viable only if it is actually possible to find a point that 
strictly satisfies the constraints, that is, if $\mathcal{P}^0 \ne \emptyset$.

The presence of the logarithmic barrier forces the iterates 
to stay in the interior of the feasible region, as this term heavily penalises 
the points that are too close to the boundary. However, the influence exerted
by the logarithmic barrier can be controlled through the penalty
parameter $\mu$.
The weight on the barrier regulates the distance from the iterates to 
the boundary: as $\mu$ tends to zero, problem $(P_\mu)$ resembles
more and more problem $(P)$.

The objective function of problem $(P_\mu)$
is a strictly convex function. 
Therefore, for a fixed $\mu$, the problem has at most one global minimum. 
The minimizer, if it exists, is completely characterised 
by the associated KKT conditions:
\[
\begin{array}{rcc}
   Ax               & = &  b \\
  \mu X^{-1}e +A^Ty & = &  c \\
   x                & > &  0.
\end{array}
\]
By substituting $s = \mu X^{-1}e$, we obtain the
standard (primal--dual) formulation of the so called 
{\em perturbed KKT conditions}:
\be \label{eq:PerturbedKKT}
\begin{array}{rcc}
   Ax       & = & b \\
   A^Ty + s & = & c \\
   XSe      & = & \mu e \\
   (x,s)    & > & 0.
\end{array}
\ee

\fb{
If the feasible domain $\mathcal{P}$ is bounded, 
then both $(P)$ and $(P_\mu)$ have optimal solution. 
}
The following lemma has been proved by Megiddo \cite{Megiddo}.
\begin{lemma}
Problem $(P_\mu)$ 
is either unbounded for every  $\mu>0$ or has a unique optimal 
solution for every $\mu>0$.
\end{lemma}

If the perturbed KKT system (\ref{eq:PerturbedKKT}) has a solution for 
any $\mu>0$, then it determines a unique continuous smooth curve 
$(x(\mu),y(\mu),s(\mu))$ toward the optimal set as $\mu\to 0$. 
In interior-point 
terminology, this curve is called the {\em central path}.
We postpone its presentation to Section~\ref{sec:CentralPath}.

%Moreover, if $A$ has full rank, then the value of $y$ is 
%uniquely determined by the value of $x$. Therefore, 
%system (\ref{eq:PerturbedKKT}) has a unique solution $(x(\mu),y(\mu))$.

Under the assumptions that for a particular $\mu > 0$ the point
$(x(\mu),y(\mu),s(\mu))$ is primal and dual feasible, we can state
a similar result to what is expressed by 
(\ref{eq:DualityComplementarityGap}):
\[
  g(\mu) = c^Tx(\mu) - b^T y(\mu) = x(\mu)^T s(\mu),
\]
that is, the duality gap corresponds to the complementarity gap.
Hence reducing either of them is identical.
Also, as $XSe - \mu e = 0$ implies $x_is_i = \mu$, $i = 1, \ldots, n$, 
we have
\be  \label{eq:AverageComplementarity}
   s(\mu)^T x(\mu) = n\mu,
\ee
and for $\mu \to 0$, also $g(\mu) \to 0$.
Megiddo \cite{Megiddo} shows that $c^Tx(\mu)\to c^Tx^*$ as $\mu\to 0$. 
Furthermore, he proves the following, stronger result.
%
\begin{lemma}
\label{th:CentralPathConvergence}
Under the assumptions of primal feasibility, dual feasibility, and
full row rank of matrix $A$, then
\[
   x(\mu) \to x^*, \quad (y(\mu),s(\mu)) \to (y^*, s^*)
\]
as $\mu \to 0$.
\end{lemma}

%
%
\subsection{Solving the perturbed KKT conditions}

Primal--dual path-following methods solve the perturbed KKT
conditions (\ref{eq:KKT}) by asking the complementarity pairs to align 
to a specific barrier parameter $\mu > 0$,
\be  \label{eq:PerturbedComplementarity}
XSe = \mu e,
\ee
while enforcing $(x,s)>0$.
However, up to now, we have not defined how to choose the
barrier parameter $\mu$ and how to update it at each iteration.

\fb{
Explain how we choose it at the beginning.
}

At each iteration, $\mu$ is monotonically decreased by the quantity
$\sigma \in (0,1)$, called {\em centering parameter} for reasons that
will become clear later on.
Hence, the perturbed KKT conditions (\ref{eq:PerturbedKKT}) 
approximate better and better
the system (\ref{eq:KKT}) of optimality conditions for the original
problem.
The choice of the centering parameter $\sigma$ 
is algorithm-dependent. We provide some theoretical insights on some
possible choices in Section~\ref{sec:TheoreticalResults}.

\ignore{
In all predictor-corrector algorithms there is a crucial decision 
to be made at every iteration, namely the choice of the penalty 
parameter $\mu$ to be used in the correction.

The paper \cite{VillasBoasPerin} tries to answer this question. 
They build a polynomial function of $\mu$ and $\alpha$, and they 
use to determine what the optimal choices of these parameters are, 
under a suitably chosen measure.

By using this strategy they achieve a better iteration count on 
most of the problems in their experiment. This, however, is not 
supported by a corresponding reduction in computational time. 
The reason for this is that the postponing of the choice of $\mu$ 
requires the solution of additional systems. While it's true that 
the most expensive operation is the computation of the Cholesky 
factors, the actual cost of the backsolves is not negligible. 
In the results of this paper, the additional cost of the extra 
backsolves is bigger than the savings obtained by the decrease 
in number of iterations.
}

Path-following interior point methods seek a solution 
to the system of equations (\ref{eq:PerturbedKKT})
\[
F(x,y,s) = \left[
  \begin{array}{c}
    Ax-b \\
    A^Ty+s-c \\
    XSe - \sigma\mu e \\
  \end{array} \right] = 0,
\]
which is nonlinear in the perturbed complementarity constraints.
We use Newton's method to linearise the system around the 
current point according to
\[
\nabla F(x,y,s) \Delta(x,y,s) = -F(x,y,s),
\]
and obtain the so-called step equations
%
\be \label{eq:NewtonSystem}
\left[ \begin{array}{ccc}
    A & 0 & 0 \\ 0 & A^T & I \\ S & 0 & X
  \end{array} \right]
\left[ \begin{array}{c}
    \Delta x \\  \Delta y \\  \Delta s
  \end{array} \right] =
\left[ \begin{array}{c}
    b - Ax \\ c - A^Ty - s \\ -XSe + \sigma\mu e
   \end{array} \right] =
\left[ \begin{array}{c}
    \xi_b \\ \xi_c \\ \xi_\mu
   \end{array} \right],
\ee
%
which need to be solved for a search direction
$\Delta w = (\Delta x, \Delta y, \Delta s)$,
with $\mu = x^Ts/n$, $\sigma \in (0,1)$.
For a feasible algorithm $\xi_b = \xi_c = 0$.

The solution of system (\ref{eq:NewtonSystem}) is the computationally
dominant step in each iteration of an interior point algorithm.
Throughout this thesis, we will 
restrict our attention to using a direct approach in solving these
equations.
We should note, however, that a wealth of research explored the use
of iterative methods in the computation of the search direction
\cite{BergamaschiGondzioZilli,OliveiraSorensen05}.

System (\ref{eq:NewtonSystem}) is usually reduced to two other
formulations by exploiting the block structure of its
matrix.
%
The {\em augmented system} formulation is obtained by using 
the last row of (\ref{eq:NewtonSystem}) to eliminate
$\Delta s = X^{-1} (\xi_\mu - S\Delta x)$.
This produces
%
\be \label{eq:AugmentedSystem}
\left[ \begin{array}{cc}
    -X^{-1}S & A^T \\ A & 0
  \end{array} \right]
\left[ \begin{array}{c}
    \Delta x \\  \Delta y
  \end{array} \right] =
\left[ \begin{array}{c}
    \xi_c - X^{-1}\xi_\mu \\ \xi_b
   \end{array} \right],
\ee
which is a symmetric but indefinite system.
%
By further eliminating $\Delta x$, we reduce system 
(\ref{eq:AugmentedSystem}) to the set of {\em normal equations}
%
\be \label{eq:NormalEquations}
  A D^2 A^T \Delta y = A D^2 (\xi_c - X^{-1} \xi_\mu) + \xi_b,
\ee
%
where we introduced the notation $D^2 = S^{-1} X$.
Under the assumption of full row rank for $A$, matrix 
$A D^2 A^T$ is positive definite, since $D^2_i = x_i/s_i > 0$ for
all $i = 1, \ldots, n$.

Besides the issue of definiteness, the two formulations differ in
terms of sparsity and conditioning, the normal equations usually 
being denser and worse conditioned.
The choice between augmented system and normal equations depends also on 
the relative density of $AD^2A^T$ with respect to $A$.
Normal equations are to be avoided when there are dense columns in $A$, 
as they generate dense blocks in $AD^2A^T$.

The augmented system formulation requires particular attention 
in the development of linear algebra routines for the fact that
it presents an indefinite matrix. This raises problems of numerical
stability, and an accurate choice of pivoting strategies is fundamental.
Maros and M\'esz\'aros \cite{MarosMeszaros} presented an in-depth 
study of the properties of the augmented system formulation.
In the broad context of weighted least squares computations, the choice
between augmented system and normal equations has been studied long
before the development of interior point methods, see for example
\cite{DuffErismanReid86}.

%
%
\subsection{The central path}
\label{sec:CentralPath}

The study of the primal--dual properties of central path was pioneered by
Megiddo \cite{Megiddo} and Bayer and Lagarias \cite{BayerLagarias}.

We have seen in Lemma~\ref{th:CentralPathConvergence}
that as $\mu \to 0$ the solution to $(P_\mu)$ 
converges to the optimal solutions of $(P)$.
More formally, the central path leads to a solution which is
characterised by strict complementarity.

\begin{theorem}[Strict complementarity]
If $(P)$ and $(D)$ are feasible, then there exist a point $x^* \in\mathcal{P}$
and a pair $(y^*,s^*) \in \mathcal{D}$ such that
\[
(x^*)^T s^* = 0 \quad\mbox{ and }\quad x^*_i + s_i^* >0, \;\; i = 1, \ldots, n.
\]
\end{theorem}

A solution $(x^*,s^*)$ that satisfies the above theorem is called 
{\em strictly complementary}. 
On the grounds of a strictly complementary
solution we can define the concept of {\em optimal partition}.
Following Jansen \cite{phd:Jansen}, we define the support set
of a vector $v \in \R^n$ as
\[
   \sigma(v) = \{ i : v_i > 0, \; i=1,\ldots,n \},
\]
and partition the set of indices $\{1,\ldots,n \}$ as
\[
   \mathcal{B} = \sigma(x^*), \quad \mathcal{M} = \sigma(s^*).
\]
This partition is well-defined in the sense that a 
strictly complementary solution satisfies both 
$\mathcal{B} \cap \mathcal{M} = \emptyset$ and 
$\mathcal{B} \cup \mathcal{M} = \{1,\ldots,n \}$. 
A proof of the latter result, also known as Goldman-Tucker theorem, 
can be found in \cite{ipm:Wright97}.
The concepts of strict complementarity and optimal partition are
recurrent motifs in the analysis of interior point methods.

Vavasis and Ye \cite{VavasisYe} studied the properties of the 
curvature of the central path, discovering that the central path
is characterised by $\bigO(n^2)$ curves of high degree and
segments where it is relatively straight.
Such curves appear in correspondence with changes in the optimal
partition.
Close to the end, when the optimal partition has been identified,
the central path becomes a straight line \cite{Megiddo}.
In this region, the algorithm displays the quadratic convergence 
property typical of Newton's method.

We now consider
the limit of $(P_\mu)$ for $\mu \to \infty$, and therefore
find the point from which the central path departs.
This corresponds to finding the point that minimizes the barrier function:
\[
\hat{x} = \arg \min_{x \in \mathcal{P}^0} \big(-\sum_i \ln x_i \big).
\]
The point $\hat{x}$ is the {\em analytic center} of the feasible polytope, 
and was first studied by Sonnevend \cite{Sonnevend86}.
Given the strict convexity of the barrier function, the concept 
of analytic center is well defined.
As the analytic center minimizes the barrier, it
is the point farthest away from the boundary.

However, there is a problem with defining the central path in terms
of analytic center: the central path is affected by the presence of 
redundant constraints. 
This happens because it is an exclusively analytic concept, which does
not exploit geometric considerations.
Other type of centers (center of gravity, center of the ellipsoid of 
maximum volume that can be inscribed in $\mathcal{P}$, volumetric center) 
can be defined, but they usually are too demanding to compute 
\cite{Gonzaga92}. 

\ignore{
See a chapter in Ye's book.
Terlaky's  Klee-Minty example.
}

%
%
\subsection{Neighbourhoods of the central path}
\label{sec:Neighbourhoods}

As we have seen, following the central path is the recommended
way of traversing the interior of the feasible region towards
the optimal solution. Nevertheless, it should be clear that keeping the
iterates {\em exactly on} the central path is an unachievable aim.
Finding a point that solves the perturbed complementarity conditions 
(\ref{eq:PerturbedComplementarity}) for a specific $\mu$ 
is as difficult as solving the optimization problem itself.
%
Therefore, we never insist on this extremely restrictive requirement,
but we rather allow the iterates to be somewhere around the central path.
This leads to the introduction of the important concept of
{\em neighbourhood} of the central path. 
We can define several neighbourhoods, characterised
by different properties.
Two neighbourhoods are often used in theoretical developments.

The first is based on the Euclidean norm, and it is often referred
to as the {\em tight neighbourhood}:
\[
\Nhood_2(\theta) = \{ (x,y,s) \in \mathcal{F}^0 :
                         \| XSe - \mu e \|_2 \le \theta\mu \},
\]
where $\theta \in (0,1)$.
This neighbourhood defines points which lie very close to the central path:
search directions generated from points in this neighbourhood can be 
followed with full step, and the barrier parameter can be decreased
by a small amount at each iteration (giving rise to the name
of {\em short-step algorithms} to the algorithms that are based on
this neighbourhood). 
The closeness to the central path that the tight neighbourhood
imposes and maintains allows to produce the best convergence result
for linear programming: short-step algorithms converge in 
$\bigO(\sqrt{n}L)$ iterations.
However, since the reduction in the barrier parameter at each iteration 
is very small, the practical value of short-step algorithms is small.

The other commonly used neighbourhood is instead based on the infinity norm, 
and it is often called {\em wide neighbourhood}:
\[
\Nhood_{-\infty}(\gamma) = \{ (x,y,s) \in \mathcal{F}^0 :
                         x_is_i \ge \gamma\mu, \;\; i = 1, \ldots, n \},
\]
where $\gamma \in (0,1).$
Algorithms based on such a neighbourhood are allowed to generate
iterates that follow more loosely the central path. The iterates 
have more freedom of movement as they can approach the boundary
of the feasible set.
Hence, algorithms based on the wide neighbourhood (usually denoted as
{\em long-step algorithms}) are less conservative and can decrease 
the barrier parameter more rapidly.
However, the Newton direction computed from points in the wide 
neighbourhood  has weaker properties, and a linesearch procedure is
needed to ensure that the positivity of the $(x,s)$ iterates is
preserved.

In Section~\ref{sec:SymNeighbourhood} we will study a variation
of the $\Nhood_{-\infty}$ neighbourhood which better describes
the centrality requirements needed for a practical algorithm.

\ignore{
$N_2(\beta) \subseteq N_\infty(\beta) (= \| XSe - \mu e\|_\infty) \subseteq N_{-\infty}(\beta)$
}


%
% Section
%
\section{Theoretical results}
\label{sec:TheoreticalResults}

Theoretical developments aim at lowering the upper bound on the number 
of steps needed for convergence. The results provided by such worst-case 
complexity analysis are informative but exceedingly pessimistic. 

Theoretical proofs of complexity generally follow a common scheme.
First they rely on a computable measure of the closeness to the central
path: this is accomplished by the concept of neighbourhood. Second,
they show that the direction computed by solving the Newton system
(\ref{eq:NewtonSystem}) can be followed with a strictly positive step
(and therefore guarantees some progress) and generates an iterate 
that retains the property of being in some neighbourhood of the central 
path (possibly larger than the one before). Finally, they require
a decrease in the barrier parameter that allows to derive a polynomial upper
bound on the number of iterations.

The theoretical analysis for the logarithmic barrier method was
initiated by Megiddo \cite{Megiddo}.
On this basis, Kojima, Mizuno and Yoshise \cite{KojimaMizunoYoshise89} 
proposed a polynomial-time algorithm with the property of convergence 
in $\bigO(nL)$ iterations.
This was refined by the same group \cite{KojimaMizunoYoshise89b} 
and by Monteiro and Adler \cite{MonteiroAdler89a}:
both papers presented a primal--dual algorithm for linear programming 
based on the tight $\Nhood_2$ neighbourhood
with the property of convergence in $\bigO(\sqrt{n}L)$ iterations.
This is still the best complexity result for interior point methods
for linear programming.

%
%
\subsection{Feasible methods}

A feasible algorithm is characterised by the requirement that
all primal and dual iterates always lie within the interior 
of the feasible region. For this reason, these algorithms 
need to start from a strictly feasible point. 

A short-step feasible method based on the $\Nhood_2$ neighbourhood
reduced the barrier parameter by 
\[
   \sigma = 1 - \delta/\sqrt{n}
\]
at each iteration, for some positive constant $\delta$.

In complexity proof, the barrier parameter is reduced slightly at
each iteration in order to guarantee that one iteration of Newton's
method can keep the point in the tight neighbourhood of the central path.
However, this is a worst-case analysis, and in practice the same would happen
even for a bigger update of the barrier parameter. This brings to
study ways of allowing a more substantial reduction of the barrier
parameter, at least in some iterations. 

One important result is this direction was obtained by 
Mizuno, Todd and Ye \cite{MizunoToddYe}, who analysed the short-step 
predictor--corrector method. Their strategy uses two nested neighbourhoods 
$\Nhood_2(\theta^2)$ and $\Nhood_2(\theta)$, $\theta \in (0,1)$, 
and exploits the
quadratic convergence property of Newton's method in such a tight
neighbourhood of the central path.
Their algorithm alternates between two search directions characterised by
different properties.
First, by choosing $\sigma = 0$ in (\ref{eq:NewtonSystem}),
the predictor direction gains optimality, possibly at the expense of
worsening the centrality, keeping the iterate in a larger neighbourhood
$\Nhood_2(\theta)$ of the central path. 
Then, a pure re-centering step is performed, by setting $\sigma = 1$,
leaving the duality gap unchanged but moves the iterate back into a 
tighter $\Nhood_2(\theta^2)$ neighbourhood. Hence, every second step the 
algorithm produces a point in $\Nhood_2(\theta^2)$.

An important contribution of this technique, is the idea 
of targeting optimality and centrality independently. 
However, the use of the very restrictive $\Nhood_2$ neighbourhood 
makes it unattractive for practical applications.
While this 
algorithm is not effective in practical terms, it provides a scheme 
upon which more computationally attractive methods can be constructed,
as we will discuss in Chapter~\ref{ch:PracticalIpm}.

Thanks to the optimizing predictor direction which is identical
to a short-step feasible method, also the predictor--corrector
algorithm converges in $\bigO(\sqrt{n}L)$ iterations, with the only
difference that the value of the barrier parameter is reduced over
two iterations.

%
%
\subsection{Infeasible methods}
\label{sec:InfeasibleMethods}

\ignore{
The results presented up to here concentrated on feasible methods. 
For these methods we assume to have a strictly feasible starting point
readily available.
}

Finding a strictly feasible starting point is, in general, a nontrivial task.
Solving the feasibility problem is an optimization problem in its
own right, and is as difficult as solving the original problem
to optimality.
Moreover, the feasible region may have empty interior,
in which case the theory developed above does not apply.
For these reasons, a need exists for practical 
implementations to dispense from this requirement.

A way to recover a strictly feasible starting point involves 
solving an artificial Phase~I subproblem by using 
the big-$M$ method. However, the performance is dependent on 
the choice of the values given to the weights, and the use
of very large values, while theoretically satisfying,
causes numerical instabilities \cite{Lustig91}.
This is worsened 
by the presence of dense columns that compromises the 
computational efficiency. 

A very different approach is based on the homogeneous 
self-dual formulation \cite[ch.~9]{ipm:Wright97}.
The self-dual formulation wraps the optimization problem into one 
of larger dimension, but for which a feasible solution is known 
from the start.
Also, the homogeneous self-dual formulation has the very
appealing property of being able to detect infeasibility
with accuracy.

\fb{
The use of a self-dual formulation comes with a price from a
computational viewpoint, particularly because of the need of one 
extra backsolve at each iteration.
Also one more variable in the system.
It works only for linear programming.
Stability issues?
Find a reference for that (Terlaky-Ye?).
}

It is possible to develop an algorithm which only requires 
the $x$ and $s$ components to be strictly positive. 
This was initiated by Lustig \cite{Lustig91}, who proposed some
new feasibility restoration directions.

\fb{
However, was the work of Kojima, Megiddo and Mizuno \cite{KojimaMegiddoMizuno}
that introduced a complete infeasible algorithm, with full
theoretical analysis of convergence.

Sure? Wright (page 109) says it was Lustig Marsten and Shanno.
}

In such 
an algorithm, all iterates are infeasible, but the limit points 
are feasible and optimal. This is obtained by using a 
neighbourhood that admits infeasible points:
\[
\Nhood_{-\infty}(\gamma,\beta) =\{ (x,y,s) | \; 
           \|(\xi_b,\xi_c)\| \le \beta\mu \frac{\|(\xi_b^0,\xi_c^0)\|}{\mu_0}, 
	   (x,s)>0, x_is_i \ge \gamma\mu \},
\]
where $\gamma\in (0,1)$ and $\beta \ge 1$ are parameters, and 
we denoted the primal and dual residuals, respectively, by 
$\xi_b = Ax-b$ and $\xi_c = A^T y + s - c$.

Therefore, there is no strict feasibility requirement for 
the iterates; however, the residuals at each iteration must be 
bounded above by a multiple of the duality measure $\mu$. 
By reducing $\mu$ we can force the primal and dual residuals 
$\xi_b$ and $\xi_c$ to zero, thus approaching complementarity and 
feasibility at the same speed.

Kojima, Megiddo and Mizuno \cite{KojimaMegiddoMizuno} proposed 
a stepsize rule that guarantees global convergence of an 
infeasible interior-point algorithm.
An algorithm is globally convergent if it is possible to choose
a strictly positive stepsize such that the complementarity gap
is reduced at each iteration. This property is very important, at it
guarantees the good behaviour of the algorithm for any given starting
point. However, in order to prove it, it is required that some
safeguards are implemented:
\begin{itemize}
\item reduction in infeasibility should be faster than in complementarity
\item $x_i s_i \ge \beta x^Ts$ for a chosen $\beta$, and all iterates must
stay in this neighbourhood.
\end{itemize}

Let $(x', y', s') = (x^0, y^0, s^0) + \alpha(\Delta x, \Delta y, \Delta s)$, 
then:
\begin{itemize}
\item $\xi'_P = (1-\alpha) \xi^0_P$
\item $\xi'_D = (1-\alpha) \xi^0_D$
\item $x'^Ts' = (1-\alpha (1 -\sigma))(x^0)^Ts^0 +\alpha^2 \Delta x^T \Delta s$
\end{itemize}
so the reduction in infeasibilities is linear, while complementarity reduces
for a small $\alpha$. When feasibility is restored, the reduction in the
complementarity measures becomes linear as well, as $\Delta x^T \Delta s = 0$.

\fb{
Order of convergence: $\bigO(n^2L)$ in Wright.
}


%
% Section
%
\section{Symmetric neighbourhood}
\label{sec:SymNeighbourhood}

The quality of centrality (understood in a simplified way as complementarity)
for a practical implementation of an interior point algorithm
is {\it not} well characterised by either of two neighbourhoods 
$\Nhood_2$ or $\Nhood_{-\infty}$ commonly used in theoretical developments 
of interior point methods.

The $\Nhood_2$ neighbourhood is too restrictive, and the short-step methods
based on it are too conservative and, ultimately, very slow.
The $\Nhood_{-\infty}$ provides a much better framework, as it allows to
reduce the barrier parameter quickly. However, at it does not actively
enforce an upper bound on the complementary products, it may allow the
iterates to produce very unbalanced products.

\fb{
The issue of unbalanced complementary products is very important for
the practical success of an interior point code. We should note how
the complementary pairs play a role in the Newton system, and how
their bad scaling causes a bad behaviour of Newton's method, which
produces unreliable directions.
}

We propose to use a symmetric neighbourhood $\Nhood_s(\gamma)$,
in which complementarity pairs have to satisfy 
$\gamma \mu \leq x_i s_i \leq \gamma^{-1} \mu$, where $\gamma \in (0,1)$, 
for a strictly feasible iterate $(x,y,s)$.

\fb{
Jansen \cite{JansenRoosTerlaky96} (or actually \cite{phd:Jansen}?)
proposed to use the ratio 
between the smallest and the largest complementarity pair as an 
indication of the quality of a point.

It mentions that was Atkinson and Vaidya \cite{AtkinsonVaidya} 
that noticed that as the ratio increases, the region in which 
Newton converges becomes smaller.
}

Practical experience with the primal--dual algorithm in HOPDM \cite{HOPDM}
suggests that one of the features responsible 
for its efficiency is the way in which the quality of centrality 
is assessed. By ``centrality'' we understand here the spread 
of complementarity products $x_i s_i$, $i = 1,\dots,n$.
Large discrepancies within the complementarity 
pairs, and therefore bad centering, create problems for the search 
directions: an unsuccessful iteration is caused not only by small
complementarity products, but also by very large ones.
%
 This can be explained by the fact that
Newton's direction tries to compensate for very 
large products, as they provide the largest gain in complementarity 
gap when a full step is taken. However, the direction thus generated 
may not properly consider the presence of very small products, 
which then become blocking components when the stepsizes are computed.

The notion of spread in complementarity products
is not well characterised by either 
of the two neighbourhoods $\Nhood_2$ or $\Nhood_{-\infty}$ commonly used 
in theoretical developments of IPMs.
To overcome this disadvantage, here we formalise a variation 
on the usual $\Nhood_{-\infty}(\gamma)$ neighbourhood, 
in which we introduce an upper bound on the complementary pairs. 
This neighbourhood was implicitly used in \cite{Gondzio96}
to define an achievable target for multiple centrality correctors
(we refer the reader to Section~\ref{sec:MultipleCC}).

We define the symmetric neighbourhood to be the set
\[
  \Nhood_s(\gamma)=\{(x,y,s)\in \mathcal{F}^0: 
  \gamma\mu\le x_is_i \le \frac{1}{\gamma}\mu, \; i=1,\ldots,n\},
\]
where $\mathcal{F}^0$ 
is the set of strictly feasible primal--dual points,
$\mu = x^Ts/n$, and $\gamma \in (0,1)$.

While the $\Nhood_{-\infty}$ neighbourhood ensures that some 
products do not approach zero too early, it does not prevent products
from becoming too large with respect to the average.
In other words, it does not provide a complete 
picture of the centrality of the iterate. The symmetric 
neighbourhood $\Nhood_s$, on the other hand, promotes 
the decrease of complementarity pairs which are too large, thus taking 
better care of centrality.

\hrulefill

An upper bound is implicit in the $\Nhood_{-\infty}$ neighbourhood. To find it,
suppose that all but one complementarity products are at the lower bound
$\gamma\mu$:
\[
  n\mu = (n-1)\gamma\mu + x_n s_n,
\]
from which it follows that in general
\be  \label{eq:UpperBoundN8hood}
  x_i s_i \le (n (1-\gamma) + \gamma) \mu, \quad i = 1, \ldots, n.
\ee
We note the dependence on the problem dimension in the definition of the
upper bound, hence its uneffectiveness for large-scale problems.

We can also determine the value of $n$ for which the upper bound of
the symmetric neighbourhood crosses the implicit upper bound
(\ref{eq:UpperBoundN8hood}), that is
\[
  n(1-\gamma) + \gamma > \frac{1}{\gamma},
\]
which means that the symmetric neighbourhood has an effective bound
for
\[
  n > \frac{1+\gamma}{\gamma}.
\]

%
%
\subsection{Analysis}

The analysis is done for the long-step feasible path-following 
algorithm, where the search direction $(\Delta x, \Delta y, \Delta s)$ 
is found by solving system (\ref{eq:NewtonSystem}) with 
$r=(0,\; 0,-XSe+\sigma\mu e)^T$, $\sigma\in(0,1)$, $\mu=x^Ts/n$.
%
The exposition follows closely the presentation of 
\cite[Chapter~5]{ipm:Wright97}. 

First we need a technical result, the proof of which can be found 
in \cite[Lemma~5.10]{ipm:Wright97} and is unchanged by the use 
of $\Nhood_s$ rather than $\Nhood_{-\infty}$.
%
\begin{lemma} \label{Wright:5.10}
If $(x,y,s)\in \Nhood_s(\gamma)$, then\,
\(
  \|\Delta X\Delta Se\| \le 2^{-3/2}\left( 1+ \displaystyle{\frac{1}{\gamma}} \right)n\mu.
\)
\end{lemma}

Our main result is presented in Theorem~\ref{th:SymNeighbourhood}. 
We prove that it is possible to find a strictly positive stepsize 
$\alpha$ such that the new iterate 
$(x(\alpha),y(\alpha),s(\alpha))=(x,y,s)+\alpha(\Delta x,\Delta y,\Delta s)$
will not leave the symmetric neighbourhood, and thus this 
neighbourhood is well defined. This result extends 
Theorem~5.11 in \cite{ipm:Wright97}.

\begin{theorem} \label{th:SymNeighbourhood}
  If $(x,y,s)\in\Nhood_s(\gamma)$, then 
  $\left(x(\alpha),y(\alpha),s(\alpha)\right)\in\Nhood_s(\gamma)$ for all
  \[
  \alpha\in \left[0,2^{3/2}\gamma\frac{1-\gamma}{1+\gamma}\frac{\sigma}{n} \right].
  \]
\end{theorem}
\begin{proof}
Let us express the complementarity product in terms of the stepsize 
$\alpha$ along the direction $(\Delta x, \Delta y, \Delta s)$:
%
\begin{eqnarray} \label{eq:CompProdAlpha}
x_i(\alpha)s_i(\alpha)&=&(x_i+\alpha\Delta x_i)(s_i+\alpha\Delta s_i)\nonumber\\ 
&=& x_is_i+\alpha(x_i\Delta s_i +s_i\Delta x_i) +\alpha^2\Delta x_i\Delta s_i\\
&=& (1-\alpha)x_is_i + \alpha\sigma\mu + \alpha^2\Delta x_i\Delta s_i.\nonumber
\end{eqnarray}
%
We need to study what happens to this complementarity product 
with respect to both bounds of the symmetric neighbourhood.
%
Let us first consider the bound $x_is_i\le \frac{1}{\gamma}\mu$.
By Lemma~\ref{Wright:5.10}, equation (\ref{eq:CompProdAlpha}) implies
\[
x_i(\alpha)s_i(\alpha) \le (1-\alpha)\frac{1}{\gamma}\mu +\alpha\sigma\mu 
+ \alpha^2 2^{-3/2}\left( 1+ \frac{1}{\gamma} \right)n\mu.
\]
At the new point $(x(\alpha),y(\alpha),s(\alpha))$, the new duality gap
is $x(\alpha)^Ts(\alpha) = n\mu(\alpha)$.
The relation $x_i(\alpha)s_i(\alpha)\le \frac{1}{\gamma}\mu(\alpha)$ 
holds provided that
\[
(1-\alpha)\frac{1}{\gamma}\mu +\alpha\sigma\mu + \alpha^2 2^{-3/2}\left( 1+ \frac{1}{\gamma} \right)n\mu 
\le\frac{1}{\gamma}(1-\alpha+\alpha\sigma)\mu,
\]
from which we derive a first bound on the stepsize:
\[
\alpha \le 2^{3/2}\frac{1-\gamma}{1+\gamma}\frac{\sigma}{n} = \bar\alpha_1.
\]
%
Considering now the bound $x_is_i\ge \gamma\mu$ and proceeding as before,
%By Lemma~\ref{Wright:5.10}, equation (\ref{eq:CompProdAlpha}) implies
%\[
%x_i(\alpha)s_i(\alpha) \ge (1-\alpha)\gamma\mu + \alpha\sigma\mu 
%- \alpha^2 2^{-3/2}\left( 1+ \frac{1}{\gamma} \right)n\mu.
%\]
%Hence, $x_i(\alpha)s_i(\alpha)\ge \gamma\mu(\alpha)$ provided that
%\[
%(1-\alpha)\gamma\mu + \alpha\sigma\mu- \alpha^2 2^{-3/2}\left( 1+ \frac{1}{\gamma} \right)n\mu 
%\ge\gamma(1-\alpha+\alpha\sigma)\mu,
%\]
we derive a second bound on the stepsize:
\[
\alpha\le 2^{3/2}\gamma\frac{1-\gamma}{1+\gamma}\frac{\sigma}{n} =\bar\alpha_2.
\]

Therefore, we satisfy both bounds and guarantee that 
$(x(\alpha),y(\alpha),s(\alpha))\in \Nhood_s(\gamma)$ if
\[
\alpha \in [0,\min(\bar\alpha_1,\bar\alpha_2)],
\]
which proves the claim, as $\gamma \in (0,1)$.
\end{proof}

It is interesting to note that the introduction of the upper bound 
on the complementarity pairs does not change the polynomial complexity 
result proved for the $\Nhood_{-\infty}(\gamma)$ neighbourhood 
\cite[Theorem~5.12]{ipm:Wright97}. Therefore, the symmetric 
neighbourhood provides a better practical environment without any 
theoretical loss. This understanding provides some additional 
insight into the desired characteristics of a well-behaved iterate.

\hrulefill

The use of the symmetric neighbourhood will be one of the theoretical
motivations of this work. Through it, in Section~\ref{sec:MultipleCC}
we will put the work of \cite{Gondzio96} inside a more sound framework.
Then we will use it again in the presentation of the weighted corrector
directions of Chapter~\ref{ch:Correctors}, and in the analysis of an 
original warm-start strategy for stochastic programming of 
Chapter~\ref{ch:Warmstart}.
