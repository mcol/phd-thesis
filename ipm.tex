%Started on 9th August 2006
%Aug: 11, 22, 23, 24, 25, 28, 29, 30, 31
%Sep:  1,  7,  8
% 2007
%Jan: 15, 16, 17, 22, 26, 29, 30
%Feb:  1,  2,  4,  5,  6,  7,  8,  9, 11, 21, 22
%Mar:  8,  9, 13, 15, 16, 17

%
% Chapter: Interior point methods
%
\label{ch:Ipm}

Interior point methods are well-suited to solving very
large scale optimization problems. Their theory is well understood
and a number of survey papers and academic books are available
\cite{AndersenGondzioMeszarosXu,GondzioTerlaky,Gonzaga92,
RoosTerlakyVial,MWright92,ipm:Wright97}.

This chapter is devoted to the derivation and analysis of primal--dual
path-following interior point methods. 
We present the elements that are at the base of this successful class
of algorithms, concentrating on their theoretical properties and
attractive features.
We also introduce and analyse the concept of symmetric neighbourhood,
and its consequences for practical algorithms.


%
% Section
%
\section{Derivation of primal--dual interior point methods}
\label{sec:Derivation}

Consider the following primal--dual pair of linear programming problems 
in standard form
%
\begin{eqnarray} \label{eq:PrimalDualPair}
  \begin{array}{crlp{2cm}crl}
     & \mbox{ min } & c^T x     &  &     & \mbox{ max }  & b^T y \\
 (P) &\mbox{ s.t. } & Ax = b,   &  & (D) & \mbox{ s.t. } & A^T y + s = c, \\
     &              & x \geq 0; &  &     &   & y \mbox{ free,} \;\; s \geq 0,
  \end{array}
\end{eqnarray}
%
where $A \in \R^{m \times n}$, $x, s, c \in \R^{n}$ 
and $y, b \in \R^{m}$, $m<n$. We assume, without loss of generality,
that $A$ has full row rank, as linearly dependent rows can be
removed without changing the solution set.
This implies that a feasible $s \ge 0$ determines in a unique
way the value of $y$.
In fact, the $y$ variables can be eliminated thus producing the
symmetric combined primal--dual form studied by Todd and Ye \cite{ToddYe90}.

We define the sets of primal feasible points and of
primal interior points 
\[
\mathcal{P} = \{ x : Ax = b, \; x \ge 0 \}, \quad
\mathcal{P}^0 = \{ x \in \mathcal{P} : x > 0 \},
\]
and, similarly, the sets of dual feasible points and of
dual interior points
\[
\mathcal{D} = \{ (y,s) : A^T y + s = c, \; s \ge 0 \}, \quad
\mathcal{D}^0 = \{ (y,s) \in \mathcal{D} : s > 0 \}.
\]
The set of feasible primal--dual points is therefore
$\mathcal{F} = \mathcal{P} \times \mathcal{D}$, and the set of primal--dual
interior points is
\[
\mathcal{F}^0 = \{ (x,y,s) \in \mathcal{F} : (x,s) > 0 \}.
\]
A point $(x,y,s) \in \mathcal{F}^0$ is said to be {\em strictly feasible}
for the primal--dual pair (\ref{eq:PrimalDualPair}).

Using this notation, the primal--dual pair (\ref{eq:PrimalDualPair}) can 
be written as
\be \label{eq:PrimalDualPair2}
\min \; c^T x, \;\;  x    \in \mathcal{P}; \qquad
\max \; b^T y, \;\; (y,s) \in \mathcal{D}.
\ee

We recall here some well-known results on the relationship between
problems $(P)$ and $(D)$.
These can be found in plenty of sources, for example 
\cite{lp:Chvatal,Schrijver86}.

\begin{lemma}[Weak duality]
Let $(x,y,s) \in \mathcal{F}$. Then $c^Tx \ge b^Ty$.
\end{lemma}

The weak duality property states that the primal and dual objective
values bound each other.
The difference $c^Tx - b^Ty$ is called {\em duality gap}.

Problem $(P)$ has a solution if and only if $\mathcal{P} \ne \emptyset$;
if also $\mathcal{D} \ne \emptyset$, then both problems admit an
optimal solution $(x^*, y^*, s^*)$, and the objective function 
values of both problems at that point coincide. This can be formalised
in the following lemma.

\begin{lemma}[Strong duality]
A point $x \in \mathcal{P}$ is an optimal solution if and only if
there exists a pair $(x,s) \in \mathcal{D}$ such that $c^Tx = b^Ty$.
\end{lemma}

If one of the sets $\mathcal{P}$ or $\mathcal{D}$ is empty, 
then the other is either unbounded or empty as well. 
In such a case, an optimal
solution for problem (\ref{eq:PrimalDualPair2}) does not exist.

In what follows, we make the standard assumption for the development
of interior point methods that $\mathcal{P}^0 \ne \emptyset$ and 
$\mathcal{D}^0 \ne \emptyset$. This is also referred to as the
{\em interior point assumption}. 
The interior point assumption corresponds to assuming that 
the primal--dual optimal face is 
bounded (this is mentioned in \cite{GonzagaCardia04} and also
in \cite[Lemma~2.2]{GulerRoosTerlakyVial}).
Cases when this assumption does not hold can be considered by allowing
the algorithm to accept infeasible iterates 
(see Section~\ref{sec:InfeasibleMethods}).

Optimality conditions let us recognise that a solution has been
found. They also provide insight on the development of algorithms 
for finding a solution.
The Karush-Kuhn-Tucker (KKT) conditions express first-order optimality 
conditions for the primal--dual pair (\ref{eq:PrimalDualPair}).
They can be written as
\be  \label{eq:KKT}
\begin{array}{rcl}
  Ax      &=& b \\
  A^Ty +s &=& c \\
  XSe     &=& 0 \\
  (x,s)   &\ge& 0,
\end{array}
\ee
where $X, S \in \R^{n\times n}$ are diagonal matrices with elements 
$x_i$ and $s_i$ respectively, and $e \in \R^n$ is a vector 
of ones. In other words, an optimal solution is characterised by 
primal feasibility, dual feasibility and complementarity.

Complementarity can be seen as a certificate for optimality 
in linear programming \cite{phd:Jansen,Schrijver86}.
For non-optimal feasible iterates, complementarity measures the distance of the
iterate to optimality:
\be  \label{eq:DualityComplementarityGap}
  c^Tx - b^T y = c^Tx - x^T A^T y = x^T(c - A^T y) = x^T s.
\ee
The quantity $x^T s$ is called {\em complementarity gap}:
when it is driven to zero, then a feasible solution is also optimal. 
It should be noted that the equality between duality gap and
complementarity gap of equation (\ref{eq:DualityComplementarityGap})
holds only for a feasible point.

%
%
\subsection{The barrier problem}

Many algorithms used in mathematical programming can be interpreted 
as path-following. Here we restrict our attention to the path described 
by the logarithmic barrier function in linear programming.
Given the linear program in standard form $(P)$,
it is possible to write the corresponding {\em barrier problem}:
\[
\begin{array}{crl}
         & \min        & c^Tx - \mu \sum_i \ln x_i \\
 (P_\mu) & \mbox{s.t.} & Ax = b, \\
         &             & x > 0.
\end{array}
\]

Problem $(P_\mu)$ denotes a family of problems parametrised by the 
quantity $\mu>0$ (typically small), called {\em barrier parameter} 
in the interior point literature. 
It is worth noting that such approach is 
viable only if it is actually possible to find a point that 
strictly satisfies the constraints, that is, if $\mathcal{P}^0 \ne \emptyset$.

The presence of the logarithmic barrier forces the iterates 
to stay in the interior of the feasible region, as this term heavily penalises 
the points that are too close to the boundary. However, the influence exerted
by the logarithmic barrier can be controlled through the penalty
parameter $\mu$.
The weight on the barrier regulates the distance from the iterates to 
the boundary: as $\mu$ tends to zero, problem $(P_\mu)$ resembles
more and more problem $(P)$.

The objective function of problem $(P_\mu)$
is a strictly convex function. 
Therefore, for a fixed $\mu$, the problem has at most one global minimum. 
The minimizer, if it exists, is completely characterised 
by the associated KKT conditions:
\[
\begin{array}{rcc}
   Ax               & = &  b \\
  \mu X^{-1}e +A^Ty & = &  c \\
   x                & > &  0.
\end{array}
\]
By substituting $s = \mu X^{-1}e$, we obtain the
standard (primal--dual) formulation of the so called 
{\em perturbed KKT conditions}:
\be \label{eq:PerturbedKKT}
\begin{array}{rcc}
   Ax       & = & b \\
   A^Ty + s & = & c \\
   XSe      & = & \mu e \\
   (x,s)    & > & 0.
\end{array}
\ee

\fb{
If the feasible domain $\mathcal{P}$ is bounded, 
then both $(P)$ and $(P_\mu)$ have optimal solution. 
}
The following lemma has been proved by Megiddo \cite{Megiddo}.
\begin{lemma}
Problem $(P_\mu)$ 
is either unbounded for every  $\mu>0$ or has a unique optimal 
solution for every $\mu>0$.
\end{lemma}

If the perturbed KKT system (\ref{eq:PerturbedKKT}) has a solution for 
any $\mu>0$, then it determines a unique continuous smooth curve 
$(x(\mu),y(\mu),s(\mu))$ toward the optimal set as $\mu\to 0$. 
In interior-point 
terminology, this curve is called the {\em central path}.
The study of the primal--dual properties of central path was pioneered by
Megiddo \cite{Megiddo} and Bayer and Lagarias \cite{BayerLagarias}.

%Moreover, if $A$ has full rank, then the value of $y$ is 
%uniquely determined by the value of $x$. Therefore, 
%system (\ref{eq:PerturbedKKT}) has a unique solution $(x(\mu),y(\mu))$.

Under the assumptions that for a particular $\mu > 0$ the point
$(x(\mu),y(\mu),s(\mu))$ is primal and dual feasible, we can state
a similar result to what is expressed by 
(\ref{eq:DualityComplementarityGap}):
\[
  g(\mu) = c^Tx(\mu) - b^T y(\mu) = x(\mu)^T s(\mu),
\]
that is, the duality gap corresponds to the complementarity gap.
Hence reducing either of them is identical.
Also, as $XSe - \mu e = 0$ implies $x_is_i = \mu$, $i = 1, \ldots, n$, 
we have
\be  \label{eq:AverageComplementarity}
   s(\mu)^T x(\mu) = n\mu,
\ee
and for $\mu \to 0$, also $g(\mu) \to 0$.
Megiddo \cite{Megiddo} shows that $c^Tx(\mu)\to c^Tx^*$ as $\mu\to 0$. 
Furthermore, he proves the following, stronger result.
%
\begin{lemma}
\label{th:CentralPathConvergence}
Under the assumptions of primal feasibility, dual feasibility, and
full row rank of matrix $A$, then
\[
   x(\mu) \to x^*, \quad (y(\mu),s(\mu)) \to (y^*, s^*)
\]
as $\mu \to 0$.
\end{lemma}

The solution that, according to Lemma~\ref{th:CentralPathConvergence},
the central path converges to is
characterised by strict complementarity.

\begin{theorem}[Strict complementarity]
If $(P)$ and $(D)$ are feasible, then there exist a point $x^* \in\mathcal{P}$
and a pair $(y^*,s^*) \in \mathcal{D}$ such that
\[
(x^*)^T s^* = 0 \quad\mbox{ and }\quad x^*_i + s_i^* >0, \;\; i = 1, \ldots, n.
\]
\end{theorem}

A solution $(x^*,s^*)$ that satisfies the above theorem is called 
{\em strictly complementary}. 
On the grounds of a strictly complementary
solution we can define the concept of {\em optimal partition}.
Following Jansen \cite{phd:Jansen}, we define the support set
of a vector $v \in \R^n$ as
\[
   \sigma(v) = \{ i : v_i > 0, \; i=1,\ldots,n \},
\]
and partition the set of indices $\{1,\ldots,n \}$ as
\[
   \mathcal{B} = \sigma(x^*), \quad \mathcal{M} = \sigma(s^*).
\]
This partition is well-defined in the sense that a 
strictly complementary solution satisfies both 
$\mathcal{B} \cap \mathcal{M} = \emptyset$ and 
$\mathcal{B} \cup \mathcal{M} = \{1,\ldots,n \}$. 
A proof of the latter result, also known as Goldman-Tucker theorem, 
can be found in \cite{ipm:Wright97}.
The concepts of strict complementarity and optimal partition are
recurrent motifs in the analysis of interior point methods.

Vavasis and Ye \cite{VavasisYe} studied the properties of the 
curvature of the central path, discovering that the central path
is characterised by $\bigO(n^2)$ curves of high degree and
segments where it is relatively straight.
Such curves appear in correspondence with changes in the optimal
partition.
Close to the end, when the optimal partition has been identified,
the central path becomes a straight line \cite{Megiddo}.
In this region, the algorithm displays the quadratic convergence 
property typical of Newton's method.

We now consider
the limit of $(P_\mu)$ for $\mu \to \infty$, and therefore
find the point from which the central path departs.
This corresponds to finding the point that minimizes the barrier function:
\[
\hat{x} = \arg \min_{x \in \mathcal{P}^0} \big(-\sum_i \ln x_i \big).
\]
The point $\hat{x}$ is the {\em analytic center} of the feasible polytope, 
and was first studied by Sonnevend \cite{Sonnevend86}.
Given the strict convexity of the barrier function, the concept 
of analytic center is well defined.
As the analytic center minimizes the barrier, it
is the point farthest away from the boundary.

However, there is a problem with defining the central path in terms
of analytic center: the central path is affected by the presence of 
redundant constraints. 
This happens because it is an exclusively analytic concept, which does
not exploit geometric considerations.
Other type of centers (center of gravity, center of the ellipsoid of 
maximum volume that can be inscribed in $\mathcal{P}$, volumetric center) 
can be defined, but they usually are too demanding to compute 
\cite{Gonzaga92}. 

\fb{
See a chapter in Ye's book.
Terlaky's  Klee-Minty example.
}

%
%
\subsection{Neighbourhoods of the central path}
\label{sec:Neighbourhoods}

As we have seen, following the central path is the recommended
way of traversing the interior of the feasible region towards
the optimal solution. Nevertheless, it should be clear that keeping the
iterates {\em exactly on} the central path is an unachievable aim.
Finding a point that solves the perturbed complementarity conditions 
(\ref{eq:PerturbedComplementarity}) for a specific $\mu$ 
is as difficult as solving the optimization problem itself.
%
Therefore, we never insist on this extremely restrictive requirement,
but we rather allow the iterates to be somewhere around the central path.
This leads to the introduction of the concept of
{\em neighbourhood} of the central path. 
We can define several neighbourhoods, characterised
by different properties.
Two neighbourhoods are often used in theoretical developments.

The first is based on the Euclidean norm, and it is often referred
to as the {\em tight neighbourhood}:
\[
\Nhood_2(\theta) = \{ (x,y,s) \in \mathcal{F}^0 :
                         \| XSe - \mu e \|_2 \le \theta\mu \},
\]
where $\theta \in (0,1)$.
This neighbourhood defines points which lie very close to the central path:
search directions generated from points in this neighbourhood can be 
followed with full step, and the barrier parameter can be decreased
by a small amount at each iteration (giving rise to the name
of {\em short-step algorithms} to the algorithms that are based on
this neighbourhood). 
The closeness to the central path that the tight neighbourhood
imposes and maintains allows to produce the best convergence result
for linear programming: short-step algorithms converge in 
$\bigO(\sqrt{n}L)$ iterations \cite{KojimaMizunoYoshise89b,MonteiroAdler89a}.
However, since the reduction in the barrier parameter at each iteration 
is very small, the practical value of short-step algorithms is small.

The other commonly used neighbourhood is instead based on the infinity norm, 
and it is often called {\em wide neighbourhood}:
\[
\Nhood_{-\infty}(\gamma) = \{ (x,y,s) \in \mathcal{F}^0 :
                         x_is_i \ge \gamma\mu, \; i = 1, \ldots, n \},
\]
where $\gamma \in (0,1).$
Algorithms based on such a neighbourhood are allowed to generate
iterates that follow more loosely the central path. The iterates 
have more freedom of movement as they can get closer to the boundary
of the feasible set.
However, the Newton direction computed from points in the wide 
neighbourhood  has weaker properties, and a linesearch procedure is
needed to ensure that the positivity of the $(x,s)$ iterates is
preserved.
Algorithms based on the wide neighbourhood (usually called
{\em long-step algorithms}) are less conservative than their short-step
counterparts, and can decrease 
the barrier parameter more rapidly.
Efficient implementations of interior point methods are based
on some variation of a long-step algorithm.

In Section~\ref{sec:SymNeighbourhood} we will study a variation
of the $\Nhood_{-\infty}$ neighbourhood which better describes
the centrality requirements needed for a practical algorithm.

\ignore{
$N_2(\beta) \subseteq N_\infty(\beta) (= \| XSe - \mu e\|_\infty) \subseteq N_{-\infty}(\beta)$
}

\hrulefill

Many theoretical developments aim at lowering the upper bound on the number 
of steps needed for convergence. The results provided by such worst-case 
complexity analysis are informative but exceedingly pessimistic. 

Theoretical proofs of complexity generally follow a common scheme.
First they rely on a computable measure of the closeness to the central
path: this is accomplished by the concept of neighbourhood. Second,
they show that the direction computed by solving the Newton system
(\ref{eq:NewtonSystem}) can be followed with a strictly positive step
(and therefore some progress is made at every iteration) 
and generates an iterate 
that retains the property of being in some neighbourhood of the central 
path (possibly larger than the one before). Finally, they require
a decrease in the barrier parameter that allows to derive a polynomial upper
bound on the number of iterations needed to reach the desired
level of accuracy.


%
% Section
%
\section{Path-following algorithms}
\label{sec:PathFollowingAlgorithms}

We now bring together the elements we presented above and describe
a complete path-following algorithm. We then discuss some
theoretical results for algorithms in this class.

Primal--dual path-following methods solve the perturbed KKT
conditions (\ref{eq:KKT}) by asking the complementarity pairs to align 
to a specific barrier parameter $\mu > 0$,
\be  \label{eq:PerturbedComplementarity}
XSe = \mu e,
\ee
while enforcing $(x,s)>0$.
However, up to now, we have not defined how to choose the
barrier parameter $\mu$ and how to update it at each iteration.

\fb{
Explain how we choose it at the beginning.
}

With the progress of iterations 
we would like the perturbed KKT conditions (\ref{eq:PerturbedKKT}) 
to approximate better and better
the system (\ref{eq:KKT}) of optimality conditions for the original
problem.
Hence, at each iteration, $\mu$ is monotonically decreased by the quantity
$\sigma \in (0,1)$, called {\em centering parameter} for reasons that
will become clear later on.
The choice of the centering parameter $\sigma$ 
is algorithm-dependent. We provide theoretical insights on some
possible choices in Section~\ref{sec:FeasibleMethods}.

\ignore{
In all predictor-corrector algorithms there is a crucial decision 
to be made at every iteration, namely the choice of the penalty 
parameter $\mu$ to be used in the correction.

The paper \cite{VillasBoasPerin} tries to answer this question. 
They build a polynomial function of $\mu$ and $\alpha$, and they 
use to determine what the optimal choices of these parameters are, 
under a suitably chosen measure.

By using this strategy they achieve a better iteration count on 
most of the problems in their experiment. This, however, is not 
supported by a corresponding reduction in computational time. 
The reason for this is that the postponing of the choice of $\mu$ 
requires the solution of additional systems. While it's true that 
the most expensive operation is the computation of the Cholesky 
factors, the actual cost of the backsolves is not negligible. 
In the results of this paper, the additional cost of the extra 
backsolves is bigger than the savings obtained by the decrease 
in number of iterations.
}

Path-following interior point methods seek a solution 
to the system of equations (\ref{eq:PerturbedKKT})
\be  \label{eq:KKTSystem}
F(x,y,s) = \left[
  \begin{array}{c}
    Ax-b \\
    A^Ty+s-c \\
    XSe - \sigma\mu e \\
  \end{array} \right] = 0,
\ee
which is nonlinear in the perturbed complementarity constraints.
We use Newton's method to linearise the system around the 
current point according to
\[
\nabla F(x,y,s) \Delta(x,y,s) = -F(x,y,s),
\]
where $\nabla F(x,y,s)$ is the Jacobian of the function (\ref{eq:KKTSystem})
evaluated at the current interate $w = (x,y,s)$.
We obtain the so-called step equations
%
\be \label{eq:NewtonSystem}
\left[ \begin{array}{ccc}
    A & 0 & 0 \\ 0 & A^T & I \\ S & 0 & X
  \end{array} \right]
\left[ \begin{array}{c}
    \Delta x \\  \Delta y \\  \Delta s
  \end{array} \right] =
\left[ \begin{array}{c}
    b - Ax \\ c - A^Ty - s \\ -XSe + \sigma\mu e
   \end{array} \right] =
\left[ \begin{array}{c}
    \xi_b \\ \xi_c \\ \xi_\mu
   \end{array} \right],
\ee
%
which need to be solved for a search direction
$\Delta w = (\Delta x, \Delta y, \Delta s)$,
with $\mu = x^Ts/n$, $\sigma \in (0,1)$.

%
%
\subsection{Solving the perturbed KKT conditions}

The solution of system (\ref{eq:NewtonSystem}) is the computationally
dominant step in each iteration of an interior point algorithm.
Throughout this thesis, we will 
restrict our attention to using a direct approach in solving these
equations.
We should note, however, that a wealth of research explored the use
of iterative methods in the computation of the search direction
\cite{BergamaschiGondzioZilli,OliveiraSorensen05}.

System (\ref{eq:NewtonSystem}) is usually reduced to two other
formulations by exploiting the block structure of its
matrix.
%
The {\em augmented system} formulation is obtained by using 
the last row of (\ref{eq:NewtonSystem}) to eliminate
$\Delta s = X^{-1} (\xi_\mu - S\Delta x)$.
This produces
%
\be \label{eq:AugmentedSystem}
\left[ \begin{array}{cc}
    -X^{-1}S & A^T \\ A & 0
  \end{array} \right]
\left[ \begin{array}{c}
    \Delta x \\  \Delta y
  \end{array} \right] =
\left[ \begin{array}{c}
    \xi_c - X^{-1}\xi_\mu \\ \xi_b
   \end{array} \right],
\ee
which is a symmetric but indefinite system.
%
By further eliminating $\Delta x$, we reduce system 
(\ref{eq:AugmentedSystem}) to the set of {\em normal equations}
%
\be \label{eq:NormalEquations}
  A D^2 A^T \Delta y = A D^2 (\xi_c - X^{-1} \xi_\mu) + \xi_b,
\ee
%
where we introduced the notation $D^2 = S^{-1} X$.
Under the standard assumption of full row rank for $A$, matrix 
$A D^2 A^T$ is positive definite, since $D^2_i = x_i/s_i > 0$ for
all $i = 1, \ldots, n$.

Besides the issue of definiteness, the two formulations differ in
terms of sparsity and conditioning, the normal equations usually 
being denser and worse conditioned.
The choice between augmented system and normal equations depends also on 
the relative density of $AD^2A^T$ with respect to $A$.
Normal equations are to be avoided when there are dense columns in $A$, 
as they generate dense blocks in $AD^2A^T$.

The augmented system formulation requires particular attention 
in the development of linear algebra routines for the fact that
it presents an indefinite matrix. This raises problems of numerical
stability, and an accurate choice of pivoting strategies is fundamental.
Maros and M\'esz\'aros \cite{MarosMeszaros} presented an in-depth 
study of the properties of the augmented system formulation.
In the broad context of weighted least squares computations, the choice
between augmented system and normal equations has been studied long
before the development of interior point methods, see for example
\cite{DuffErismanReid86}.

%
%
\subsection{Feasible methods}
\label{sec:FeasibleMethods}

A feasible algorithm is characterised by the requirement that
all primal and dual iterates always lie within the interior 
of the feasible region. For this reason, these algorithms 
need to start from a strictly feasible point. 

Therefore, a feasible algorithm has $\xi_b = \xi_c = 0$
in the right-hand side of system (\ref{eq:NewtonSystem}). 
It is easy to see that feasibility
is maintained throughout the algorithm. Since
the search direction computed from (\ref{eq:NewtonSystem})
guarantees
\[
  A \Delta x = 0 \quad \text{and} \quad
  A^T \Delta y + \Delta s = 0,
\]
we verify that
\[
  \begin{split}
  A (x + \Delta x) &= Ax + A\Delta x = b, \\
  A^T(y +\Delta y) + (s +\Delta s) &= (A^T y +s) + (A^T\Delta y +\Delta s) = c.
  \end{split}
\]

As the progress in optimization is concerned,
we have that from $s^T \Delta x + x^T \Delta s = -x^Ts + \sigma\mu$
we obtain
\be  \label{eq:OptimalityProgress}
   x(\alpha)^Ts(\alpha) = x^Ts + \alpha(s^T \Delta x + x^T \Delta s)
     + \alpha^2 \Delta x^T \Delta s = x^Ts (1 - \alpha (1 - \sigma)).
\ee

From (\ref{eq:OptimalityProgress}) we observe that the progress in
optimization depends on both $\alpha$ and $\sigma$.
For a fixed $\sigma$, the length of the step $\alpha$ taken in 
the search direction
$(\Delta x,\Delta y ,\Delta s)$ computed from (\ref{eq:NewtonSystem})
measures the reduction in complementarity gap: 
the longer the step, the bigger the reduction. This motivates
the attempts to enlarge the stepsize by the use of correctors
techniques (see Section~\ref{sec:MultipleCC} and Chapter~\ref{ch:Correctors}).

The centering parameter $\sigma$ plays an important role as well.
We can see that the biggest reduction is obtained for $\sigma = 0$.
This does not come as a surprise, as the choice of $\sigma = 0$
corresponds to solving the KKT conditions (\ref{eq:KKT}) which
describe the optimality conditions for the linear program
(\ref{eq:PrimalDualPair}). The choice of $\sigma = 1$, instead,
leaves the complementarity gap unchanged. While this does not
produce a progress towards optimality, it tends to move the iterate
closer to the central path. A step done in a direction computed
with $\sigma = 1$ is often called a {\em pure centering step}.
It is therefore essential to choose $\sigma$ appropriately, trying
to balance the often conflicting aims of optimality and centrality.

In a short-step feasible method based on the $\Nhood_2$ neighbourhood
the barrier parameter is reduced by 
\[
   \sigma = 1 - \delta/\sqrt{n}
\]
at each iteration, for some positive constant $\delta$, usually
very small (0.05 according to Gonzaga \cite{Gonzaga91a}).

One important result is this direction was obtained by 
Mizuno, Todd and Ye \cite{MizunoToddYe}, who introduced a short-step 
predictor--corrector method. Their strategy uses two nested neighbourhoods 
$\Nhood_2(\theta^2)$ and $\Nhood_2(\theta)$, $\theta \in (0,1)$, 
and exploits the
quadratic convergence property of Newton's method in such a tight
neighbourhood of the central path.
Their algorithm alternates between two search directions characterised by
different properties.
First, by choosing $\sigma = 0$ in (\ref{eq:NewtonSystem}),
the predictor direction gains optimality, possibly at the expense of
worsening the centrality, keeping the iterate in a larger neighbourhood
$\Nhood_2(\theta)$ of the central path. 
Then, a pure re-centering step is performed, by setting $\sigma = 1$,
leaving the duality gap unchanged but moves the iterate back into a 
tighter $\Nhood_2(\theta^2)$ neighbourhood. Hence, every second step the 
algorithm produces a point in $\Nhood_2(\theta^2)$.

An important contribution of this technique, is the idea 
of targeting optimality and centrality independently. 
The use of the very restrictive $\Nhood_2$ neighbourhood 
makes the Mizuno-Todd-Ye algorithm unattractive for practical applications.
However, it provides a scheme 
upon which more computationally attractive methods can be constructed,
as we will discuss in Chapter~\ref{ch:PracticalIpm}.

\hrulefill

Kojima, Mizuno and Yoshise \cite{KojimaMizunoYoshise89} 
proposed a polynomial-time long-step algorithm with the property of convergence
in $\bigO(nL)$ iterations.
This was refined by the same group \cite{KojimaMizunoYoshise89b} 
and by Monteiro and Adler \cite{MonteiroAdler89a}:
both papers presented a primal--dual algorithm for linear programming 
based on the tight $\Nhood_2$ neighbourhood
with the property of convergence in $\bigO(\sqrt{n}L)$ iterations.
This is still the best complexity result for interior point methods
for linear programming.

In complexity proof, the barrier parameter is reduced slightly at
each iteration in order to guarantee that one iteration of Newton's
method can keep the point in the tight neighbourhood of the central path.
However, this is a worst-case analysis, and in practice the same would happen
even for a bigger update of the barrier parameter. This brings to
study ways of allowing a more substantial reduction of the barrier
parameter, at least in some iterations. 

The Mizuno-Todd-Ye predictor--corrector algorithm \cite{MizunoToddYe},
discussed in Section~\ref{sec:FeasibleMethods},
achieves this without losing the $\bigO(\sqrt{n}L)$ convergence property.
This appealing result is obtained thanks to the optimizing predictor
direction which guarantees the same progress achievable by a short-step 
feasible method, with the only 
difference that the value of the barrier parameter is reduced over
two iterations.

%
%
\subsection{Infeasible methods}
\label{sec:InfeasibleMethods}

The results presented up to here concentrated on feasible methods. 
For these methods we assume to have a strictly feasible starting point
readily available.
Finding a strictly feasible starting point is, in general, a nontrivial task.
Solving the feasibility problem is an optimization problem in its
own right, and is as difficult as solving the original problem
to optimality.
Moreover, the feasible region may have empty interior,
in which case the theory developed above does not apply.
Allowing an infeasible starting point is particularly important
for the algorithms implemented in practical interior point solvers.
For these reasons, a need exists for developing techniques
that dispense from the requirement of feasibility of the 
starting iterate.

A way to recover a strictly feasible starting point involves 
solving an artificial Phase~I subproblem by using 
the big-$M$ method. However, the performance is dependent on 
the choice of the values given to the weights, and the use
of very large values, while theoretically satisfying,
causes numerical instabilities \cite{Lustig91}.
This is worsened 
by the presence of dense columns that compromises the 
computational efficiency. 

A very different approach is based on the homogeneous 
self-dual formulation introduced by Ye, Todd and Mizuno
\cite{YeToddMizuno94}.
This and a simplified variant are presented in \cite[ch.~9]{ipm:Wright97}.
The self-dual formulation wraps the optimization problem into one 
of slightly larger dimension, but for which a strictly feasible solution
is known from the start.
Therefore, once embedded in the homogeneous self-dual form,
the problem can be solved with a feasible algorithm.
Also, this formulation has the very
appealing property of being able to detect infeasibility
with accuracy.
The use of a self-dual formulation, however, comes with a price from a
computational viewpoint, particularly because of the need of
two extra backsolves at each iteration.
\fb{
It works only for linear programming.
Stability issues?
}

It is possible to develop an algorithm which only requires 
the $x$ and $s$ components to be strictly positive. 
This was initiated by Lustig \cite{Lustig91}, who proposed some
new feasibility restoration directions.
However, it was the work of Kojima, Megiddo and Mizuno 
\cite{KojimaMegiddoMizuno}
that introduced a complete infeasible algorithm, with full
theoretical analysis of convergence.

\fb{
Sure? Wright (page 109) says it was Lustig Marsten and Shanno.
}

In such 
an algorithm, all iterates are infeasible, but the limit points 
are feasible and optimal. This is obtained by using a 
neighbourhood that admits infeasible points:
\[
\Nhood_{-\infty}(\gamma,\beta) =\{ (x,y,s) :
           \|(\xi_b,\xi_c)\| \le \beta\mu \frac{\|(\xi_b^0,\xi_c^0)\|}{\mu_0}, 
	   \; (x,s)>0, \; x_is_i \ge \gamma\mu, \; i = 1, \ldots, n \},
\]
where $\gamma\in (0,1)$ and $\beta \ge 1$ are parameters, and 
$\xi_b^0,\,\xi_c^0$ are the primal and dual residuals, respectively, 
at the initial iterate $(x^0,y^0,s^0)$.

In the $\Nhood_{-\infty}(\gamma,\beta)$ neighbourhood
there is no strict feasibility requirement for 
the iterates; however, the residuals at each iteration must be 
bounded above by a multiple of the complementarity measure $\mu$. 
By reducing $\mu$ we can force the primal and dual residuals 
$\xi_b$ and $\xi_c$ to zero, thus approaching complementarity and 
feasibility at the same speed.

Kojima, Megiddo and Mizuno \cite{KojimaMegiddoMizuno} proposed 
a stepsize rule that guarantees global convergence of an 
infeasible interior-point algorithm.
An algorithm is globally convergent if it is possible to choose
a strictly positive stepsize such that the complementarity gap
is reduced at each iteration. This property is very important, at it
guarantees the good behaviour of the algorithm for any given starting
point. 
However, for the global convergence property to hold, two 
safeguards need to be introduced.
The first requires that the reduction in infeasibility should be faster 
than in the complementarity gap. This ensures that the algorithm
does not get too close to a complementary point before feasibility
is restored.
The second safeguard forces $x_i s_i \ge \beta x^Ts$ for a chosen 
$\beta$, and requires that all iterates must stay in this neighbourhood.
\fb{
Check this Kojima-Megiddo-Mizuno stuff!

Order of convergence for an infeasible algorithm is 
$\bigO(n^2L)$ in Wright.
}

Letting 
$(x(\alpha),y(\alpha),s(\alpha)) =(x,y,s)+\alpha(\Delta x,\Delta y,\Delta s)$,
then we can show that
\[
  \xi_b(\alpha) = (1-\alpha) \xi_b \quad \text{and} \quad 
  \xi_c(\alpha) = (1-\alpha) \xi_c,
\]
so infeasibilities reduce linearly with $\alpha$, while for the 
complementarity gap
\[
  x(\alpha)^Ts(\alpha)=(1-\alpha(1 -\sigma))x^Ts +\alpha^2 \Delta x^T \Delta s,
\]
a reduction happens for a sufficiently small $\alpha$. 
When feasibility is restored, an infeasible algorithm becomes identical
to a feasible algorithm. 


%
% Section
%
\section{Symmetric neighbourhood}
\label{sec:SymNeighbourhood}

The quality of centrality (understood in a simplified way as complementarity)
for a practical implementation of an interior point algorithm
is {\it not} well characterised by either of two neighbourhoods 
$\Nhood_2$ or $\Nhood_{-\infty}$ commonly used in theoretical developments 
of interior point methods.

The $\Nhood_2$ neighbourhood is too restrictive, and the short-step methods
based on it are too conservative and, ultimately, very slow.
The $\Nhood_{-\infty}$ provides a much better framework, as it allows to
reduce the barrier parameter quickly. However, at it does not actively
enforce an upper bound on the complementary products, it may allow the
iterates to produce very unbalanced products.

The issue of unbalanced complementary products is very important for
the practical success of an interior point code. 

\fb{
We should note how
the complementary pairs play a role in the Newton system, and how
their bad scaling causes a bad behaviour of Newton's method, which
produces unreliable directions.

Jansen \cite{JansenRoosTerlaky96} (or actually \cite{phd:Jansen}?)
proposed to use the ratio 
between the smallest and the largest complementarity pair as an 
indication of the quality of a point.

It mentions that was Atkinson and Vaidya \cite{AtkinsonVaidya} 
that noticed that as the ratio increases, the region in which 
Newton converges becomes smaller.
}

Practical experience with the primal--dual algorithm in HOPDM \cite{HOPDM}
suggests that one of the features responsible 
for its efficiency is the way in which the quality of centrality 
is assessed. By ``centrality'' we understand here the spread 
of complementarity products $x_i s_i$, $i = 1,\dots,n$.
Large discrepancies within the complementarity 
pairs, and therefore bad centering, create problems for the search 
directions: an unsuccessful iteration is caused not only by small
complementarity products, but also by very large ones.
%
 This can be explained by the fact that
Newton's direction tries to compensate for very 
large products, as they provide the largest gain in complementarity 
gap when a full step is taken. However, the direction thus generated 
may not properly consider the presence of very small products, 
which then become blocking components when the stepsizes are computed.

The notion of spread in complementarity products
is not well characterised by either 
of the two neighbourhoods $\Nhood_2$ or $\Nhood_{-\infty}$ commonly used 
in theoretical developments of interior point methods.
To overcome this disadvantage, here we formalise a variation 
on the usual $\Nhood_{-\infty}(\gamma)$ neighbourhood, 
in which we introduce an upper bound on the complementary pairs. 
We propose to use a symmetric neighbourhood $\Nhood_s(\gamma)$,
in which complementarity pairs have to satisfy 
$\gamma \mu \leq x_i s_i \leq \gamma^{-1} \mu$, where $\gamma \in (0,1)$, 
for a strictly feasible iterate $(x,y,s)$.
This neighbourhood was implicitly used in \cite{Gondzio96}
to define an achievable target for multiple centrality correctors
(we refer the reader to Section~\ref{sec:MultipleCC}).

We define the symmetric neighbourhood to be the set
\[
  \Nhood_s(\gamma)=\{(x,y,s)\in \mathcal{F}^0: 
  \gamma\mu\le x_is_i \le \frac{1}{\gamma}\mu, \; i=1,\ldots,n\},
\]
where $\mathcal{F}^0$ 
is the set of strictly feasible primal--dual points,
$\mu = x^Ts/n$, and $\gamma \in (0,1)$.

While the $\Nhood_{-\infty}$ neighbourhood ensures that some 
products do not approach zero too early, it does not prevent products
from becoming too large with respect to the average.
In other words, it does not provide a complete 
picture of the centrality of the iterate. The symmetric 
neighbourhood $\Nhood_s$, on the other hand, promotes 
the decrease of complementarity pairs which are too large, thus taking 
better care of centrality.

An upper bound on the size of comlementary products
is implicit in the $\Nhood_{-\infty}$ neighbourhood. To find it,
suppose that all but one complementarity products are at the lower bound
$\gamma\mu$:
\[
  n\mu = x_1s_1 + \sum_{i=2}^n x_is_i = x_1 s_1 + (n-1)\gamma\mu,
\]
from which it follows that in general
\be  \label{eq:UpperBoundN8hood}
  x_i s_i \le (n (1-\gamma) + \gamma) \mu, \quad i = 1, \ldots, n.
\ee
We note the dependence on the problem dimension in the definition of the
upper bound, hence its uneffectiveness for large-scale problems.

We now determine the value of $n$ for which the upper bound of
the symmetric neighbourhood is tighter than the implicit upper bound
(\ref{eq:UpperBoundN8hood}), that is
\[
  n(1-\gamma) + \gamma > \frac{1}{\gamma},
\]
which means that the symmetric neighbourhood has a stronger bound
for
\[
  n > \frac{1+\gamma}{\gamma}.
\]

%
%
\subsection{Analysis}

We analyse a long-step feasible path-following 
algorithm based on the symmetric neighbourhood $\Nhood_s(\gamma)$, 
where the search direction $(\Delta x, \Delta y, \Delta s)$ 
is found by solving system (\ref{eq:NewtonSystem}) with 
$r=(0,\; 0,-XSe+\sigma\mu e)^T$, $\sigma\in(0,1)$, $\mu=x^Ts/n$.
%
The exposition follows closely the presentation of 
\cite[Chapter~5]{ipm:Wright97}. 

First we need a technical result, the proof of which can be found 
in \cite[Lemma~5.10]{ipm:Wright97} and is unchanged by the use 
of $\Nhood_s$ rather than $\Nhood_{-\infty}$.
%
\begin{lemma} \label{Wright:5.10}
If $(x,y,s)\in \Nhood_s(\gamma)$, then\,
\(
  \|\Delta X\Delta Se\| \le 2^{-3/2}\left( 1+ \displaystyle{\frac{1}{\gamma}} \right)n\mu.
\)
\end{lemma}

Our main result is presented in Theorem~\ref{th:SymNeighbourhood}. 
We prove that it is possible to find a strictly positive stepsize 
$\alpha$ such that the new iterate 
$(x(\alpha),y(\alpha),s(\alpha))=(x,y,s)+\alpha(\Delta x,\Delta y,\Delta s)$
will not leave the symmetric neighbourhood, and thus this 
neighbourhood is well defined. This result extends 
Theorem~5.11 in \cite{ipm:Wright97}.

\begin{theorem} \label{th:SymNeighbourhood}
  If $(x,y,s)\in\Nhood_s(\gamma)$, then 
  $\left(x(\alpha),y(\alpha),s(\alpha)\right)\in\Nhood_s(\gamma)$ for all
  \[
  \alpha\in \left[0,2^{3/2}\gamma\frac{1-\gamma}{1+\gamma}\frac{\sigma}{n} \right].
  \]
\end{theorem}
\begin{proof}
Let us express the complementarity product in terms of the stepsize 
$\alpha$ along the direction $(\Delta x, \Delta y, \Delta s)$:
%
\begin{eqnarray} \label{eq:CompProdAlpha}
x_i(\alpha)s_i(\alpha)&=&(x_i+\alpha\Delta x_i)(s_i+\alpha\Delta s_i)\nonumber\\ 
&=& x_is_i+\alpha(x_i\Delta s_i +s_i\Delta x_i) +\alpha^2\Delta x_i\Delta s_i\\
&=& (1-\alpha)x_is_i + \alpha\sigma\mu + \alpha^2\Delta x_i\Delta s_i.\nonumber
\end{eqnarray}
%
We need to study what happens to this complementarity product 
with respect to both bounds of the symmetric neighbourhood.
%
Let us first consider the bound $x_is_i\le \frac{1}{\gamma}\mu$.
By Lemma~\ref{Wright:5.10}, equation (\ref{eq:CompProdAlpha}) implies
\[
x_i(\alpha)s_i(\alpha) \le (1-\alpha)\frac{1}{\gamma}\mu +\alpha\sigma\mu 
+ \alpha^2 2^{-3/2}\left( 1+ \frac{1}{\gamma} \right)n\mu.
\]
At the new point $(x(\alpha),y(\alpha),s(\alpha))$, the duality gap
is $x(\alpha)^Ts(\alpha) = n\mu(\alpha)$.
The relation $x_i(\alpha)s_i(\alpha)\le \frac{1}{\gamma}\mu(\alpha)$ 
holds provided that
\[
(1-\alpha)\frac{1}{\gamma}\mu +\alpha\sigma\mu + \alpha^2 2^{-3/2}\left( 1+ \frac{1}{\gamma} \right)n\mu 
\le\frac{1}{\gamma}(1-\alpha+\alpha\sigma)\mu,
\]
from which we derive a first bound on the stepsize:
\[
\alpha \le 2^{3/2}\frac{1-\gamma}{1+\gamma}\frac{\sigma}{n} = \bar\alpha_1.
\]
%
Considering now the bound $x_is_i\ge \gamma\mu$ and using again
Lemma~\ref{Wright:5.10}, equation (\ref{eq:CompProdAlpha}) implies
\[
x_i(\alpha)s_i(\alpha) \ge (1-\alpha)\gamma\mu + \alpha\sigma\mu 
- \alpha^2 2^{-3/2}\left( 1+ \frac{1}{\gamma} \right)n\mu.
\]
Hence, $x_i(\alpha)s_i(\alpha)\ge \gamma\mu(\alpha)$ provided that
\[
(1-\alpha)\gamma\mu + \alpha\sigma\mu- \alpha^2 2^{-3/2}\left( 1+ \frac{1}{\gamma} \right)n\mu 
\ge\gamma(1-\alpha+\alpha\sigma)\mu,
\]
from which we derive a second bound on the stepsize:
\[
\alpha\le 2^{3/2}\gamma\frac{1-\gamma}{1+\gamma}\frac{\sigma}{n} =\bar\alpha_2.
\]

Therefore, for
\[
\alpha \in [0,\min(\bar\alpha_1,\bar\alpha_2)] = [0, \bar\alpha_2],
\]
we satisfy both bounds of the symmetric neighbourhood.

To conclude our proof we need to show that the 
$(x(\alpha),y(\alpha),s(\alpha))$ iterate is still strictly feasible.
Feasibility is trivially maintained, for the same argument shown 
in Section~\ref{sec:FeasibleMethods}.
About positivity, we have that
\[
  x_i(\alpha)s_i(\alpha) \ge \gamma \mu(\alpha) 
                         = \gamma(1 - \alpha(1-\sigma))\mu > 0,
\]
as $\gamma\in (0,1)$ and $\sigma \in (0,1)$. 
Hence $x(\alpha)>0$ and $s(\alpha)>0$.
\end{proof}

It is interesting to note that the introduction of the upper bound 
on the complementarity pairs does not change the polynomial complexity 
result proved for the $\Nhood_{-\infty}(\gamma)$ neighbourhood 
\cite[Theorem~5.12]{ipm:Wright97}. Therefore, the symmetric 
neighbourhood provides a better practical environment without any 
theoretical loss. This understanding provides some additional 
insight into the desired characteristics of a well-behaved iterate.

The use of the symmetric neighbourhood will be one of the theoretical
motivations of this work. Through it, in Section~\ref{sec:MultipleCC}
we will put the work of \cite{Gondzio96} inside a more sound framework.
Then we will use it again in the presentation of the weighted corrector
directions strategy of Chapter~\ref{ch:Correctors}, and in the analysis of an 
original warm-start strategy for stochastic programming of 
Chapter~\ref{ch:Warmstart}.
