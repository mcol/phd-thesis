%Started on 9th August 2006
%Aug: 11, 22, 23, 24, 25

%
% Chapter: Background and introduction.
%
\label{ch:Introduction}

Since their introduction following Karmarkar's groundbreaking paper
\cite{Karmarkar}, interior point methods (IPMs for short) have attracted 
the interest of a growing number of researchers.
Over the last 20 years, an impressive wealth of theoretical research
has been published, and computational developments have brought life
to a field, that of Linear Programming, that seemed not to attract much
attention anymore.

\fb{
Cite the reports on how much the simplex method has improved as a 
consequence of that.
}

Interior point methods are well-suited to solving very
large scale optimization problems. Their theory is well understood
\cite{ipm:Wright97} and the techniques used in their implementation are
documented in extensive literature (see, for example, 
\cite{AndersenGondzioMeszarosXu,GondzioTerlaky} and the references therein).


This chapter is devoted to the derivation and analysis of interior 
point methods. 
The theoretical developments will be accompanied by computational 
arguments.


%
% Section
%
\section{Primal--dual path-following methods}
\label{sec:Derivation}

%In this section we recall the main ideas leading to the use of centrality
%correctors techniques.

Consider the following primal--dual pair of linear programming problems 
in standard form
%
\begin{eqnarray} \label{eq:PrimalDualPair}
  \begin{array}{rlp{2cm}rl}
%   \mbox{\small Primal } & & & \mbox{\small Dual } \\[0.1cm]
    \mbox{ min }  & c^T x  & & \mbox{ max }  & b^T y \\
    \mbox{ s.t. } & Ax = b,& & \mbox{ s.t. } & A^T y + s = c, \\
                  & x \geq 0; &  &   & y \mbox{ free,} \;\; s \geq 0,
  \end{array}
\end{eqnarray}
%
where $A \in {\cal R}^{m \times n}$, $x, s, c \in {\cal R}^{n}$ 
and $y, b \in {\cal R}^{m}$. It is assumed, without loss of generality,
that $A$ has full row rank, as linearly dependent rows can be
removed without affecting the solution set.

The Karush-Kuhn-Tucker (KKT) conditions express first-order optimality 
conditions for the primal--dual pair (\ref{eq:PrimalDualPair}).
They can be expressed as
\be  \label{KKT}
\begin{array}{rcl}
  Ax      &=& b \\
  A^Ty +s &=& c \\
  XSe     &=& 0 \\
  (x,s)   &\ge& 0,
\end{array}
\ee
where $X, S \in \mathcal{R}^n$ are diagonal matrices with elements 
$x_i$ and $s_i$ respectively, and $e \in \mathcal{R}^n$ is a vector 
of ones. In other words, an optimal solution is characterised by 
primal feasibility, dual feasibility and complementarity.

\fb{
Spend a few words on the concept of complementarity.
}

Interior point methods arrive to a solution that satisfies the KKT
conditions (\ref{KKT}) by ``relaxing'' the complementarity constraints.
%
Path-following interior point methods \cite{ipm:Wright97} perturb 
the above conditions by asking the complementarity pairs to align 
to a specific barrier parameter $\mu > 0$,
\[
XSe = \mu e,
\]
while enforcing $(x,s)>0$.
As $\mu$ is decreased iteration after iteration, the solution of the 
perturbed Karush-Kuhn-Tucker conditions approximates better and better
the system of optimality conditions (\ref{KKT}).

If the perturbed KKT system has a solution for any $\mu > 0$, then
it traces a unique continuous path $(x(\mu),s(\mu))$ toward the 
optimal set as $\mu \to 0$. 
In interior-point terminology, such a path is called
the {\em central path}.

%
%
\subsection{The central path}

The study of the notion of central path and its properties was started by
Megiddo's seminal paper \cite{Megiddo}, which is still considered a basic 
reference in the interior-point literature.

Many algorithms used in mathematical programming can be interpreted as path-following. What is studied here is the path described by the barrier functions in linear programming.

Given a linear program in standard form $P$:
\[
\begin{array}{rl}
  \max        & c^Tx \\
  \mbox{s.t.} & Ax = b \\
              & x \ge 0,
\end{array}
\]
it is possible to write the corresponding barrier problem $P_\mu$:
\[
\begin{array}{rl}
  \max        & c^Tx + \mu \sum_j \ln x_j \\
  \mbox{s.t.} & Ax = b \\
              & x > 0.
\end{array}
\]

This second problem is parametrized by the quantity $\mu > 0$, 
tipically small. The presence of the barrier forces the iterates 
to stay in the interior of the feasible region. Such approach is 
viable only if it is actually possible to find a point that 
satisfies the constraints.

The associated KKT conditions are:
\[
\begin{array}{lcc}
  \mu X^{-1}e -A^Ty & = & -c \\
   Ax               & = &  b
\end{array}
\]

If the feasible domain $\{ x: Ax=b,\: x\ge 0 \}$ is bounded, 
then both $P$ and $P_\mu$ have optimal solution. Since the 
objective function in $P_\mu$ is strictly concave, $P_\mu$ 
has a unique solution for every $\mu>0$.

Megiddo then proves the following proposition: problem $P_\mu$ 
is either unbounded for every  $\mu>0$ or has a unique optimal 
solution for every $\mu>0$.

If the KKT system has a solution for any $\mu>0$, then it 
determines a unique continuous path $x(\mu)$ as $\mu\to 0$. 
Moreove, if $A$ has full rank, then the value of $y$ is 
uniquely determined by the value of $x$. Therefore, the KKT 
system has a unique solution $(x(\mu),y(\mu))$.

Megiddo shows that $c^Tx(\mu)\to c^Tx^*$ as $\mu\to 0$. 
Furthermore, he proves the stronger result that 
$x(\mu)\to x^*$ as $\mu\to 0$.

\fb{
\begin{itemize}
\item Properties of the curvature of the central path \cite{VavasisYe}, 
and straight line towards the end \cite{Megiddo}.
\item Explore the ``videos'' of the central path on Wright's webpage.
\item Problems with central path defined as analytic center: Terlaky's
 Klee-Minty example, discussion with Coralia (she mentioned a 
chapter in Ye's book).
\end{itemize}
}

%
%
\subsection{Solving the perturbed KKT conditions}

Path-following interior point methods seek a solution 
to the nonlinear system of equations
\[
F(x,y,s) = \left[
  \begin{array}{c}
    Ax-b \\
    A^Ty+s-c \\
    XSe - \mu e \\
  \end{array} \right] = 0,
\]
where the nonlinearity derives from the complementarity conditions.
We use Newton's method to linearise the system according to
\[
\nabla F(x,y,s) \Delta(x,y,s) = -F(x,y,s),
\]
and obtain the so-called step equations
%
\be \label{eq:NewtonSystem}
\left[ \begin{array}{ccc}
    A & 0 & 0 \\ 0 & A^T & I \\ S & 0 & X
  \end{array} \right]
\left[ \begin{array}{c}
    \Delta x \\  \Delta y \\  \Delta s
  \end{array} \right] =
\left[ \begin{array}{c}
    b - Ax \\ c - A^Ty - s \\ -XSe + \mu e
   \end{array} \right] =
\left[ \begin{array}{c}
    \xi_b \\ \xi_c \\ \xi_\mu
   \end{array} \right],
\ee
%
which need to be solved with a specified $\mu$ for a search direction
$(\Delta x, \Delta y, \Delta s)$. Throughout this thesis, we will 
restrict our attention to using a direct approach in solving these
equations.

System (\ref{eq:NewtonSystem}) is usually reduced to two other
formulations by exploiting the block structure of the constraint
matrix.
%
The {\em augmented system} formulation is obtained by using 
the last row of (\ref{eq:NewtonSystem}) to eliminate
$\Delta s = X^{-1} (\xi_\mu - S\Delta x)$.
This produces
%
\be \label{eq:AugmentedSystem}
\left[ \begin{array}{cc}
    -X^{-1}S & A^T \\ A & 0
  \end{array} \right]
\left[ \begin{array}{c}
    \Delta x \\  \Delta y
  \end{array} \right] =
\left[ \begin{array}{c}
    \xi_c - X^{-1}\xi_\mu \\ \xi_b
   \end{array} \right],
\ee
which is a symmetric but indefinite system.
%
By further eliminating $\Delta x$, we reduce system 
(\ref{eq:AugmentedSystem}) to the set of {\em normal equations}
%
\be \label{eq:NormalEquations}
  A D^2 A^T \Delta y = A D^2 (\xi_c - X^{-1} \xi_\mu) + \xi_b,
\ee
%
where we introduced the notation $D^2 = S^{-1} X$.
Under the assumption of full row rank for $A$, matrix 
$A D^2 A^T$ is positive definite, since $D^2_i = x_i/s_i > 0$ for
all $i = 1, \ldots, n$.

Besides the issue of definiteness, the two formulation differ in
terms of sparsity, the normal equations usually being dense.

\fb{
\begin{itemize}
\item Compare the two approaches, and discuss what happens in software.
\item Dependence on linear algebra.
\end{itemize}
}



%
% Section
%
\section{Theoretical results}
\label{sec:TheoreticalResults}

Theoretical developments aim at lowering the upper bound on the number 
of steps needed for convergence. The results provided by such worst-case 
complexity analysis
are informative but exceedingly pessimistic. A common complexity result 
states that interior point methods (for linear and quadratic programming) 
converge arbitrarily close to an optimal solution in a number of iterations 
which is proportional to the problem dimension or to the square root of it.

Monteiro and Adler \cite{MonteiroAdler89a} showed that the use of
the small $N_2$ neighbourhood allows to reach the solution in
$O(\sqrt{n}L)$ iterations.

\fb{
Not sure about that reference! Mehrotra actually cites another paper
from M-A.
}

%
%
\subsection{Mizuno-Todd-Ye predictor--corrector algorithm}

Mizuno, Todd and Ye \cite{MizunoToddYe} analysed the short-step 
predictor--corrector method. Their algorithm uses two nested neighbourhoods 
$N_2(\theta^2)$ and $N_2(\theta)$, $\theta \in (0,1)$, and exploits the
quadratic convergence property of Newton's method in this type of 
neighbourhood.
The predictor direction gains optimality, possibly at the expense of
worsening the centrality, keeping the iterate in a larger neighbourhood
$N_2(\theta)$ of the central path. It is then followed with a 
pure re-centering step which throws the iterate back into a 
tighter $N_2(\theta^2)$ neighbourhood. Hence, every second step the 
algorithm produces a point in $N_2(\theta^2)$. This is a clever 
approach, but the use of the very restrictive $N_2$ neighbourhood 
makes it unattractive for practical applications.

An important contribution of this technique, however, is the idea 
of targeting optimality and centrality independently. While this 
algorithm is not effective in practical terms, it provides a scheme 
upon which more computationally attractive methods can be constructed.


%
%
%
\section{A practical algorithm}

Interior point methods require the computation of the Newton 
direction for the associated barrier problem and make a step along 
this direction, thus usually reducing primal and dual infeasibilities 
and complementarity gap; eventually, after a number of iterations, 
they reach optimality. 
Since finding the Newton direction is usually a major computational task, 
the efforts in the theory and practice of IPMs concentrate on reducing 
the number of Newton systems (\ref{eq:NewtonSystem}) to be solved.

In practice, convergence is much faster than stated by the theoretical
results presented in Section~\ref{sec:TheoreticalResults}: 
optimality is usually reached in a number of iterations which is 
proportional to the logarithm of the problem dimension. Practical 
developments aim to reduce this number even further. 

\fb{
Practical algorithms are very different from the ones used for
theoretical purposes, and usually they implement some variation
of infeasible interior point algorithms. In particular they show
differences in the search directions, the evaluation of the stepsize, 
the neighbourhood. 
This is further complicated by issues of computational efficiency
and numerical stability, which often suggest the use of amended
techniques or heuristic approaches, which make their analysis
extremely difficult.
}

%
%
\subsection{Correcting techniques}

Two techniques have proved particularly successful in this respect:
Mehrotra's predictor--corrector algorithm \cite{Mehrotra92} 
and multiple centrality correctors \cite{Gondzio96}. These 
techniques have been implemented in most of commercial and academic 
interior point solvers for linear and quadratic programming such 
as BPMPD, Cplex, HOPDM, Mosek, OOPS, OOQP, PCx and Xpress. 
They have also been used with success in semidefinite 
programming with IPMs \cite{Haeberly99}.

Both correcting techniques originate from the observation that 
(when direct methods of linear algebra are used) the computation 
of the Newton direction requires factoring a sparse symmetric matrix, 
followed by a backsolve which uses this factorization. 
The cost of computing the factors is usually significantly larger than that 
of backsolving: in some cases the ratio between these two computational 
efforts may even exceed 1000. 

\fb{
Present a small table that shows the cost of factoring and of backsolving
for HOPDM (and PC-x?).
}

Consequently, it is worth adding more (cheap) 
backsolves if this reduces the number of (expensive) factorizations. 
Mehrotra's predictor--corrector technique \cite{Mehrotra92} uses two 
backsolves per factorization; the multiple centrality correctors technique
\cite{Gondzio96} allows recursive corrections: a larger number 
of backsolves per iteration is possible, leading to a further reduction 
in the number of factorizations. 

\fb{
Since these two methods were developed, there have been a number of 
attempts to investigate their behaviour rigorously and thus provide
further insight. Such objectives are difficult to achieve because 
correctors use heuristics which are successful in practice but hard 
to analyse theoretically. 
Besides, both correcting techniques are applied to long-step and infeasible 
algorithms which have very little in common with the short-step and 
feasible algorithms that display the best known theoretical complexity.
Nevertheless, we would like to mention several of such theoretical 
attempts as they shed light on some issues which are important in 
efficient implementations of IPMs. 
}

Practical implementations usually follow Mehrotra's predictor--corrector 
algorithm \cite{Mehrotra92}. In such a framework, we first generate a 
predictor direction to make progress toward optimality, and then we 
compute a corrector to remedy for some of the error made by the predictor
and move the point closer to the central path .


A number of advantages can be obtained by splitting the computation 
of the Newton direction into two steps, corresponding to solving the linear
system (\ref{eq:NewtonSystem}) independently for the two right-hand 
sides 
\be \label{PredictorRhs}
r_1 =\left[ 
  \begin{array}{c}
    b-Ax \\ c-A^Ty-s \\ -XSe
  \end{array} \right] \quad \mbox{and} \quad
r_2 =\left[ 
  \begin{array}{c}
    0 \\ 0 \\ \mu e
  \end{array} \right].
\ee

First, we can postpone the choice of $\mu$ and base it
on the assessment of the quality of the affine-scaling direction;
second, the error made by the affine-scaling direction may be 
taken into account and
corrected. Mehrotra's predictor--corrector technique \cite{Mehrotra92}
translates these observations into a powerful computational method.

In what follows, we will provide a review of Mehrotra's achievements.


%
%
\subsection{Affine-scaling predictor direction}

The predictor direction is obtained by solving the step equations 
for the pure Newton direction (affine scaling direction). This 
direction is strongly optimizing, as it aims to a point for which 
all complementarity products go to zero. 

The affine scaling direction may well point towards the boundary 
of the positive orthant, causing the need for a very small stepsize. 
This is the reason why affine scaling alone is not enough in a 
practical implementation of interior point methods. It has to be 
complemented by other techniques.

The role of the centering term is to remedy this situation by 
affecting the search direction to move closer to the central path, 
therefore allowing a longer stepsize. 
As noted in \cite{TapiaZhangSaltzmanWeiser}, the choice of the 
centering parameter can be crucial both in theory and in practice. 
It is suggested that it is a function of the Newton step. 
This calls for two separate backsolves at each iteration.

Another problem with affine scaling is that it is only a linear 
approximation to the central path. However, the central path is a curve
with many high-degree turns \cite{VavasisYe}, and only close to the 
solution set becomes approximately a straight line \cite{Megiddo}. 
Therefore, affine scaling may be easily distracted by points that 
have small complementarity products but are not optimal. In particular, 
these points may not be feasible (in the feasible algorithm), or 
may produce a much bigger improvement in optimality than what 
attained in feasibility (in the general infeasible algorithm).

This is usually worsened if the current iterate is badly centered, 
and therefore only a very small step is acceptable in order to 
maintain positivity or to keep the iterate in the neighbourhood 
of the central path.

%
%
\subsection{Mehrotra's predictor--corrector algorithm}

Mehrotra's predictor--corrector algorithm \cite{Mehrotra92,LustigMarstenShanno}
is extremely efficient in practice. Since its introduction, it has 
been the considered the method of choice for practical implementations 
because it is usually very fast and reliable. Moreover, it has a 
convincing interpretation in terms of second order approximations.

\fb{
Mehrotra's algorithm exploits a centrality corrector in order to 
remedy to badly centered points. The purpose of this direction is 
to move closer to the central path, and therefore reduce the spread 
in complementarity products, without aiming for more optimality. 
This is a somewhat conservative direction, which it is hoped will 
provide more room for movement at the next iteration.
}

\fb{
Mehrotra \cite{Mehrotra92} introduced two important innovations: 
\begin{itemize}
\item A dynamic evaluation of the centering parameter $\sigma$;
\item A second order correction.
\end{itemize}
}

One tool introduced by Mehrotra \cite{Mehrotra92} is a dynamic evaluation 
of the centering parameter $\sigma$. It is based on a simple and cleverly 
implemented heuristic that evaluates the quality of the predictor direction
in order to judge the amount of centering term needed.

First of all, the {\em affine-scaling predictor direction} 
$\Delta_a w = (\Delta_a x, \Delta_a y, \Delta_a s)$ is obtained by solving 
system (\ref{eq:NewtonSystem}) with right-hand side $r_1$ defined above.

Then it computes the maximum feasible stepsizes $\alpha_P$ and $\alpha_D$ 
allowed and uses it to predict the complementarity gap after such a step:
\be \label{eq:PredictedGap}
  g_a = (x + \alpha_P \Delta_a x)^T(s + \alpha_D \Delta_a s).
\ee

The ratio $\,g_a / x^{T}s \in (0,1)$ measures the quality of the 
predictor direction.
A small ratio indicates a successful reduction of the complementarity 
gap. On the other hand, if the ratio is close to one, then very little 
progress is achievable in direction $\Delta_a$, and a strong recentering 
is recommended.

In \cite{Mehrotra92} the following choice of the new barrier parameter 
is suggested
%
\be \label{Mu}
  \mu = \left( \frac{g_a}{x^{T}s} \right)^{\! 2} \, \frac{g_a}{n}
           = \left( \frac{g_a}{x^{T}s} \right)^{\! 3} \, \frac{x^{T}s}{n},
\ee
%
corresponding to the choice of $\sigma = (g_a / x^Ts)^3$ 
for the centering parameter. 
Other choices of the exponent are possible. Mehrotra \cite{Mehrotra92}
studied the effect of different values $p=1,2,3,4$ for the exponent
on a subset of Netlib problems, and concluded that for $p$ between
2 and 4 there was not much difference.
\fb{
Mehrotra's heuristic was actually more elaborate.
}
Also Lustig et al. \cite{LustigMarstenShanno} commented on the
weak dependence of the computational performance on the choice 
of the exponent.

\ignore{ %%%%%%%%%%%%%%%%%
The centering parameter, more generally could be chosen as
\[
  \sigma = \left( \frac{g_a}{x^Ts} \right)^p\!\!.
\]
}        %%%%%%%%%%%%%%%%%

\fb{
Mehrotra notices that ``it is not clear if the central path (with
equal weights) is the best path to follow, particularly since it
is affected by the presence of redundant constraints. Furthermore,
the points on (or near) the central path are only intermediate to
solving the linear programming problem. It is only the limit point 
on this path that is of interest to us.''
}

If affine scaling provides a good improvement, a small $\sigma$ 
is chosen, and therefore very little centering will be used. When, 
on the other hand, affine scaling produces very small stepsizes 
and so very little improvement can be achieved, $\sigma$ will be 
close to one, and so a stronger recentering will occur.

\fb{
Another problem with affine scaling is that it is only a linear 
approximation to the central path. However, the central path is 
a highly nonlinear curve which only close to the solution set 
becomes approximately a straight line \cite{Megiddo}.
}

A second important contribution by Mehrotra consists in the 
use of a second order direction. The affine-scaling direction 
corresponds to a linear approximation to the the trajectory from 
the current point to the optimal set. Here no information about 
higher order terms is taken into account. This linearization, 
however, produces an error.


If a full step in the affine-scaling direction is made, then 
the new complementarity products are equal to
%
\begin{eqnarray*}
  (X + \Delta_a X) (S + \Delta_a S) e 
   \;=\; XSe + (S \Delta_a x + X \Delta_a s) + \Delta_a X \Delta_a S e
   \;=\; \Delta_a X \Delta_a S e,
\end{eqnarray*}
%
as the third equation in the Newton system satisfies 
$S \Delta_a x + X \Delta_a s = -XSe.$
%
The term $\Delta_a X \Delta_a S e$ corresponds to the error introduced
by Newton's method in linearising the perturbed complementarity condition.


Ideally we would like the next iterate to be perfectly centered: 
\[
  (X+\Delta X)(S+\Delta S)e=\mu e,
\]
which is equivalent to solving the nonlinear system
\[
  S\Delta x + X\Delta s = -XSe +\mu e - \Delta X\Delta Se.
\]
The linearization error made by the affine-scaling direction is exactly 
the $\Delta X\Delta Se$ term that is ignored in the right-hand side.
%Hence the {\it corrector} direction 
%should compensate for the error $\, \Delta_a X \Delta_a S e - \mu e$.

\fb{
Here there should be a comparison between the last equation and one 
well above that shows the affine scaling right-hand side.
}

Mehrotra introduced a second-order correction in which the 
linearization error is taken into account. 

Mehrotra's corrector term is obtained by solving the Newton system 
(\ref{eq:NewtonSystem}) with right-hand side
\be \label{MehrotraRhs}
r =\left[ \begin{array}{c}
    0 \\ 0 \\ -\Delta_a X\Delta_a Se + \sigma \mu e
  \end{array} \right],
\ee
for the direction $\Delta_c (x,y,s)$.
Such corrector direction combines the centrality term $\sigma \mu e$
and the second-order term $\Delta_a X\Delta_a Se$.

Once the predictor and corrector terms are computed, they are 
added to produce the composite predictor--corrector direction
\be \label{eq:CompositeDirection}
\Delta w = \Delta_a w+ \Delta_c w.
\ee

The next iterate is given by
\[
w^{k+1} = (x^k,y^k,s^k)
        + (\alpha_P\Delta x^k,\alpha_D\Delta y^k,\alpha_D\Delta s^k)
\]
where $\alpha_P$ and $\alpha_D$ are again chosen to satisfy
\[
x^k+\alpha_P\Delta x^k \ge 0, \quad s^k+\alpha_D\Delta s^k \ge 0.
\]

For reasons of computational efficiency, Mehrotra's algorithm exploits 
the same Jacobian matrix used to find the affine-scaling direction. 
Hence, the same Cholesky factors are reused.
The cost of a single iteration in the predictor--corrector 
method is only slightly larger than that of the standard 
method because two backsolves per iteration have to be executed, 
one for the predictor and one for the corrector. 

The practical advantage of Mehrotra's predictor--corrector technique
is that it often produces longer stepsizes before violating the 
non-negativity constraints.
%
This usually translates in significant savings in the number of IPM 
iterations and, for all non-trivial problems, lead into significant 
CPU time savings \cite{LustigMarstenShanno,Mehrotra92}. Indeed, 
Mehrotra's predictor--corrector technique is advantageous in all 
interior point implementations which use direct methods to compute 
the Newton direction.

It should be noted that in the computation of the predicted gap, 
it is assumed that a full step in the affine-scaling has been taken. 
Also, the affine-scaling predictor and Mehrotra's corrector direction 
contribute with equal weights to the final search direction. 
This argument will be considered again in Chapter~\ref{ch:Correctors}, 
where we study the use of a weighting strategy for the corrector
directions.

As mentioned above, this is an heuristic procedure: thus there are 
no global convergence results or polynomial complexity results. 
There is a local convergence result \cite{TapiaZhangSaltzmanWeiser}, 
according to which Mehrotra's method can be interpreted as a 
perturbed-composite Newton method, but this result lies on strong 
assumptions (non degeneracy, strict complementarity).

Tapia et al. \cite{TapiaZhangSaltzmanWeiser} interpreted the Newton step 
produced by Mehrotra's predictor--corrector algorithm as a perturbed
composite Newton method and gave results on the order of convergence. 
They proved that a level-1 composite Newton method, when applied 
to the perturbed Karush-Kuhn-Tucker system, produces the same 
sequence of iterates as Mehrotra's predictor--corrector algorithm. 
While, in general, a level-$m$ composite Newton method has 
a $Q$-convergence rate of $m+2$ \cite{OrtegaRheinboldt},
the same result does not hold 
if the stepsize has to be damped to keep non-negativity of the iterates, 
as is necessary in an interior-point setting. However, under 
the additional assumptions of strict complementarity and nondegeneracy 
of the solution and feasibility of the starting point, Mehrotra's 
predictor--corrector method can be shown to have $Q$-cubic convergence
\cite{TapiaZhangSaltzmanWeiser}.

%
%
\subsection{Multiple centrality correctors}
\label{ss:MultipleCC}

Mehrotra's predictor--corrector, as it is implemented in optimization 
solvers \cite{LustigMarstenShanno,Mehrotra92}, is a very aggressive 
technique. It is based on the assumption, rarely satisfied, that a 
{\it full} step in the corrected direction will be achievable.
Moreover, an attempt to correct all complementarity products to the 
same value $\mu$ is also very demanding and occasionally
counterproductive. 
\fb{
Consider how Salahi et al. \cite{SalahiPengTerlaky} modify the centrality
term in the corrector.
}
Besides, practitioners noticed that this technique may sometimes 
behave erratically, especially when used for a predictor direction 
applied from highly infeasible and not well centered points. 
\fb{
Is there a reference for that?
}
Finally, Mehrotra's corrector does not provide CPU time savings 
when used recursively \cite{CarpenterLustigMulveyShanno}.

Trying to provide a remedy to the above considerations, Gondzio 
\cite{Gondzio96} introduced the multiple centrality corrector technique 
as an additional tool to complement those presented by Mehrotra. 
The idea behind this technique is to ``force'' and increase in the 
length of the stepsizes by correcting the centrality of Mehrotra's 
iterate.

These correctors can be described as ``less ambitious'' than Mehrotra's
corrector. Instead of attempting to correct for the whole second-order error,
they concentrate on improving the complementarity pairs which really seem 
to hinder the progress of the algorithm, ie. the complementarity products 
that are far from the average.

\fb{
Introduce the symmetric neighbourhood.
}

%Like the previous one, this method also uses a composite direction 
%of the following form 
%\[
%  \Delta = \Delta_p + \Delta_{\Red{m}},
%\]
%where $\Delta_{p}$ and $\Delta_{m}$ are the predictor
%and corrector terms, respectively. The initial predictor is usually 
%chosen to be the affine-scaling direction although different choices 
%are also possible and, in certain special circumstances such as 
%for example warm-starting, may be justified.

In this context, Mehrotra's predictor--corrector direction 
(\ref{eq:CompositeDirection}) is considered to be a new predictor direction
to which one or more centrality correctors can be applied. 

Assume that a predictor direction $\Delta_p$ is given and the corresponding
feasible stepsizes $\alpha_{P}$ and $\alpha_{D}$ 
in the primal and dual spaces are 
determined. We look for a centrality corrector $\Delta_m$ such that larger
steps will be made in the composite direction $\Delta = \Delta_p + \Delta_m$.
We want to enlarge the stepsizes to 
%
\begin{eqnarray*} 
   \tilde{\alpha}_{P} = \min(\alpha_{P} \! + \! \delta, \,1) 
   \quad \mbox{ and } \quad
   \tilde{\alpha}_{D} = \min(\alpha_{D} \! + \! \delta, \,1), 
\end{eqnarray*}
%
for some aspiration level $\delta \in(0,1)$. We compute a trial point
%
\[
  \tilde{x} = x + \tilde{\alpha}_{P} \Delta_{p} x, \quad 
  \tilde{s} = s + \tilde{\alpha}_{D} \Delta_{p} s,
\]
%
and the corresponding complementarity products 
$\tilde v = \tilde X \tilde S e \in {\cal R}^{n}$.

The products $\tilde v$ are very unlikely to align to the same value $\mu$.
Some of them are significantly smaller than $\mu$, 
including cases of negative components in $\tilde v$, 
and some exceed $\mu$. Instead of trying to correct 
them all to the value of $\mu$, we correct only the {\it outliers}. 
Namely, we try to move small products 
$(\tilde x_j \tilde s_j \leq \gamma \mu)$ to $\gamma \mu$ and move 
large products $(\tilde x_j \tilde s_j \geq \gamma^{-1} \mu)$ 
to $\gamma^{-1} \mu$, where $\, \gamma \in (0,1)$;
complementarity products 
which satisfy $\gamma \mu \leq x_j s_j \leq \gamma^{-1} \mu$ are
already reasonably close to their target values, and 
do not need to be changed. 

Therefore, the corrector term $\Delta_m$ is computed by solving the usual 
system of equations (\ref{eq:NewtonSystem}) for a special right-hand side
$(0, \,0,\, t)^T$, where the target $t$ is defined as follows:
%
\begin{eqnarray} \label{Target}
  t_j = \left\{
  \begin{array}{ll}
    \gamma \mu - \tilde x_j \tilde s_j  
    & \mbox{ if } \;\; \tilde x_j \tilde s_j \leq \gamma \mu  \\
    \gamma^{-1} \mu - \tilde x_j \tilde s_j  
    & \mbox{ if } \;\; \tilde x_j \tilde s_j \geq \gamma^{-1} \mu  \\
    0    
    & \mbox{ otherwise.}
  \end{array}
  \right.
\end{eqnarray}

\fb{
The target point is not on the central path, but in the symmetric
neighbourhood.
}

One important feature of multiple centrality correctors is that they
can be successfully applied recursively. 
The number of centrality correctors to be computed is determined 
heuristically, trying to balance the cost of additional backsolves to 
the savings in iteration count.

\fb{
Present the heuristic according to \cite{Gondzio96}.

This technique is applied 
recursively on the direction $\Delta_p := \Delta_p + \Delta_m$.
Indeed, we use it as long as the stepsizes increase 
at least by a fraction of the aspiration level $\delta$.
}

The computational experience presented in \cite{Gondzio96} showed 
that this strategy is effective and the stepsizes in the primal and 
dual spaces computed for the composite direction are larger than 
those corresponding to the predictor direction. 

Virtually all existing interior point codes implement such technique 
\cite[Appendix B]{ipm:Wright97}.


%
% Section
%
\section{Other stuff}

%
%
\subsection{Mehrotra's starting point heuristic}

The choice of an initial iterate for interior point methods is a
critical one. It challenges both the feasible and infeasible
algorithms, and the solutions proposed in the two contexts are
extremely different.

For the feasible algorithm, a starting iterate needs necessarily 
to be primal and dual feasible.
\fb{
What about centrality?
}

Solving the feasibility problem is an optimization problem in its
own right, which is as difficult as solving the original problem.
One important tool in this respect is the self-dual formulation.
\fb{
Add a citation!
}
The self-dual formulation wraps the optimization problem into one 
of larger dimension, but for which a feasible solution is known 
from the start.

\fb{
The use of a self-dual formulation is not attractive from a
computational viewpoint, particularly because of the need of one 
extra backsolve at each iteration.

Stability issues?
}

However, the homogeneous self-dual formulation has the very
appealing property of being able to detect infeasibility.

Turning our attention to infeasible algorithms, the major hurdle
of finding a feasible starting point is removed. 
However, the practical performance is very sensitive to the initial
iterate, so the use of arbitrary points is not recommended.
In particular, two requiremengs become important: the centrality 
of the point and the magnitude of the corresponding infeasibilities.
\fb{
Expand!
}

Mehrotra \cite{Mehrotra92} introduced a tool to find a starting point 
that attemps to fulfill the above requirements. In this
heuristic, we solve two least squares problems which aim to
satisfy the primal and dual constraints:
\begin{eqnarray*}
  \min_x    \!\! & x^Tx & \;\;\mbox{s.t. }\; Ax = b,      \\
  \min_{y,s}\!\! & s^Ts & \;\;\mbox{s.t. }\; A^Ty + s = c.
\end{eqnarray*}
The corresponding solution $(\tilde x, \tilde y, \tilde s)$ is further 
shifted inside the positive orthant, and the starting point is
\[
(x_0,y_0,s_0) = (\tilde x + \delta_x e,\, \tilde y,\, \tilde s + \delta_s e),
\]
where $\delta_x$ and $\delta_s$ are positive quantities. 
Their values depends on the distance of $\tilde x$ and $\tilde s$
to non-negativity, and an additional correction term to ensure
strict positivity.

The following variation is described in \cite{GondzioTerlaky}:
\begin{eqnarray*} 
  \min_x    \!\! & c^Tx + \rho x^Tx & \;\;\mbox{s.t. }\; Ax = b,      \\
  \min_{y,s}\!\! & b^Ty + \rho s^Ts & \;\;\mbox{s.t. }\; A^Ty + s = c,
\end{eqnarray*}
where the parameter $\rho$ is fixed to a predetermined value, in order
to compensate for the contribution of the primal and dual objectives.

A problem with Mehrotra's strategy is that it is scale dependent,
it is affected by the presence of redundant constraints,
and it does not guarantee to produce a well-centered iterate.

The procedure of finding an appropriate starting point will be discussed
again in Chapter~\ref{ch:Warmstart}, where a specialised technique for
the case of stochastic linear programming problems will be analysed.

%
%
\subsection{Weaknesses in the existing approaches}

The approaches briefly outlined above have some weaknesses that 
can be pointed out. Recognizing them could be the first step in 
order to propose something better, though it is expected to be a 
hard task.

A weakness in Mehrotra's algorithm concerns the computation of the 
second order direction. In this procedure, it is assumed that a 
full step in the affine scaling direction has been taken. This is 
most definitely not the case, as a full step in the affine scaling 
direction would produce the optimal solution. Moreover, the real 
step in the predictor direction is computed. In Mehrotra's algorithm, 
this is only used to evaluate the quality of the predictor.

Multiple centrality correctors try to fix the presence of outliers in 
complementarity products after the computation of a complete 
predictor--corrector pair. This setup overlooks the fact that the 
presence of outliers is known from the beginning of the iteration. 
Still, in the centrality term,  we ask all of them to align to the 
average $\mu$.

%
%
\subsection{Subspace searches}

\fb{
Subspace searches are a different strategy of generating search 
directions. 
They usually derived differently than from the Newton system, and 
are built according to some criteria that are believed to be essential 
for a good search direction.
}


Jarre and Wechs \cite{JarreWechs} took a more pragmatic view and
looked for an implementable technique for generating efficient 
higher-order search directions in a primal--dual interior-point framework.
In the Newton system, while it is clear what to consider as right-hand 
side for primal and dual feasibility constraints (the residual 
at the current point), the complementarity component leaves more 
freedom in choosing a target $t$. They argue that there exists 
an optimal choice for which the corresponding Newton system would 
produce immediately the optimizer; however, it is not obvious how 
to find it.
Therefore, they propose to search a subspace spanned by $k$ different 
directions $\Delta w_1, \Delta w_2, \ldots, \Delta w_k$ generated 
from some affinely independent targets $t_1,t_2,\ldots,t_k$.
As the quality of a search direction can be measured by the length 
of the stepsize and the reduction in complementarity gap, they aim 
to find the combination 
\[
\Delta w = \Delta w(\rho) 
         = \rho_1\Delta w_1 + \rho_2\Delta w_2 + \ldots + \rho_k\Delta w_k
\]
that maximizes these measures.
This can be formulated as a small linear subproblem which can 
be solved approximately to produce a search direction $\Delta w$ 
that is generally better than Mehrotra's predictor--corrector direction.
%
%  They noticed that following targets generated by iterating 
%  Mehrotra's corrector in a straightforward recursion of the type
%  \[
%    t^{k+1} = \mu e - XSe -\Delta X^k\Delta S^k e
%  \]
%  is usually divergent for general positive starting points.


\fb{
Present analysis from Jarre-Wechs \cite{JarreWechs}.
}

While the approach presented in Section~\ref{ss:MultipleCC} 
generates a series of correctors 
that are evaluated and applied recursively, Mehrotra and Li 
\cite{MehrotraLi} propose a scheme in which a collection of linearly 
independent directions is combined through a small linear subproblem.

Following the approach explored by Jarre and Wechs \cite{JarreWechs}, 
they express the requirements for a good search direction as a linear 
program. In particular, they impose conditions aimed at ensuring 
global convergence of the algorithm when using generic search directions.
The directions considered in the subspace search can include all 
sorts of linearly independent directions: affine-scaling direction, 
Mehrotra's corrector, multiple centrality correctors, Jarre--Wechs 
directions. 
In the recently proposed approach, Mehrotra and Li \cite{MehrotraLi}
generate directions using a Krylov subspace mechanism.

% This new approach for generating corrector directions uses an exact 
% factorization from an earlier iteration to generate directions 
% via Krylov subspaces. 
% Therefore, information obtained from an iterative scheme is used 
% to improve the performance of an implementation based on direct 
% methods. In some sense, it is a hybrid of direct and iterative methods.

% When generating correctors through Krylov subspaces, the directions 
% have to satisfy the primal--dual feasibility constraints, but not 
% the complementarity constraints. This means that any convex combination 
% of these directions will still satisfy the feasibility requirements; 
% this gives the additional freedom of choosing the combination that 
% satisfies the complementarity conditions in the best possible way. 
% {\bf (Note: M\&L don't require the linear combination to be convex.)}

At the $k$-th iteration of interior point method we have to solve 
the Newton system $H_k \Delta_k = \xi_k$, where 
\[
\xi_k = 
%\left[
%  \begin{array}{c}
%    \xi_p \\
%    \xi_d \\
%    \xi_{\mu} 
%  \end{array}
%  \right] =
\left[
  \begin{array}{c}
    b - A x^k \\
    c - A^T y^k - s^k \\
    \mu e - X^k S^k e 
  \end{array}
  \right]
\]
is the right-hand side evaluated at the current iterate and $H_k$ 
is the corresponding Jacobian matrix. 
%
The direction $\Delta_k$ is used to compute a trial point:
\[
\tilde{x} = x^k + \alpha_P \Delta_k x, \quad
\tilde{y} = y^k + \alpha_D \Delta_k y, \quad
\tilde{s} = s^k + \alpha_D \Delta_k s.
\]
%
At the trial point $(\tilde x, \tilde y, \tilde s)$, a usual 
interior point method would have to solve the system
$\tilde H \tilde \Delta = \tilde \xi$
in order to find the next search direction. Instead, 
Mehrotra and Li \cite{MehrotraLi} generate a Krylov subspace 
for $\tilde H \tilde \Delta = \tilde \xi$.
The Krylov subspace of dimension $j$ is defined as
\[
K_j (H_k, \tilde H, \, \tilde \xi) =
{\mbox{span}} \{ \xi_H, G \xi_H, G^2 \xi_H, \dots,  G^{j-1} \xi_H \}, 
\]
where $\xi_H = H_k^{-1} \tilde \xi$, and $G = I - H_k^{-1} \tilde H$. 
Note that for stability reasons $\tilde H$ is preconditioned with $H_k$, 
the factors of which have already been computed.
The subspace thus generated contains $j$ linearly independent directions. 

In the algorithm of \cite{MehrotraLi}, the affine-scaling
direction $\Delta_a$, Mehrotra's corrector $\Delta_0$, 
the first $j$ directions $\Delta_1, \Delta_2, \dots, \Delta_j$ 
from $K_j (H_k, \tilde H, \tilde \xi)$ and, but only under some 
circumstances, a pure recentering direction $\Delta_{cen}$ are 
combined with appropriate weights $\rho$:
\[
\Delta(\rho) = \rho_a\Delta_a + \sum_{i=0}^j \rho_i \Delta_i 
             + \rho_{cen}\Delta_{cen}.
\]
The choice of the best set of weights $\rho$ in the combined search 
direction is obtained by solving an auxiliary linear programming 
subproblem. The subproblem maximizes the rate of decrease 
in duality gap whilst satisfying a series of requirements:
non-negativity of the new iterate,
upper bounds on the magnitude of the search direction,
upper bounds on infeasibilities,
decrease in the average complementarity gap,
and closeness to the central path.

%Both the theoretical formulation and the computer implementation 
%allow for the independent choice of weights $\rho$ for the primal 
%and dual spaces.

%
%
\subsection{Spare parts}

We know polynomial-time algorithms for linear programming, 
namely the ellipsoid method (see \cite[ch.~13]{ip:Schrijver86} 
and \cite[ch.~I.6]{ip:NemhauserWolsey88}) and interior point 
methods (\cite{ipm:Wright97}). Despite being an exponential 
algorithm, the simplex method shows polynomial complexity in 
the average time, and is therefore widely adopted in the 
solution of linear programming problems.

\hrulefill

Much of the theoretical research concentrates on feasible 
methods. In this case, a feasible starting point (one which 
satisfies equality constraints and positivity of variables) 
is assumed to be easily available. However, this is not the 
case in practice. Finding a starting point is a separate 
subproblem. For this reason, a need exists for practical 
implementations to dispense from this requirement.

The feasible algorithm needs to start from a strictly feasible 
point. This involves solving an artificial subproblem by using 
the big-$M$ method for example. Lustig (Feasibility issues in 
PD IPM) notes that this causes numerical instability, worsened 
by the presence of dense columns that compromises the 
computational efficiency. A very different approach to deal 
with the starting point issue is the self-dual formulation.

\hrulefill

The logarithmic barrier function method consists of solving the 
family of problems
\begin{eqnarray*}
   \min & c^Tx - \mu \sum \ln x_j \\
   \mbox{s.t.} & Ax = b \\
               & x>0
\end{eqnarray*}

where $\mu>0$ is the barrier parameter. The problem is solved 
for several values of $\mu$, with $\mu$ decreasing to zero. 
The result is a sequence of feasible points converging to a 
solution of the original problem. Note that the objective 
function is a strictly convex function. Therefore, for a 
fixed $\mu$, the problem has at most one global minimum. 
This global minimum, if it exists, is completely characterized 
by the KKT conditions.

\hrulefill

Choice of step length: either use neighbourhoods or strict 
positivity of iterates. The latter works very efficiently 
in practice but does not ensure global convergence.


%
%
\subsubsection{Choice of penalty parameter}

In all predictor-corrector algorithms there is a crucial decision 
to be made at every iteration, namely the choice of the penalty 
parameter $\mu$ to be used in the correction.

The paper \cite{VillasBoasPerin} tries to answer this question. 
They build a polynomial function of $\mu$ and $\alpha$, and they 
use to determine what the optimal choices of these parameters are, 
under a suitably chosen measure.

By using this strategy they achieve a better iteration count on 
most of the problems in their experiment. This, however, is not 
supported by a corresponding reduction in computational time. 
The reason for this is that the postponing of the choice of $\mu$ 
requires the solution of additional systems. While it's true that 
the most expensive operation is the computation of the Cholesky 
factors, the actual cost of the backsolves is not negligible. 
In the results of this paper, the additional cost of the extra 
backsolves is bigger than the savings obtained by the decrease 
in number of iterations.
