%Started on 9th August 2006
%Aug: 11, 22, 23, 24, 25, 28, 29, 30, 31
%Sep:  1,  7,  8
% 2007
%Jan: 15, 16, 17, 22, 26, 29, 30
%Feb:  1,  2,  4,  5

%
% Chapter: Interior point methods
%
\label{ch:Ipm}

Interior point methods are well-suited to solving very
large scale optimization problems. Their theory is well understood
\cite{ipm:Wright97}.

This chapter is devoted to the derivation and analysis of interior 
point methods. 
The theoretical developments presented will be accompanied by computational 
arguments.


%
% Section
%
\section{Primal--dual path-following methods}
\label{sec:Derivation}

Consider the following primal--dual pair of linear programming problems 
in standard form
%
\begin{eqnarray} \label{eq:PrimalDualPair}
  \begin{array}{crlp{2cm}crl}
     & \mbox{ min } & c^T x     &  &     & \mbox{ max }  & b^T y \\
 (P) &\mbox{ s.t. } & Ax = b,   &  & (D) & \mbox{ s.t. } & A^T y + s = c, \\
     &              & x \geq 0; &  &     &   & y \mbox{ free,} \;\; s \geq 0,
  \end{array}
\end{eqnarray}
%
where $A \in \R^{m \times n}$, $x, s, c \in \R^{n}$ 
and $y, b \in \R^{m}$, $m<n$. We assume, without loss of generality,
that $A$ has full row rank, as linearly dependent rows can be
removed without changing the solution set.
This implies that a feasible $s \ge 0$ determines in a unique
way the value of $y$.

We define the set of primal feasible points and the set of
primal interior points 
\[
\mathcal{P} = \{ x : Ax = b, \; x \ge 0 \}, \quad
\mathcal{P}^0 = \{ x \in \mathcal{P} : x > 0 \},
\]
and, similarly, the set of dual feasible points and the set of
dual interior points
\[
\mathcal{D} = \{ (y,s) : A^T y + s = c, \; s \ge 0 \}, \quad
\mathcal{D}^0 = \{ (y,s) \in \mathcal{D} : s > 0 \}.
\]

The set of feasible primal--dual point is therefore
$\mathcal{F} = \mathcal{P} \times \mathcal{D}$, and the set of primal--dual
interior points is
\[
\mathcal{F}^0 = \{ (x,y,s) \in \mathcal{F} : (x,s) > 0 \}.
\]
A point $(x,y,s) \in \mathcal{F}^0$ is said to be {\em strictly feasible}.

Using this notation, the primal--dual pair (\ref{eq:PrimalDualPair}) can 
be written as
\be \label{eq:PrimalDualPair2}
\min \; c^T x, \quad x \in \mathcal{P}; \qquad
\max \; b^T y, \quad (y,s) \in \mathcal{D},
\ee

We recall here some well-known results on the relationship between
problems $(P)$ and $(D)$.
These can be found in plenty of sources, for example \cite{lp:Chvatal} \ldots

\begin{lemma}[Weak duality]
Let $(x,y,s) \in \mathcal{F}$. Then $c^Tx \ge b^Ty$.
\end{lemma}

The difference $c^Tx - b^Ty$ is called {\em duality gap}.

Problem $(P)$ has a solution if and only if $\mathcal{P} \ne \emptyset$;
if also $\mathcal{D} \ne \emptyset$, then both problems admit an
optimal solution $(x^*, y^*, s^*)$, and the objective function 
values of both problems at that point coincide. This can be formalised
in the following lemma.

\begin{lemma}[Strong duality]
A point $x \in \mathcal{P}$ is an optimal solution if and only if
there exists a pair $(x,s) \in \mathcal{D}$ such that $c^Tx = b^Ty$.
\end{lemma}

However, one of the sets might be empty: in such a case, an optimal
solution for problem (\ref{eq:PrimalDualPair2}) does not exist,
as the set is either unbounded or empty as well.
In what follows, we make the standard assumption for the development
of interior point methods that $\mathcal{P}^0 \ne \emptyset$ and 
$\mathcal{D}^0 \ne \emptyset$. This is also referred to as the
{\em interior point assumption}. 
Cases when this assumption does not hold can be considered by allowing
the algorithm to accept infeasible iterates 
(see Section~\ref{sec:InfeasibleMethods}).
The interior point assumption corresponds to assuming that 
the primal--dual optimal face is 
bounded (this is mentioned in \cite{GonzagaCardia04} and also
in \cite[Lemma~2.2]{GulerRoosTerlakyVial}).

\fb{
See Jansen \cite{phd:Jansen}, p.14.
}

\fb{
Formalize these results.
See Guler Roos Terlaky Vial \cite{GulerRoosTerlakyVial}.

Introduce barrier problems and show similar theorems for them,
as done in Megiddo \cite{Megiddo}.
}

Optimality conditions let us recognise that a solution has been
found. They also provide insight on the development of algorithms 
for finding a solution.

The Karush-Kuhn-Tucker (KKT) conditions express first-order optimality 
conditions for the primal--dual pair (\ref{eq:PrimalDualPair}).
They can be expressed as
\be  \label{eq:KKT}
\begin{array}{rcl}
  Ax      &=& b \\
  A^Ty +s &=& c \\
  XSe     &=& 0 \\
  (x,s)   &\ge& 0,
\end{array}
\ee
where $X, S \in \R^{n\times n}$ are diagonal matrices with elements 
$x_i$ and $s_i$ respectively, and $e \in \R^n$ is a vector 
of ones. In other words, an optimal solution is characterised by 
primal feasibility, dual feasibility and complementarity.

Complementarity can be seen as a certificate for optimality 
in linear programming \cite{phd:Jansen,Schrijver86}.
For non-optimal solutions, complementarity measures the distance of the
solution to optimality:
\be  \label{eq:DualityComplementarityGap}
  c^Tx - b^T y = c^Tx - x^T A^T y = (c - A^T y)^T x = s^T x.
\ee
The quantity $x^T s$ is called {\em complementarity gap}.
When this distance is driven to zero, then the
solution found is also optimal.
It should be noted that the equality between duality gap and
complementarity gap of equation (\ref{eq:DualityComplementarityGap})
holds only for a feasible point.

Many algorithms used in mathematical programming can be interpreted 
as path-following. Here we restrict our attention to the path described 
by the logarithmic barrier function in linear programming.
Given the linear program in standard form $(P)$,
it is possible to write the corresponding barrier problem:
\[
\begin{array}{crl}
         & \min        & c^Tx - \mu \sum_j \ln x_j \\
 (P_\mu) & \mbox{s.t.} & Ax = b, \\
         &             & x > 0.
\end{array}
\]

Problem $(P_\mu)$ is a family of problems parametrised by the 
quantity $\mu>0$ (typically small), called {\em barrier parameter} 
in the interior point literature. 
It is worth noting that such approach is 
viable only if it is actually possible to find a point that 
satisfies the constraints, that is, $\mathcal{P}^0 \ne \emptyset$.

The presence of the logarithmic barrier term forces the iterates 
to stay in the interior of the feasible region, as this term penalises 
iterates that get too close to zero. However, the influence exerted
by the logarithmic barrier can be controlled through the penalty
parameter $\mu$.
The weight on the barrier regulates the distance from the iterates to 
the boundary: as $\mu$ tends to zero, problem $(P_\mu)$ resembles
more and more problem $(P)$.

Note that the objective function of problem $(P_\mu)$
is a strictly convex function. 
Therefore, for a fixed $\mu$, the problem has at most one global minimum. 
This global minimum, if it exists, is completely characterised 
by the associated KKT conditions:
\[
\begin{array}{rcc}
   Ax               & = &  b \\
  \mu X^{-1}e -A^Ty & = & -c. 
\end{array}
\]
By introducing the notation $s = \mu X^{-1}e$, we obtain the
standard formulation of the so called {\em perturbed KKT conditions}:
\be \label{eq:PerturbedKKT}
\begin{array}{rcc}
   Ax       & = & b \\
   A^Ty + s & = & c \\
   XSe      & = & \mu e \\
   (x,s)    & > & 0.
\end{array}
\ee

If the feasible domain $\mathcal{P}$ is bounded, 
then both $(P)$ and $(P_\mu)$ have optimal solution. 

The following lemma has been proved by Megiddo \cite{Megiddo}.
\begin{lemma}
Problem $(P_\mu)$ 
is either unbounded for every  $\mu>0$ or has a unique optimal 
solution for every $\mu>0$.
\end{lemma}

If the perturbed KKT system (\ref{eq:PerturbedKKT}) has a solution for 
any $\mu>0$, then it determines a unique continuous path $(x(\mu),s(\mu))$ 
toward the optimal set as $\mu\to 0$. In interior-point terminology, 
such a path is called the {\em central path}.
We postpone its presentation to Section~\ref{sec:CentralPath}.

Moreover, if $A$ has full rank, then the value of $y$ is 
uniquely determined by the value of $x$. Therefore, 
system (\ref{eq:PerturbedKKT}) has a unique solution $(x(\mu),y(\mu))$.

Megiddo shows that $c^Tx(\mu)\to c^Tx^*$ as $\mu\to 0$. 
Furthermore, he proves the stronger result that 
$x(\mu)\to x^*$ as $\mu\to 0$.

\hrulefill

Interior point methods arrive to a solution that satisfies the KKT
conditions (\ref{eq:KKT}) by ``relaxing'' the complementarity constraints.
%
Path-following interior point methods \cite{ipm:Wright97} perturb 
the above conditions by asking the complementarity pairs to align 
to a specific barrier parameter $\mu > 0$,
\[
XSe = \mu e,
\]
while enforcing $(x,s)>0$.

\fb{
The above inequality cannot be strictly enforced, as it would not make
the solution of the KKT conditions (\ref{eq:KKT}) any easier. Instead,
the concept of neighbourhood is required, and is presented in
Section~(\ref{sec:Neighbourhoods}).
}

As $\mu$ is monotonically decreased at each iteration, the solution of the 
perturbed KKT conditions (\ref{eq:PerturbedKKT}) approximates better and better
the system (\ref{eq:KKT}) of optimality conditions for the original
problem.
The way in which the barrier parameter $\mu$ is updated at each iteration
is algorithm-dependent. We provide some theoretical insights on some
possible choices in Section~\ref{sec:TheoreticalResults}.

Under the assumptions that for a particular $\mu > 0$ the point
$(x(\mu),y(\mu),s(\mu))$ is primal and dual feasible, we can show
the following result:
\begin{eqnarray*}
  g(\mu) & = & c^Tx(\mu) - b^T y(\mu) \\
         & = & c^Tx(\mu) - x(\mu)^T A^T y(\mu) \\
         & = & ( c^T - y(\mu)^T A) x(\mu) \\
	 & = & s(\mu)^T x(\mu),
\end{eqnarray*}
that is, the duality gap corresponds to the complementarity gap.
Hence reducing either of them is identical.

Also, as $XSe - \mu e = 0$ implies $x_is_i = \mu$, $i = 1, \ldots, n$, 
we have
\[
   s(\mu)^T x(\mu) = n\mu,
\]
and for $\mu \to 0$, also $g(\mu) \to 0$.

\begin{lemma}
Under the assumptions of primal feasibility, dual feasibility, and
full row rank of matrix $A$, as $\mu \to 0$ then 
\[
   x(\mu) \to x^*, \qquad (y(\mu),s(\mu)) \to (y^*, s^*).
\]
\end{lemma}

\begin{theorem}[Strict complementarity]
If $(P)$ and $(D)$ are feasible, then there exist a point $x^* \in\mathcal{P}$
and a pair $(y^*,s^*) \in \mathcal{D}$ such that
\[
(x^*)^T s^* = 0 \quad\mbox{ and }\quad x^*_i + s_i^* >0, \;\; i = 1, \ldots, n.
\]
\end{theorem}

A solution $(x^*,s^*)$ that satisfies the above theorem is called 
{\em strictly complementary}. On the grounds of a strictly complementary
solution we can define the concept of {\em optimal partition}.

\fb{
The central path leads to a strictly complementary solution.
}

Contrary to active set algorithms, interior point methods reach a
solution only asymptotically. 
Because of the presence of the barrier term, they can never produce
an exact solution, but terminate when an iterate satisfies, 
according to the desired accuracy, some approximation of the
optimality conditions.

However, once a solution has been found within a prescribed 
optimality tolerance, such a point can be projected onto a face 
of the polyhedron in an efficient way.
This can be done thanks to a strongly polynomial algorithm due
to Megiddo \cite{Megiddo91}. This procedure goes under the name
of {\em basis crossover}, 
as given a complementary primal--dual solution it generates a basis 
that is both primal and dual feasible.

However, we should discuss what we mean by ``exact'' solution. In most
cases we do not need the additional precision of being on a vertex (in
this case, integer programming is the notable exception).
Also, in case of multiple solutions, interior point methods terminate
at the analytic center of the optimal face rather than at a vertex; 
but note the arbitrariness of the simplex in the choice of the solution
vertex.
This difference has important consequences on the use of the solution
for sensitivity analysis. 

\fb{
Find the correct citations! Have a look at Jansen's and Yildirim's phds.
}

%
%
\subsection{The central path}
\label{sec:CentralPath}

The study of the primal--dual properties of central path has been pioneered by
Megiddo \cite{Megiddo} and Bayer and Lagarias \cite{BayerLagarias}.

The properties of the curvature of the central path are discussed
in \cite{VavasisYe}, where it is explained that the central path
is characterised by curves of high order. 
Such curves appear in correspondence with changes in the optimal
partition.
Close to the end, when the optimal partition has been identified,
the central path becomes a straight line \cite{Megiddo}.
In this region, the quadratic convergence property of Newton's
method kicks in.

We have seen that as $\mu \to 0$ the solution to $(P_\mu)$ 
converges to the analytic center of the set of optimal solutions of $(P)$.
We now discuss the opposite situation, that is we consider
the limit of $(P_\mu)$ for $\mu \to \infty$, and therefore
find the point from which the central path departs.
This point, the {\em analytic center}, is defined as
\[
\theta = \arg \min_{x \in \mathcal{P}^0} -\sum_j \ln x_j,
\]
and this concept is well defined, given the strict convexity of the 
barrier function. The concept of analytic center was first 
introduced by Sonnevend.

The analytic center is the point that minimizes the barrier, hence
is the point farthest away from the boundary.
However, there is a problem with defining the central path in terms
of analytic center: the central path is affected by the presence of 
redundant constraints. 
This happens because it is an exclusively analytic concept, which does
not exploit geometric considerations.

\fb{
Gonzaga \cite{Gonzaga92} mentions various centers (center of gravity, 
analytic center). See also Nick Gould's slides at cerfacs, and
discussion with Coralia (she mentioned a chapter in Ye's book).

Terlaky's  Klee-Minty example.
}

\fb{
In \cite[Section 8]{Gonzaga92} there is a nice part on scaling,
and also on primal--dual scaling.
}

Upon discussing the choice for the centering term in his algorithm,
Mehrotra \cite{Mehrotra92} makes this comment:
\begin{quote}
It is not clear if the central path (with
equal weights) is the best path to follow, particularly since it
is affected by the presence of redundant constraints. Furthermore,
the points on (or near) the central path are only intermediate to
solving the linear programming problem. It is only the limit point 
on this path that is of interest to us.
\end{quote}


%
%
\subsection{Neighbourhoods of the central path}
\label{sec:Neighbourhoods}

As we have seen, following the central path provides the recommended
way of traversing the interior of the feasible region towards
the optimal solution. Nevertheless, it should be clear that keeping the
iterates {\em exactly on} the central path is as difficult as
solving the optimization problem itself.
%
Therefore, we never insist in this extremely restrictive requirement,
but we rather allow the iterates to be somewhere around the central path.
This leads to the introduction of the important concept of
{\em neighbourhood} of the central path. 
We can define different neighbourhoods, characterised
by different properties.
Two neighbourhoods are often used in theoretical developments.

The first is based on the Euclidean norm, and it is often referred
to as the {\em tight neighbourhood}:
\[
\mathcal{N}_2(\theta) = \{ (x,y,s) \in \mathcal{F}^0 :
                         \| XSe - \mu e \|_2 \le \theta\mu \}.
\]
This neighbourhood follows very closely the central path:
search directions generated from points in this neighbourhood can be 
followed with full step, and the barrier parameter can be decreased
by a small amount at each iteration (giving rise to the name
of {\em short-step algorithms} to the algorithms that are based on
this neighbourhood). 
The closeness to the central path that the tight neighbourhood
imposes and maintains allows to produce the best convergence result
for linear programming: short-step algorithms converge in 
$\bigO(\sqrt{n}L)$ iterations.
However, since the reduction in the barrier parameter at each iteration 
is very small, the practical value of short-step algorithms is small.

The other commonly used neighbourhood is instead based on the infinity norm, 
and it is often called {\em wide neighbourhood}:
\[
\mathcal{N}_{-\infty}(\gamma) = \{ (x,y,s) \in \mathcal{F}^0 :
                         x_is_i \ge \gamma\mu, \; \forall i \}.
\]
Algorithms based on such a neighbourhood are allowed to generate
iterates that follow more loosely the central path. The iterates 
have more freedom of movement as they can approach the border.
Also, algorithms based on the wide neighbourhood (usually denoted as
{\em long-step algorithms}) are less conservative and can decrease 
the barrier parameter more rapidly.
However, the Newton direction computed from points in the wide 
neighbourhood  has weaker properties, and a linesearch procedure is
needed to ensure that the positivity of the $(x,s)$ iterates is
preserved.

In Section (\ref{sec:SymNeighbourhood}) we will study a variation
of the $\mathcal{N}_{-\infty}$ neighbourhood which better describes
the centrality requirements needed for a practical algorithm.

\fb{
$N_2(\beta) \subseteq N_\infty(\beta) (= \| XSe - \mu e\|_\infty) \subseteq N_{-\infty}(\beta)$
}


%
%
\subsection{Solving the perturbed KKT conditions}

Path-following interior point methods seek a solution 
to the system of equations
\[
F(x,y,s) = \left[
  \begin{array}{c}
    Ax-b \\
    A^Ty+s-c \\
    XSe - \mu e \\
  \end{array} \right] = 0,
\]
which is nonlinear in the perturbed complementarity constraints.
We use Newton's method to linearise the system according to
\[
\nabla F(x,y,s) \Delta(x,y,s) = -F(x,y,s),
\]
and obtain the so-called step equations
%
\be \label{eq:NewtonSystem}
\left[ \begin{array}{ccc}
    A & 0 & 0 \\ 0 & A^T & I \\ S & 0 & X
  \end{array} \right]
\left[ \begin{array}{c}
    \Delta x \\  \Delta y \\  \Delta s
  \end{array} \right] =
\left[ \begin{array}{c}
    b - Ax \\ c - A^Ty - s \\ -XSe + \mu e
   \end{array} \right] =
\left[ \begin{array}{c}
    \xi_b \\ \xi_c \\ \xi_\mu
   \end{array} \right],
\ee
%
which need to be solved with a specified $\mu$ for a search direction
$(\Delta x, \Delta y, \Delta s)$. Throughout this thesis, we will 
restrict our attention to using a direct approach in solving these
equations.

System (\ref{eq:NewtonSystem}) is usually reduced to two other
formulations by exploiting the block structure of the constraint
matrix.
%
The {\em augmented system} formulation is obtained by using 
the last row of (\ref{eq:NewtonSystem}) to eliminate
$\Delta s = X^{-1} (\xi_\mu - S\Delta x)$.
This produces
%
\be \label{eq:AugmentedSystem}
\left[ \begin{array}{cc}
    -X^{-1}S & A^T \\ A & 0
  \end{array} \right]
\left[ \begin{array}{c}
    \Delta x \\  \Delta y
  \end{array} \right] =
\left[ \begin{array}{c}
    \xi_c - X^{-1}\xi_\mu \\ \xi_b
   \end{array} \right],
\ee
which is a symmetric but indefinite system.
%
By further eliminating $\Delta x$, we reduce system 
(\ref{eq:AugmentedSystem}) to the set of {\em normal equations}
%
\be \label{eq:NormalEquations}
  A D^2 A^T \Delta y = A D^2 (\xi_c - X^{-1} \xi_\mu) + \xi_b,
\ee
%
where we introduced the notation $D^2 = S^{-1} X$.
Under the assumption of full row rank for $A$, matrix 
$A D^2 A^T$ is positive definite, since $D^2_i = x_i/s_i > 0$ for
all $i = 1, \ldots, n$.

Besides the issue of definiteness, the two formulations differ in
terms of sparsity and conditioning, the normal equations usually 
being dense and worse conditioned.

\fb{
Compare the two approaches, and discuss what happens in software.\\
Explain the dependence on linear algebra.

The augmented system uses the Bunch-Parlett factorization.
}

The choice between augmented system and normal equations depends also on 
the relative density of $AD^2A^T$ with respect to $A$.
Normal equations are to be avoided when there are dense columns in $A$, 
as they generate dense blocks in $AD^2A^T$.

Maros and M\'esz\'aros \cite{MarosMeszaros} presented an in-depth 
study of the properties of the augmented system formulation.

%
% Section
%
\section{Theoretical results}
\label{sec:TheoreticalResults}

Theoretical developments aim at lowering the upper bound on the number 
of steps needed for convergence. The results provided by such worst-case 
complexity analysis are informative but exceedingly pessimistic. 

The theoretical analysis for the logarithmic barrier method was
done by Megiddo \cite{Megiddo}, who also presented a primal--dual
framework in 1986. 

On the basis of this seminal paper, Kojima, Mizuno and Yoshise (1987) 
presented a polynomial-time primal--dual algorithm for linear programming, 
with the property of convergence in $\bigO(nL)$ iterations.

Further improvements were then introduced by Monteiro and Adler 
\cite{MonteiroAdler89a}, who refined the algorithm and showed that 
the use of the small $N_2$ neighbourhood allows to reach the solution 
in $\bigO(\sqrt{n}L)$ iterations.

\fb{
Not sure about that reference! Mehrotra actually cites another paper
from Monteiro and Adler.
}

Theoretical proofs of complexity generally follow a common scheme.
First they rely on a computable measure of the closeness to the central
path: this is accomplished by the concept of neighbourhood. Second,
they show that the direction computed by solving the Newton system
(\ref{eq:NewtonSystem}) can be followed with a strictly positive step
(and therefore guarantees some progress) and generates an iterate 
that retains the property of being in some neighbourhood of the central 
path (possibly larger than the one before). Finally, they require
a decrease in the barrier parameter that allows a polynomial upper
bound on the number of iterations.

%
%
\subsection{Feasible methods}

In primal--dual feasible algorithms for linear programming, 
all primal and dual iterates always lie within the interior 
of the feasible region. For this reason, these algorithms 
need to start from a strictly feasible point. 

Convergence of the algorithm (Kojima-Mizuno-Yoshise): 
Let $(x', y', s') = (x^0, y^0, s^0) + \alpha(\Delta x, \Delta y, \Delta s)$, 
then:
\begin{itemize}
\item $\xi'_P = (1-\alpha) \xi^0_P$
\item $\xi'_D = (1-\alpha) \xi^0_D$
\item $x'^Ts' = (1-\alpha (1 -\sigma))(x^0)^Ts^0 +\alpha^2 \Delta x^T \Delta s$
\end{itemize}
so the reduction in infeasibilities is linear, while complementarity reduces
for a small $\alpha$ (but if the point is feasible, then the reduction is
linear as well, since $\Delta x^T \Delta s = 0$.)

In complexity proof, the barrier parameter is reduced slightly at
each iteration in order to guarantee that one iteration of Newton's
method can keep the point in the small neighbourhood of the central path.
However, this is a worst-case analysis, and in practice the same happens
even for a bigger update of the barrier parameter. This brings to
study ways of allowing a more substantial reduction of the barrier
parameter, at least in some iterations. One important result is this
direction is the Mizuno-Todd-Ye predictor--corrector algorithm.

%
%
\subsection{Mizuno-Todd-Ye predictor--corrector algorithm}

Mizuno, Todd and Ye \cite{MizunoToddYe} analysed the short-step 
predictor--corrector method. Their strategy uses two nested neighbourhoods 
$N_2(\theta^2)$ and $N_2(\theta)$, $\theta \in (0,1)$, and exploits the
quadratic convergence property of Newton's method in this type of 
neighbourhood.
Their algorithm computes two search directions at each iterations.
First, the predictor direction gains optimality, possibly at the expense of
worsening the centrality, keeping the iterate in a larger neighbourhood
$N_2(\theta)$ of the central path. Then, a pure re-centering step 
is performed, which leaves the duality gap unchanged but 
moves the iterate back into a 
tighter $N_2(\theta^2)$ neighbourhood. Hence, every second step the 
algorithm produces a point in $N_2(\theta^2)$. This is a clever 
approach, but the use of the very restrictive $N_2$ neighbourhood 
makes it unattractive for practical applications.

An important contribution of this technique, however, is the idea 
of targeting optimality and centrality independently. While this 
algorithm is not effective in practical terms, it provides a scheme 
upon which more computationally attractive methods can be constructed.

Thanks to the optimizing predictor direction which is identical
to a short-step feasible method, also the predictor--corrector
algorithm converges in $\bigO(\sqrt{n}L)$ iterations, with the only
difference that the value of the barrier parameter is reduced over
two iterations.

%
%
\subsection{Infeasible methods}
\label{sec:InfeasibleMethods}

The results presented up to here concentrated on feasible methods. 
For these methods we assume to have a strictly feasible starting point
readily available.
However, this is not the case in real-life situations, where finding 
a strictly feasible  starting point is, in general, a nontrivial task
and can be considered a separate subproblem.
Moreover, the feasible region may have empty interior,
in which case the theory developed above does not apply.
For these reasons, a need exists for practical 
implementations to dispense from this requirement.

A way to recover a strictly feasible starting point involves 
solving an artificial Phase~I subproblem by using 
the big-$M$ method. However, the performance is dependent on 
the choice of the values given to the weights, and the use
of very large values, while theoretically satifying,
causes numerical instabilities \cite{Lustig91}.
This is worsened 
by the presence of dense columns that compromises the 
computational efficiency. A very different approach to deal 
with the starting point issue is the self-dual formulation.

\fb{
Find a reference for that (Terlaky-Ye?).
}

It is possible to develop an algorithm which only requires 
the $x$ and $s$ components to be strictly positive. 

\fb{
This was achieved by Kojima, Megiddo and Mizuno \cite{KojimaMegiddoMizuno}.

Sure? Wright (page 109) says it was Lustig Marsten and Shanno.
}

In such 
an algorithm, all iterates are infeasible, but the limit points 
are feasible and optimal. This is obtained by using a 
neighbourhood that admits infeasible points:
\[
\mathcal{N}_{-\infty}(\gamma,\beta) =\{ (x,\lambda,s) | \; \|(r_b,r_c)\| \le \beta\mu \frac{\|(r_b^0,r_c^0)\|}{\mu_0}, (x,s)>0, x_is_i \ge \gamma\mu \},
\]
where $\gamma\in (0,1)$ and $\beta \ge 1$ are parameters, and 
we denoted the primal and dual residuals, respectively, by 
$r_b = Ax-b$ and $r_c = A^T\lambda+s-c$.

Therefore, there is no strict feasibility requirement for 
the iterates; however, the residuals at each iteration must be 
bounded above by a multiple of the duality measure $\mu$. 
By reducing $\mu$ we can force the primal and dual residuals 
$r_b$ and $r_c$ to zero, thus approaching complementarity and 
feasibility at the same speed.

Kojima, Megiddo and Mizuno \cite{KojimaMegiddoMizuno} proposed 
a stepsize rule that guarantees global convergence of an 
infeasible interior-point algorithm.
An algorithm is globally convergent if it is possible to choose
a strictly positive stepsize such that the complementarity gap
is reduced at each iteration. This property is very important, at it
guarantees the good behaviour of the algorithm for any given starting
point. However, in order to prove it, it is required that some
safeguards are implemented:
\begin{itemize}
\item reduction in infeasibility should be faster than in complementarity
\item $x_i s_i \ge \beta x^Ts$ for a chosen $\beta$, and all iterates must
stay in this neighbourhood.
\end{itemize}

\fb{
Order of convergence: $\bigO(n^2L)$ in Wright.
}

%
% Section
%
\section{Symmetric neighbourhood}
\label{sec:SymNeighbourhood}

The quality of centrality (understood in a simplified way as complementarity)
is {\it not} well characterised by either of two neighbourhoods 
$N_2$ or $N_{-\infty}$ commonly used in theoretical developments of IPMs.
Instead, we propose to use a symmetric neighbourhood $N_s(\gamma)$,
in which complementarity pairs have to satisfy 
$\gamma \mu \leq x_i s_i \leq \gamma^{-1} \mu$, where $\gamma \in (0,1)$, 
for a strictly feasible iterate $(x,y,s)$.

\fb{
Jansen \cite{JansenRoosTerlaky96} (or actually \cite{phd:Jansen}?)
proposed to use the ratio 
between the smallest and the largest complementarity pair as an 
indication of the quality of a point.

It mentions that was Atkinson and Vaidya \cite{AtkinsonVaidya} 
that noticed that as the ratio increases, the region in which 
Newton converges becomes smaller.
}

Practical experience with the primal--dual algorithm in HOPDM \cite{HOPDM}
suggests that one of the features responsible 
for its efficiency is the way in which the quality of centrality 
is assessed. By ``centrality'' we understand here the spread 
of complementarity products $x_i s_i$, $i = 1,\dots,n$.
Large discrepancies within the complementarity 
pairs, and therefore bad centering, create problems for the search 
directions: an unsuccessful iteration is caused not only by small
complementarity products, but also by very large ones.
%
 This can be explained by the fact that
Newton's direction tries to compensate for very 
large products, as they provide the largest gain in complementarity 
gap when a full step is taken. However, the direction thus generated 
may not properly consider the presence of very small products, 
which then become blocking components when the stepsizes are computed.

The notion of spread in complementarity products
is not well characterised by either 
of the two neighbourhoods $N_2$ or $N_{-\infty}$ commonly used 
in theoretical developments of IPMs.
To overcome this disadvantage, here we formalise a variation 
on the usual $\mathcal{N}_{-\infty}(\gamma)$ neighbourhood, 
in which we introduce an upper bound on the complementary pairs. 
This neighbourhood was implicitly used in Section~\ref{sec:MultipleCC}
to define an achievable target for multiple centrality correctors.
We define the symmetric neighbourhood to be the set
\[
  \mathcal{N}_s(\gamma)=\{(x,y,s)\in \mathcal{F}^0: 
  \gamma\mu\le x_is_i \le \frac{1}{\gamma}\mu, \; i=1,\ldots,n\},
\]
where $\mathcal{F}^0=\{(x,y,s) : Ax=b,A^Ty+s=c,(x,s)>0\}$ 
is the set of strictly feasible primal--dual points,
$\mu = x^Ts/n$, and $\gamma \in (0,1)$.

While the $\mathcal{N}_{-\infty}$ neighbourhood ensures that some 
products do not approach zero too early, it does not prevent products
from becoming too large with respect to the average.
In other words, it does not provide a complete 
picture of the centrality of the iterate. The symmetric 
neighbourhood $\mathcal{N}_s$, on the other hand, promotes 
the decrease of complementarity pairs which are too large, thus taking 
better care of centrality.

\fb{
Actually also the $\mathcal{N}_{-\infty}$ neighbourhood expresses an upper
bound, but it's much higher.
}

The analysis is done for the long-step feasible path-following 
algorithm, where the search direction $(\Delta x, \Delta y, \Delta s)$ 
is found by solving system (\ref{eq:NewtonSystem}) with 
$r=(0,\; 0,-XSe+\sigma\mu e)^T$, $\sigma\in(0,1)$, $\mu=x^Ts/n$.
%
The exposition follows closely the presentation of 
\cite[Chapter~5]{ipm:Wright97}. 

First we need a technical result, the proof of which can be found 
in \cite[Lemma~5.10]{ipm:Wright97} and is unchanged by the use 
of $\mathcal{N}_s$ rather than $\mathcal{N}_{-\infty}$.
%
\begin{lemma} \label{Wright:5.10}
If $(x,y,s)\in \mathcal{N}_s(\gamma)$, then\,
\(
  \|\Delta X\Delta Se\| \le 2^{-3/2}\left( 1+ \displaystyle{\frac{1}{\gamma}} \right)n\mu.
\)
\end{lemma}

Our main result is presented in Theorem~\ref{th:SymNeighbourhood}. 
We prove that it is possible to find a strictly positive stepsize 
$\alpha$ such that the new iterate 
$(x(\alpha),y(\alpha),s(\alpha))=(x,y,s)+\alpha(\Delta x,\Delta y,\Delta s)$
will not leave the symmetric neighbourhood, and thus this 
neighbourhood is well defined. This result extends 
Theorem~5.11 in \cite{ipm:Wright97}.

\begin{theorem} \label{th:SymNeighbourhood}
  If $(x,y,s)\in\mathcal{N}_s(\gamma)$, then 
  $\left(x(\alpha),y(\alpha),s(\alpha)\right)\in\mathcal{N}_s(\gamma)$ for all
  \[
  \alpha\in \left[0,2^{3/2}\gamma\frac{1-\gamma}{1+\gamma}\frac{\sigma}{n} \right].
  \]
\end{theorem}
\begin{proof}
Let us express the complementarity product in terms of the stepsize 
$\alpha$ along the direction $(\Delta x, \Delta y, \Delta s)$:
%
\begin{eqnarray} \label{eq:CompProdAlpha}
x_i(\alpha)s_i(\alpha)&=&(x_i+\alpha\Delta x_i)(s_i+\alpha\Delta s_i)\nonumber\\ 
&=& x_is_i+\alpha(x_i\Delta s_i +s_i\Delta x_i) +\alpha^2\Delta x_i\Delta s_i\\
&=& (1-\alpha)x_is_i + \alpha\sigma\mu + \alpha^2\Delta x_i\Delta s_i.\nonumber
\end{eqnarray}
%
We need to study what happens to this complementarity product 
with respect to both bounds of the symmetric neighbourhood.
%
Let us first consider the bound $x_is_i\le \frac{1}{\gamma}\mu$.
By Lemma~\ref{Wright:5.10}, equation (\ref{eq:CompProdAlpha}) implies
\[
x_i(\alpha)s_i(\alpha) \le (1-\alpha)\frac{1}{\gamma}\mu +\alpha\sigma\mu 
+ \alpha^2 2^{-3/2}\left( 1+ \frac{1}{\gamma} \right)n\mu.
\]
At the new point $(x(\alpha),y(\alpha),s(\alpha))$, the new duality gap
is $x(\alpha)^Ts(\alpha) = n\mu(\alpha)$.
The relation $x_i(\alpha)s_i(\alpha)\le \frac{1}{\gamma}\mu(\alpha)$ 
holds provided that
\[
(1-\alpha)\frac{1}{\gamma}\mu +\alpha\sigma\mu + \alpha^2 2^{-3/2}\left( 1+ \frac{1}{\gamma} \right)n\mu 
\le\frac{1}{\gamma}(1-\alpha+\alpha\sigma)\mu,
\]
from which we derive a first bound on the stepsize:
\[
\alpha \le 2^{3/2}\frac{1-\gamma}{1+\gamma}\frac{\sigma}{n} = \bar\alpha_1.
\]
%
Considering now the bound $x_is_i\ge \gamma\mu$ and proceeding as before,
%By Lemma~\ref{Wright:5.10}, equation (\ref{eq:CompProdAlpha}) implies
%\[
%x_i(\alpha)s_i(\alpha) \ge (1-\alpha)\gamma\mu + \alpha\sigma\mu 
%- \alpha^2 2^{-3/2}\left( 1+ \frac{1}{\gamma} \right)n\mu.
%\]
%Hence, $x_i(\alpha)s_i(\alpha)\ge \gamma\mu(\alpha)$ provided that
%\[
%(1-\alpha)\gamma\mu + \alpha\sigma\mu- \alpha^2 2^{-3/2}\left( 1+ \frac{1}{\gamma} \right)n\mu 
%\ge\gamma(1-\alpha+\alpha\sigma)\mu,
%\]
we derive a second bound on the stepsize:
\[
\alpha\le 2^{3/2}\gamma\frac{1-\gamma}{1+\gamma}\frac{\sigma}{n} =\bar\alpha_2.
\]

Therefore, we satisfy both bounds and guarantee that 
$(x(\alpha),y(\alpha),s(\alpha))\in \mathcal{N}_s(\gamma)$ if
\[
\alpha \in [0,\min(\bar\alpha_1,\bar\alpha_2)],
\]
which proves the claim, as $\gamma \in (0,1)$.
\end{proof}

It is interesting to note that the introduction of the upper bound 
on the complementarity pairs does not change the polynomial complexity 
result proved for the $\mathcal{N}_{-\infty}(\gamma)$ neighbourhood 
\cite[Theorem~5.12]{ipm:Wright97}. Therefore, the symmetric 
neighbourhood provides a better practical environment without any 
theoretical loss. This understanding provides some additional 
insight into the desired characteristics of a well-behaved iterate.
