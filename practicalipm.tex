%Started on 9th August 2006
%Aug: 11, 22, 23, 24, 25, 28, 29, 30, 31
%Sep:  1,  7,  8
% 2007
%Jan: 15, 16, 17, 22
%Feb:  1,  2,  4,  5,  6,  8,  9, 11

%
% Chapter: Practical implementations of Interior point methods
%
\label{ch:PracticalIpm}

In this chapter we turn our attention to the computational side of
interior point methods. We concentrate on the main strategies which are
at the basis of effective implementations of interior point methods
for linear programming, and present other issues that are peculiar 
to practical algorithms.
These are documented in extensive literature (see, for example, 
\cite{AndersenGondzioMeszarosXu,GondzioTerlaky} and the references therein).


%
% Section
%
\section{A practical algorithm}

Interior point methods require the computation of the Newton 
direction (\ref{eq:NewtonSystem})
for the associated barrier problem and make a step along 
this direction, thus usually reducing infeasibilities 
and complementarity gap.
The computation of the Cholesky factorization
dominates the cost of each iteration.
As this is usually a major computational task, 
the efforts in the theory and practice of 
interior point methods concentrate on reducing 
the number of times the Newton system matrix (\ref{eq:NewtonSystem}) 
has to be factorised.

In Section~\ref{sec:TheoreticalResults} we presented the theoretical
results on the order of convergence of some interior point algorithms.
In practice, convergence is much faster than stated by those results:
optimality is usually reached in a number of iterations 
proportional to the logarithm of the problem dimension. 
As it happens with the analysis of the simplex method, we see quite
a gap between the predicted and the observed performance that is still to
be fully understood.

\fb{
Find a citation for the logarithm thing!
}

Practical algorithms are very different from the ones used for
theoretical purposes, and usually they implement some variation
of infeasible interior point algorithms. In particular they show
differences in the search directions, the evaluation of the stepsize, 
the use of the neighbourhood concept, and the update of the
barrier parameter.
This is further complicated by issues of computational efficiency
and numerical stability, which often suggest the use of amended
techniques or heuristic approaches, which make extremely difficult 
the analysis of the algorithms implemented in practice.

The computation of different stepsizes in the primal and dual spaces is
almost the rule for linear programming.
% (while for quadratic programs the stepsizes have to be identical). 
This has the advantage of
speeding up the restoration of feasibility. According to
\cite{GondzioTerlaky}, the use of different stepsizes contributes 
to a reduction in number of iterations of 10\% on the Netlib
set of tests.

In order to ensure that $(x,s)$ remain positive after moving along the
$\Delta w$ direction, we need to employ a linesearch procedure 
and find the maximum feasible stepsizes $\alpha_P$ and $\alpha_D$ 
such that
\[
  x + \alpha_P \Delta x > 0, \qquad  s + \alpha_D \Delta s > 0.
\]
The achievable stepsizes for a given search direction $\Delta w$, 
in the primal and dual space respectively, are computed as:
\be  \label{eq:Alphas}
  \alpha_P =\min \left\{ -\frac{x_i}{\Delta x_i} : \Delta x_i < 0 \right\},
  \quad\;
  \alpha_D =\min \left\{ -\frac{s_i}{\Delta s_i} : \Delta s_i < 0 \right\}.
\ee

We remark that in the computation of the stepsizes, we 
maintain the strict positivity of the iterates, without restricting
the point inside a neighbourhood.
While this is done on the grounds of computational efficiency, we may
lose the property of global convergence ensured by keeping the
iterates in a neighbourhood of the central path.

Upon discussing the choice for the centering term in his algorithm,
Mehrotra \cite{Mehrotra92} makes this comment:
\begin{quote}
It is not clear if the central path (with
equal weights) is the best path to follow, particularly since it
is affected by the presence of redundant constraints. Furthermore,
the points on (or near) the central path are only intermediate to
solving the linear programming problem. It is only the limit point 
on this path that is of interest to us.
\end{quote}

%
%
\subsection{Correcting techniques}

Two techniques have proved particularly successful in reducing 
the number of iterations within practical algorithms:
Mehrotra's predictor--corrector algorithm \cite{Mehrotra92} 
and multiple centrality correctors \cite{Gondzio96}. These 
techniques have been implemented in most of commercial and academic 
interior point solvers for linear and quadratic programming such 
as BPMPD, Cplex, HOPDM, Mosek, OOPS, OOQP, PCx and Xpress. 

\ignore{
They have also been used with success in semidefinite 
programming with IPMs \cite{Haeberly99}.
}

Both correcting techniques originate from the observation that 
(when direct methods of linear algebra are used) the computation 
of the Newton direction requires two computationally intensive
operations: 
\begin{enumerate}
\item Factorization: computation of the Cholesky factors of a sparse 
symmetric matrix; 
\item Backsolve: use of the factors just computed to solve the system. 
\end{enumerate}
However, the cost of computing the factors is usually significantly 
larger than that of backsolving: in some cases the ratio between 
these two computational efforts may even exceed 1000. 

\fb{
Present a small table that shows the cost of factoring and of backsolving
for HOPDM (and PC-x?).
}

Consequently, it is worth adding more (cheap) 
backsolves if this reduces the number of (expensive) factorizations. 
Mehrotra's predictor--corrector technique \cite{Mehrotra92} uses two 
backsolves per factorization; the multiple centrality correctors technique
\cite{Gondzio96} allows recursive corrections: a larger number 
of backsolves per iteration is allowed, leading to a further reduction 
in the overall number of factorizations. 

Since these two methods were developed, there have been a number of 
attempts to investigate their behaviour rigorously and thus provide
further insight on their successfulness. 
Such objectives are difficult to achieve because 
correctors use heuristics that are effective in practice but hard 
to analyse theoretically. 
Besides, both correcting techniques are applied to long-step and infeasible 
algorithms which have very little in common with the short-step and 
feasible algorithms that display the best known theoretical complexity.

These two strategies will be the focus of the next sections.


%
% Section
%
\section{Mehrotra's predictor--corrector algorithm}
\label{sec:MehrotraPC}

Practical implementations usually follow Mehrotra's predictor--corrector 
algorithm \cite{Mehrotra92}. In such a framework, we first generate a 
predictor direction to make progress towards optimality, and then we 
compute a corrector to remedy for some of the error made by the predictor
and move the point closer to the central path.

A number of advantages can be obtained by splitting the computation 
of the Newton direction into two steps, corresponding to solving the linear
system (\ref{eq:NewtonSystem}) independently for the two right-hand 
sides 
\be \label{eq:PredictorRhs}
r_1 =\left[ 
  \begin{array}{c}
    b-Ax \\ c-A^Ty-s \\ -XSe
  \end{array} \right] \quad \mbox{and} \quad
r_2 =\left[ 
  \begin{array}{c}
    0 \\ 0 \\ \sigma\mu e
  \end{array} \right].
\ee

First, we can postpone the choice of the centering parameter 
$\sigma$ and base it on the assessment of the quality of the 
pure Newton direction computed with right-hand side $r_1$;
second, the error made by this direction may be 
taken into account and
corrected. Mehrotra's predictor--corrector technique \cite{Mehrotra92}
translates these observations into a powerful computational method.

Mehrotra's predictor--corrector algorithm \cite{Mehrotra92,LustigMarstenShanno}
is extremely efficient in practice. Since its introduction, it has 
been the considered the method of choice for practical implementations 
because it is usually very fast and reliable. Moreover, it has a 
convincing interpretation in terms of second order approximations.

%
%
\subsection{Affine-scaling predictor direction}

The {\em predictor direction} 
$\Delta_a w = (\Delta_a x, \Delta_a y, \Delta_a s)$ is obtained by solving 
system (\ref{eq:NewtonSystem}) with right-hand side $r_1$ defined 
in (\ref{eq:PredictorRhs}).
This corresponds to computing the pure Newton direction for the
original KKT system (\ref{eq:KKT}), and for this reason is often
called {\em affine-scaling} direction. This 
direction is strongly optimizing, as it targets a point for which 
all complementarity products go to zero. 
The achievable stepsizes for the predictor direction, 
in the primal and dual space respectively, are computed 
according to (\ref{eq:Alphas}).

As it targets a point for which $XSe = 0$, the affine-scaling direction 
may be distracted by points that have small complementarity products 
but are not optimal. 
In particular, it may well point towards the boundary of the 
positive orthant or approach an infeasible vertex, generating 
a very small stepsize. 
This issue is usually worsened if the current iterate is badly centered, 
and therefore only a very small step is acceptable in order to 
maintain positivity or to keep the iterate in the neighbourhood 
of the central path.

Since it completely ignores the central path, the affine-scaling 
direction is not enough in a practical implementation of 
interior point methods, but it has to be 
complemented by other techniques.
The role of the centering term is to remedy this situation by 
affecting the search direction to move closer to the central path, 
therefore allowing a longer stepsize. 

Tapia et al. \cite{TapiaZhangSaltzmanWeiser} noted that the choice of the 
centering parameter can be crucial both in theory and in practice,
and suggest that it is a function of the Newton step. 
This calls for two separate backsolves at each iteration.

\ignore{
Another problem with affine scaling is that it is only a linear 
approximation to the central path. However, the central path is a curve
with many high-degree turns \cite{VavasisYe}, and only close to the 
solution set becomes approximately a straight line \cite{Megiddo}. 
}

%
%
\subsection{Second order corrector direction}

Mehrotra's algorithm exploits a centrality corrector in order to 
remedy to badly centered points. The purpose of this direction is 
to move closer to the central path, and therefore reduce the spread 
in complementarity products, without aiming for more optimality. 
This is a somewhat conservative direction, which it is hoped will 
provide more room for movement at the next iteration.

One tool introduced by Mehrotra \cite{Mehrotra92} is a dynamic evaluation 
of the centering parameter $\sigma$. It is based on a simple  
heuristic that evaluates the quality of the predictor direction
in order to judge the amount of centering term needed.
%
The length of the stepsizes $\alpha_P$ and $\alpha_D$ are used to 
predict the complementarity gap after such a step:
\be \label{eq:PredictedGap}
  g_a = (x + \alpha_P \Delta_a x)^T(s + \alpha_D \Delta_a s).
\ee

The ratio $g_a / x^{T}s \in (0,1)$ measures the quality of the 
predictor direction.
A small ratio indicates a successful reduction of the complementarity 
gap. On the other hand, if the ratio is close to one, then very little 
progress is achievable in direction $\Delta_a$, and a strong recentering 
is recommended.

In \cite{Mehrotra92} the following choice of the new barrier parameter 
is suggested
%
\be \label{eq:Mu}
  \mu = \left( \frac{g_a}{x^{T}s} \right)^{\! 2} \, \frac{g_a}{n}
           = \left( \frac{g_a}{x^{T}s} \right)^{\! 3} \, \frac{x^{T}s}{n},
\ee
%
corresponding to the choice of $\sigma = (g_a / x^Ts)^3$ 
for the centering parameter. 
\fb{
Mehrotra's heuristic was actually more elaborate.
}

The centering parameter, more generally could be chosen as
\[
  \sigma = \left( \frac{g_a}{x^Ts} \right)^p\!\!,
\]
for various choices of the exponent. Mehrotra \cite{Mehrotra92}
studied the effect of different values $p=1,2,3,4$ for the exponent
on a subset of Netlib problems, and concluded that for $p$ between
2 and 4 there was not much difference.
Also Lustig et al. \cite{LustigMarstenShanno} commented on the
weak dependence of the computational performance on the choice 
of the exponent.

If the predictor provides a good improvement, a small $\sigma$ 
is chosen, and therefore very little centering will be used. When, 
on the other hand, affine scaling produces very small stepsizes 
and so very little improvement can be achieved, $\sigma$ will be 
close to one, and so a stronger recentering will occur.

A further important contribution by Mehrotra consists in the 
introduction of a second-order direction. 
As said above, the affine-scaling direction 
corresponds to a linear approximation to the the trajectory from 
the current point to the optimal set, where no information about 
higher-order terms is taken into account. This linearisation, 
however, produces an error which can be determined analytically.
%
Assuming that a full step in the affine-scaling direction is made, 
the new complementarity products are equal to
%
\begin{eqnarray*}
  (X + \Delta_a X) (S + \Delta_a S) e 
   \;=\; XSe + (S \Delta_a x + X \Delta_a s) + \Delta_a X \Delta_a S e
   \;=\; \Delta_a X \Delta_a S e,
\end{eqnarray*}
%
as the third equation in the Newton system satisfies 
$S \Delta_a x + X \Delta_a s = -XSe.$
%
The term $\Delta_a X \Delta_a S e$ corresponds to the error introduced
by Newton's method in linearising the complementarity conditions of 
(\ref{eq:KKT}).

Ideally, we would like the next iterate to be perfectly centered: 
\[
  (X+\Delta X)(S+\Delta S)e = \sigma\mu e,
\]
which is equivalent to solving the nonlinear system
\be  \label{eq:SecondOrder}
  S\Delta x + X\Delta s = -XSe +\sigma\mu e - \Delta X\Delta Se.
\ee
By comparing (\ref{eq:SecondOrder}) and (\ref{eq:NewtonSystem}),
we see that 
the linearisation error made by the affine-scaling direction is exactly 
the $\Delta X\Delta Se$ term that is missing in the right-hand side
of the last equation of (\ref{eq:NewtonSystem}).

Mehrotra introduced a second-order correction in which the 
linearisation error is taken into account. Therefore, 
Mehrotra's corrector term is obtained by solving the Newton system 
(\ref{eq:NewtonSystem}) with right-hand side
\be \label{eq:MehrotraRhs}
r =\left[ \begin{array}{c}
    0 \\ 0 \\ -\Delta_a X\Delta_a Se + \sigma \mu e
  \end{array} \right],
\ee
for the direction $\Delta_c w = (\Delta_c x,\Delta_c y,\Delta_c s)$.
Such corrector direction combines the centrality term $\sigma \mu e$
and the second-order term $\Delta_a X\Delta_a Se$.

Once the predictor and corrector terms are computed, they are 
added to produce the composite predictor--corrector direction
\be \label{eq:CompositeDirection}
\Delta w = \Delta_a w+ \Delta_c w.
\ee
Note that the affine-scaling predictor and Mehrotra's corrector direction 
contribute with equal weights to the final search direction. 
This argument will be considered again in Chapter~\ref{ch:Correctors}, 
where we study the use of a weighting strategy for the corrector
directions.

The next iterate is given by
\[
w^{k+1} = w^k
        + (\alpha_P\Delta x^k,\alpha_D\Delta y^k,\alpha_D\Delta s^k)
\]
where $\alpha_P$ and $\alpha_D$ are chosen according to (\ref{eq:Alphas}).

For reasons of computational efficiency, in the computation of
the corrector, Mehrotra's algorithm exploits 
the same Jacobian matrix employed in finding the affine-scaling direction: 
hence, the same Cholesky factors are reused.
The cost of a single iteration in the predictor--corrector 
method is only slightly larger than that of the standard 
method because two backsolves per iteration have to be executed, 
one for the predictor and one for the corrector. 

The practical advantage of Mehrotra's predictor--corrector technique
is that it often produces longer stepsizes before violating the 
non-negativity constraints.
%
This usually translates in significant savings in the number of 
iterations: Mehrotra \cite{Mehrotra92} reports on savings of the
order of 35\%-50\% compared to other strategies.
For problems for which the factorisation cost is relevant, this
leads into significant 
CPU time savings \cite{LustigMarstenShanno,Mehrotra92}. Indeed, 
Mehrotra's predictor--corrector technique is advantageous in all 
interior point implementations for linear programming 
which use direct methods to compute 
the Newton direction.

\ignore{
It should be noted that in the computation of the predicted gap, 
it is assumed that a full step in the affine-scaling has been taken. 
}

As mentioned above, Mehrotra's way of assessing the value of $\sigma$
and the computation of the second order term is an heuristic procedure: 
thus there are 
no global convergence results or polynomial complexity results. 
Tapia et al. \cite{TapiaZhangSaltzmanWeiser} interpreted the Newton step 
produced by Mehrotra's predictor--corrector algorithm as a perturbed
composite Newton method and gave results on the order of convergence. 
They proved that a level-1 composite Newton method, when applied 
to the perturbed Karush-Kuhn-Tucker system, produces the same 
sequence of iterates as Mehrotra's predictor--corrector algorithm. 
While, in general, a level-$m$ composite Newton method has 
a $Q$-convergence rate of $m+2$ \cite{OrtegaRheinboldt},
the same result does not hold 
if the stepsize has to be damped to keep non-negativity of the iterates, 
as is necessary in an interior-point setting. However, under 
the additional assumptions of strict complementarity and nondegeneracy 
of the solution and feasibility of the starting point, Mehrotra's 
predictor--corrector method can be shown to have $Q$-cubic convergence
\cite{TapiaZhangSaltzmanWeiser}.


%
% Section
%
\section{Multiple centrality correctors}
\label{sec:MultipleCC}

Mehrotra's predictor--corrector, as it is implemented in optimization 
solvers \cite{LustigMarstenShanno,Mehrotra92}, is a very aggressive 
technique. It is based on the assumption, rarely satisfied, that a 
{\it full} step in the corrected direction will be achievable.
Moreover, an attempt to correct all complementarity products to the 
same value $\mu$ is also very demanding and occasionally
counterproductive. 
Besides, practitioners noticed that this technique may sometimes 
behave erratically, especially when used for a predictor direction 
applied from highly infeasible and not well centered points. 
\fb{
Is there a reference for that?

Consider how Salahi et al. \cite{SalahiPengTerlaky} modify the centrality
term in the corrector.
}
Finally, Mehrotra's corrector does not provide CPU time savings 
when used recursively \cite{CarpenterLustigMulveyShanno}.

Trying to provide a remedy to the above considerations, Gondzio 
\cite{Gondzio96} introduced the multiple centrality corrector technique 
as an additional tool to complement those presented by Mehrotra. 
The idea behind this technique is to ``force'' an increase in the 
length of the stepsizes by correcting the centrality of Mehrotra's 
iterate.
Instead of attempting to correct for the whole second-order error,
multiple centrality correctors 
concentrate on improving the complementarity pairs which really seem 
to hinder the progress of the algorithm, ie. the complementarity products 
that are far from the average.

We assume that a long-step path-following algorithm is used, 
and we work with the symmetric neighbourhood $\mathcal{N}_s(\gamma)$
of the central path as defined in Section~\ref{sec:SymNeighbourhood}.

\ignore{
In the authors' experience, such a neighbourhood best describes the desired 
properties of a ``well-centered'' interior point iterate.
}

%Like the previous one, this method also uses a composite direction 
%of the following form 
%\[
%  \Delta = \Delta_p + \Delta_{\Red{m}},
%\]
%where $\Delta_{p}$ and $\Delta_{m}$ are the predictor
%and corrector terms, respectively. The initial predictor is usually 
%chosen to be the affine-scaling direction although different choices 
%are also possible and, in certain special circumstances such as 
%for example warm-starting, may be justified.

In this context, Mehrotra's predictor--corrector direction 
(\ref{eq:CompositeDirection}) is considered to be a new predictor direction
$\Delta_p$ to which one or more centrality correctors can be applied. 
In the framework of multiple centrality correctors, we look for a 
centrality corrector $\Delta_m$ for which larger
steps are allowed in the composite direction $\Delta = \Delta_p + \Delta_m$.

Assume that a predictor direction $\Delta_p$ is given and the corresponding
feasible stepsizes $\alpha_{P}$ and $\alpha_{D}$ 
in the primal and dual spaces are determined. 
We want to enlarge the stepsizes to 
%
\begin{eqnarray*} 
   \tilde{\alpha}_{P} = \min(\alpha_{P} \! + \! \delta, \,1) 
   \quad \mbox{ and } \quad
   \tilde{\alpha}_{D} = \min(\alpha_{D} \! + \! \delta, \,1), 
\end{eqnarray*}
%
for some fixed aspiration level $\delta \in(0,1)$. We compute a trial point
%
\[
  \tilde{x} = x + \tilde{\alpha}_{P} \Delta_{p} x, \quad 
  \tilde{s} = s + \tilde{\alpha}_{D} \Delta_{p} s,
\]
%
and the corresponding complementarity products 
$\tilde v = \tilde X \tilde S e \in \R^{n}$.
It is worth noting that this trial point is necessarily infeasible: 
this is not a drawback, as the trial point is used exclusively in
determining a target for the centrality corrector.

The products $\tilde v$ are very unlikely to align to the same value $\mu$.
Some of them are significantly smaller than $\mu$, 
including cases of negative components in $\tilde v$, 
and some exceed $\mu$. Instead of trying to correct 
them all to the value of $\mu$, we correct only those that
lie outwith the symmetric neighbourhood.
Namely, we try to move small products 
$(\tilde x_i \tilde s_i \leq \gamma \mu)$ to $\gamma \mu$ and move 
large products $(\tilde x_i \tilde s_i \geq \gamma^{-1} \mu)$ 
to $\gamma^{-1} \mu$, where $\gamma \in (0,1)$;
complementarity products 
which satisfy $\gamma \mu \leq x_i s_i \leq \gamma^{-1} \mu$ are
already reasonably close to their target values, and 
do not need to be changed. 
In other words, we attempt to move the iterate inside the symmetric
neighbourhood of the central path.

The corrector term $\Delta_m$ is computed by solving the usual 
system of equations (\ref{eq:NewtonSystem}) for a special right-hand side
$(0, \,0,\, t)^T$, where the target $t$ is defined as follows:
%
\begin{eqnarray} \label{eq:Target}
  t_i = \left\{
  \begin{array}{ll}
    \gamma \mu - \tilde x_i \tilde s_i
    & \mbox{ if } \;\; \tilde x_i \tilde s_i \leq \gamma \mu  \\
    \gamma^{-1} \mu - \tilde x_i \tilde s_i
    & \mbox{ if } \;\; \tilde x_i \tilde s_i \geq \gamma^{-1} \mu  \\
    0    
    & \mbox{ otherwise.}
  \end{array}
  \right.
\end{eqnarray}

The target point is not on the central path, but in the symmetric
neighbourhood.

One important feature of multiple centrality correctors technique
is that it can be applied recursively on the direction 
$\Delta_p := \Delta_p + \Delta_m$.
The maximum number of centrality correctors allowed is
problem dependent. Such number is determined 
heuristically, trying to balance the cost of additional backsolves to 
the savings in iteration count.
Each corrector computed is accepted as long as the stepsizes increase 
at least by a fraction of the aspiration level $\delta$.

\ignore{
Present the heuristic according to \cite{Gondzio96}.
}

The computational experience presented in \cite{Gondzio96} showed 
that this strategy is effective, as the stepsizes in the primal and 
dual spaces computed for the composite direction are larger than 
those corresponding to the predictor direction. 
This leads to reductions in the number of iterations, which in turn 
translate into bigger CPU time savings the higher the factorization
cost.

Virtually all existing interior point codes implement such technique 
\cite[Appendix B]{ipm:Wright97}.
Their use depends on the quality of the linear algebra codes.

%
% Section
%
\section{Other considerations for implementations}

In this section we mention a few more topics that are relevant to any 
practical implementation of interior point methods.

They concern the choice of the starting point, the termination
criteria. This selection has been made according to the relevance
of this thesis.
Therefore, we will not discuss other topics of extreme importance 
in practical algorithms (presolve techniques, detection of infeasibilities, 
linear algebra issues, use of iterative methods).
We refer the reader to the references in 
\cite{AndersenGondzioMeszarosXu,GondzioTerlaky}.

\ignore{
The initialisation of an interior point method consists of two
logically independent phases: the presolve (find duplicate rows or
columns, fixed variables, redundant constraints, bound tightening),
and the process of finding an initial iterate.
}

%
%
\subsection{Mehrotra's starting point heuristic}
\label{sec:StartingPoint}

The choice of an initial iterate for interior point methods is a
critical one. It challenges both the feasible and infeasible
algorithms, and the solutions proposed in the two contexts are
extremely different.

Turning our attention to infeasible algorithms, the major hurdle
of finding a feasible starting point is removed. 
However, the practical performance is very sensitive to the initial
iterate, so the use of arbitrary points is not recommended.
In particular, two requirements become important: the centrality 
of the point and the magnitude of the corresponding infeasibilities.
\fb{
Expand!
}

Mehrotra \cite{Mehrotra92} introduced a tool to find a starting point 
that attempts to fulfil the above requirements. In this
heuristic, we solve two least squares problems which aim to
satisfy the primal and dual constraints:
\begin{eqnarray*}
  \min_x    \!\! & x^Tx & \;\;\mbox{s.t. }\; Ax = b,      \\
  \min_{y,s}\!\! & s^Ts & \;\;\mbox{s.t. }\; A^Ty + s = c.
\end{eqnarray*}
The corresponding solution $(\tilde x, \tilde y, \tilde s)$ is further 
shifted inside the positive orthant, and the starting point is
\[
(x_0,y_0,s_0) = (\tilde x + \delta_x e,\, \tilde y,\, \tilde s + \delta_s e),
\]
where $\delta_x$ and $\delta_s$ are positive quantities. 
Their values depends on the distance of $\tilde x$ and $\tilde s$
to non-negativity, and an additional correction term to ensure
strict positivity.

The following variation is described in \cite{GondzioTerlaky}:
\begin{eqnarray*} 
  \min_x    \!\! & c^Tx + \rho x^Tx & \;\;\mbox{s.t. }\; Ax = b,      \\
  \min_{y,s}\!\! & b^Ty + \rho s^Ts & \;\;\mbox{s.t. }\; A^Ty + s = c,
\end{eqnarray*}
where the parameter $\rho$ is fixed to a predetermined value, in order
to compensate for the contribution of the primal and dual objectives.

A problem with Mehrotra's strategy is that it is scale dependent,
it is affected by the presence of redundant constraints,
and it does not guarantee to produce a well-centered iterate.

The procedure of finding an appropriate starting point will be discussed
again in Chapter~\ref{ch:Warmstart}, where a specialised technique for
the case of stochastic linear programming problems will be analysed.

%
%
\subsection{Termination criteria}

Contrary to active set algorithms, interior point methods reach a
solution only asymptotically. 
Because of the presence of the barrier term that keeps the iterates
away from the boundary, they can never produce an exact solution.
When working in the context of limited precision of the
floating point representation typical of computers, feasibility and
complementarity can be attained only within a certain level
of accuracy.

For these reasons, some criteria have to be established according
to which a decision in made about the termination of the algorithm.
The common termination criteria used in practice are the
following \cite{GondzioTerlaky}:
\be  \label{eq:TerminationCriteria}
\frac{\| Ax - b \|}{1 + \|x\|_\infty} \le10 ^{-p}, 
\qquad
\frac{\| A^Ty + s - c \|}{1 + \|s\|_\infty} \le10 ^{-p},
\qquad
\frac{| c^Tx - b^Ty |}{1 + | b^Ty |} \le10 ^{-p}.
\ee

The value of $p$ required depends on the specific application.
In the literature, it is common to use the value $p = 8$.

The third condition is usually the most important, as it is
commonly attained only after the feasibility requirements are satisfied.

\fb{
Dependence on scaling.
}

Once a solution has been found within a prescribed 
optimality tolerance, such a point can be projected onto a face 
of the polyhedron in an efficient way.
This can be done thanks to a strongly polynomial algorithm due
to Megiddo \cite{Megiddo91}. This procedure goes under the name
of {\em basis crossover}, 
as given a complementary primal--dual solution it generates a basis 
that is both primal and dual feasible.

However, we should discuss what we mean by ``exact'' solution. In most
cases we do not need the additional precision of being on a vertex (in
this case, integer programming is the notable exception).
Also, in case of multiple solutions, interior point methods terminate
at the analytic center of the optimal face rather than at a vertex; 
but note the arbitrariness of the simplex in the choice of the solution
vertex.
This difference has important consequences on the use of the solution
for sensitivity analysis. 

\fb{
Find the correct citations! Have a look at Jansen's and Yildirim's phds.
}
