%Started on 9th August 2006
%Aug: 11, 22, 23, 24, 25, 28, 29, 30, 31
%Sep:  1,  7,  8
% 2007
%Jan: 15, 16, 17, 22
%Feb:  1,  2, 4

%
% Chapter: Practical implementations of Interior point methods
%
\label{ch:PracticalIpm}

In this chapter we turn our attention to the computational side of
interior point methods. We concentrate on the main strategies which are
at the basis of effective implementations of interior point methods
for linear programming, and present other issues that are peculiar 
to practical algorithms.
These are documented in extensive literature (see, for example, 
\cite{AndersenGondzioMeszarosXu,GondzioTerlaky} and the references therein).


%
% Section
%
\section{A practical algorithm}

Interior point methods require the computation of the Newton 
direction for the associated barrier problem and make a step along 
this direction, thus usually reducing primal and dual infeasibilities 
and complementarity gap; eventually, after a number of iterations, 
they reach optimality. 
Since finding the Newton direction is usually a major computational task, 
the efforts in the theory and practice of IPMs concentrate on reducing 
the number of Newton systems (\ref{eq:NewtonSystem}) to be solved.

In practice, convergence is much faster than stated by the theoretical
results presented in Section~\ref{sec:TheoreticalResults}: 
optimality is usually reached in a number of iterations which is 
proportional to the logarithm of the problem dimension. 

\fb{
Find a citation for that!
}

Practical developments aim to reduce this number even further. 

\hrulefill

Practical algorithms are very different from the ones used for
theoretical purposes, and usually they implement some variation
of infeasible interior point algorithms. In particular they show
differences in the search directions, the evaluation of the stepsize, 
the neighbourhood. 

The use of different stepsizes in the primal and dual spaces is
almost the rule for linear programming (while for quadratic programs
the stepsizes have to be identical). This has the advantage of
speeding up the restoration of feasibility. According to
\cite{GondzioTerlaky}, the use of different stepsizes contributes 
to a reduction in number of iterations of 10\% on the Netlib
set of tests.

This is further complicated by issues of computational efficiency
and numerical stability, which often suggest the use of amended
techniques or heuristic approaches, which make their analysis
extremely difficult.

%
%
\subsection{Correcting techniques}

Two techniques have proved particularly successful in reducing 
the number of iterations within practical algorithms:
Mehrotra's predictor--corrector algorithm \cite{Mehrotra92} 
and multiple centrality correctors \cite{Gondzio96}. These 
techniques have been implemented in most of commercial and academic 
interior point solvers for linear and quadratic programming such 
as BPMPD, Cplex, HOPDM, Mosek, OOPS, OOQP, PCx and Xpress. 
They have also been used with success in semidefinite 
programming with IPMs \cite{Haeberly99}.

Both correcting techniques originate from the observation that 
(when direct methods of linear algebra are used) the computation 
of the Newton direction requires two computationally intensive
operations: the factorization of a sparse symmetric matrix, and
the backsolve which uses the factors just computed. 
However, the cost of computing the factors is usually significantly 
larger than that of backsolving: in some cases the ratio between 
these two computational efforts may even exceed 1000. 

\fb{
Present a small table that shows the cost of factoring and of backsolving
for HOPDM (and PC-x?).
}

Consequently, it is worth adding more (cheap) 
backsolves if this reduces the number of (expensive) factorizations. 
Mehrotra's predictor--corrector technique \cite{Mehrotra92} uses two 
backsolves per factorization; the multiple centrality correctors technique
\cite{Gondzio96} allows recursive corrections: a larger number 
of backsolves per iteration is allowed, leading to a further reduction 
in the number of factorizations. 

\fb{
Since these two methods were developed, there have been a number of 
attempts to investigate their behaviour rigorously and thus provide
further insight. Such objectives are difficult to achieve because 
correctors use heuristics which are successful in practice but hard 
to analyse theoretically. 
Besides, both correcting techniques are applied to long-step and infeasible 
algorithms which have very little in common with the short-step and 
feasible algorithms that display the best known theoretical complexity.
}

These two strategies will be the focus of the next sections.


%
% Section
%
\section{Mehrotra's predictor--corrector algorithm}
\label{sec:MehrotraPC}

Practical implementations usually follow Mehrotra's predictor--corrector 
algorithm \cite{Mehrotra92}. In such a framework, we first generate a 
predictor direction to make progress toward optimality, and then we 
compute a corrector to remedy for some of the error made by the predictor
and move the point closer to the central path.

A number of advantages can be obtained by splitting the computation 
of the Newton direction into two steps, corresponding to solving the linear
system (\ref{eq:NewtonSystem}) independently for the two right-hand 
sides 
\be \label{eq:PredictorRhs}
r_1 =\left[ 
  \begin{array}{c}
    b-Ax \\ c-A^Ty-s \\ -XSe
  \end{array} \right] \quad \mbox{and} \quad
r_2 =\left[ 
  \begin{array}{c}
    0 \\ 0 \\ \mu e
  \end{array} \right].
\ee

Mehrotra \cite{Mehrotra92} introduced two important innovations: 
\begin{itemize}
\item A dynamic evaluation of the centering parameter $\sigma$;
\item A second order correction.
\end{itemize}

First, we can postpone the choice of $\mu$ and base it
on the assessment of the quality of the affine-scaling direction;
second, the error made by the affine-scaling direction may be 
taken into account and
corrected. Mehrotra's predictor--corrector technique \cite{Mehrotra92}
translates these observations into a powerful computational method.

Mehrotra's predictor--corrector algorithm \cite{Mehrotra92,LustigMarstenShanno}
is extremely efficient in practice. Since its introduction, it has 
been the considered the method of choice for practical implementations 
because it is usually very fast and reliable. Moreover, it has a 
convincing interpretation in terms of second order approximations.

%
%
\subsection{Affine-scaling predictor direction}

The predictor direction is obtained by solving the step equations 
for the pure Newton direction (affine scaling direction). This 
direction is strongly optimizing, as it aims to a point for which 
all complementarity products go to zero. 

The {\em affine-scaling predictor direction} 
$\Delta_a w = (\Delta_a x, \Delta_a y, \Delta_a s)$ is obtained by solving 
system (\ref{eq:NewtonSystem}) with right-hand side $r_1$ defined 
in (\ref{eq:PredictorRhs}).
In order to ensure that $(x,s)$ remain positive after moving along the
$\Delta_a w$ direction, we need to employ a linesearch procedure such that
\[
  x + \alpha_P \Delta_a x > 0, \qquad  s + \alpha_D \Delta_a s > 0.
\]

Hence, the maximum feasible stepsizes $\alpha_P$ and $\alpha_D$ achievable
in the predictor direction, in the primal and dual space respectively, 
are computed as:
\be
  \alpha_P =\min \left\{ -\frac{x_i}{\Delta_a x_i} : \Delta_a x_i < 0 \right\},
  \quad\;
  \alpha_D =\min \left\{ -\frac{s_i}{\Delta_a s_i} : \Delta_a s_i < 0 \right\}.
\ee

The affine scaling direction may well point towards the boundary 
of the positive orthant, causing the need for a very small stepsize. 
This is the reason why affine scaling alone is not enough in a 
practical implementation of interior point methods. It has to be 
complemented by other techniques.

The role of the centering term is to remedy this situation by 
affecting the search direction to move closer to the central path, 
therefore allowing a longer stepsize. 
As noted in \cite{TapiaZhangSaltzmanWeiser}, the choice of the 
centering parameter can be crucial both in theory and in practice. 
It is suggested that it is a function of the Newton step. 
This calls for two separate backsolves at each iteration.

Another problem with affine scaling is that it is only a linear 
approximation to the central path. However, the central path is a curve
with many high-degree turns \cite{VavasisYe}, and only close to the 
solution set becomes approximately a straight line \cite{Megiddo}. 
Therefore, affine scaling may be easily distracted by points that 
have small complementarity products but are not optimal. In particular, 
these points may not be feasible (in the feasible algorithm), or 
may produce a much bigger improvement in optimality than what 
attained in feasibility (in the general infeasible algorithm).

The above issues are usually worsened if the current iterate is 
badly centered, 
and therefore only a very small step is acceptable in order to 
maintain positivity or to keep the iterate in the neighbourhood 
of the central path.

%
%
\subsection{Mehrotra's corrector direction}

Mehrotra's algorithm exploits a centrality corrector in order to 
remedy to badly centered points. The purpose of this direction is 
to move closer to the central path, and therefore reduce the spread 
in complementarity products, without aiming for more optimality. 
This is a somewhat conservative direction, which it is hoped will 
provide more room for movement at the next iteration.

One tool introduced by Mehrotra \cite{Mehrotra92} is a dynamic evaluation 
of the centering parameter $\sigma$. It is based on a simple and cleverly 
implemented heuristic that evaluates the quality of the predictor direction
in order to judge the amount of centering term needed.
%
The length of the stepsizes $\alpha_P$ and $\alpha_D$ are used to 
predict the complementarity gap after such a step:
\be \label{eq:PredictedGap}
  g_a = (x + \alpha_P \Delta_a x)^T(s + \alpha_D \Delta_a s).
\ee

The ratio $g_a / x^{T}s \in (0,1)$ measures the quality of the 
predictor direction.
A small ratio indicates a successful reduction of the complementarity 
gap. On the other hand, if the ratio is close to one, then very little 
progress is achievable in direction $\Delta_a$, and a strong recentering 
is recommended.

In \cite{Mehrotra92} the following choice of the new barrier parameter 
is suggested
%
\be \label{eq:Mu}
  \mu = \left( \frac{g_a}{x^{T}s} \right)^{\! 2} \, \frac{g_a}{n}
           = \left( \frac{g_a}{x^{T}s} \right)^{\! 3} \, \frac{x^{T}s}{n},
\ee
%
corresponding to the choice of $\sigma = (g_a / x^Ts)^3$ 
for the centering parameter. 
Other choices of the exponent are possible. Mehrotra \cite{Mehrotra92}
studied the effect of different values $p=1,2,3,4$ for the exponent
on a subset of Netlib problems, and concluded that for $p$ between
2 and 4 there was not much difference.
\fb{
Mehrotra's heuristic was actually more elaborate.
}
Also Lustig et al. \cite{LustigMarstenShanno} commented on the
weak dependence of the computational performance on the choice 
of the exponent.

\ignore{ %%%%%%%%%%%%%%%%%
The centering parameter, more generally could be chosen as
\[
  \sigma = \left( \frac{g_a}{x^Ts} \right)^p\!\!.
\]
}        %%%%%%%%%%%%%%%%%

If affine scaling provides a good improvement, a small $\sigma$ 
is chosen, and therefore very little centering will be used. When, 
on the other hand, affine scaling produces very small stepsizes 
and so very little improvement can be achieved, $\sigma$ will be 
close to one, and so a stronger recentering will occur.

\fb{
Another problem with affine scaling is that it is only a linear 
approximation to the central path. However, the central path is 
a highly nonlinear curve which only close to the solution set 
becomes approximately a straight line \cite{Megiddo}.
}

A further important contribution by Mehrotra consists in the 
use of a second order direction. As said above, the affine-scaling direction 
corresponds to a linear approximation to the the trajectory from 
the current point to the optimal set, where no information about 
higher order terms is taken into account. This linearisation, 
however, produces an error which can be determined analytically.
%
If a full step in the affine-scaling direction is made, then 
the new complementarity products are equal to
%
\begin{eqnarray*}
  (X + \Delta_a X) (S + \Delta_a S) e 
   \;=\; XSe + (S \Delta_a x + X \Delta_a s) + \Delta_a X \Delta_a S e
   \;=\; \Delta_a X \Delta_a S e,
\end{eqnarray*}
%
as the third equation in the Newton system satisfies 
$S \Delta_a x + X \Delta_a s = -XSe.$
%
The term $\Delta_a X \Delta_a S e$ corresponds to the error introduced
by Newton's method in linearising the perturbed complementarity condition.

Ideally we would like the next iterate to be perfectly centered: 
\[
  (X+\Delta X)(S+\Delta S)e=\mu e,
\]
which is equivalent to solving the nonlinear system
\[
  S\Delta x + X\Delta s = -XSe +\mu e - \Delta X\Delta Se.
\]
The linearisation error made by the affine-scaling direction is exactly 
the $\Delta X\Delta Se$ term that is ignored in the right-hand side
of (\ref{eq:NewtonSystem}).

Mehrotra introduced a second-order correction in which the 
linearisation error is taken into account. Therefore, 
Mehrotra's corrector term is obtained by solving the Newton system 
(\ref{eq:NewtonSystem}) with right-hand side
\be \label{eq:MehrotraRhs}
r =\left[ \begin{array}{c}
    0 \\ 0 \\ -\Delta_a X\Delta_a Se + \sigma \mu e
  \end{array} \right],
\ee
for the direction $\Delta_c (x,y,s)$.
Such corrector direction combines the centrality term $\sigma \mu e$
and the second-order term $\Delta_a X\Delta_a Se$.

Once the predictor and corrector terms are computed, they are 
added to produce the composite predictor--corrector direction
\be \label{eq:CompositeDirection}
\Delta w = \Delta_a w+ \Delta_c w.
\ee

The next iterate is given by
\[
w^{k+1} = (x^k,y^k,s^k)
        + (\alpha_P\Delta x^k,\alpha_D\Delta y^k,\alpha_D\Delta s^k)
\]
where $\alpha_P$ and $\alpha_D$ are again chosen to satisfy
\[
x^k+\alpha_P\Delta x^k \ge 0, \quad s^k+\alpha_D\Delta s^k \ge 0.
\]

For reasons of computational efficiency, in the computation of
the corrector, Mehrotra's algorithm exploits 
the same Jacobian matrix used to find the affine-scaling direction: 
hence, the same Cholesky factors are reused.
The cost of a single iteration in the predictor--corrector 
method is only slightly larger than that of the standard 
method because two backsolves per iteration have to be executed, 
one for the predictor and one for the corrector. 

The practical advantage of Mehrotra's predictor--corrector technique
is that it often produces longer stepsizes before violating the 
non-negativity constraints.
%
This usually translates in significant savings in the number of IPM 
iterations: Mehrotra \cite{Mehrotra92} reports on savings of the
order of 35\%-50\% compared to other strategies.
For problems for which the factorisation cost is relevant, this
leads into significant 
CPU time savings \cite{LustigMarstenShanno,Mehrotra92}. Indeed, 
Mehrotra's predictor--corrector technique is advantageous in all 
interior point implementations for linear programming 
which use direct methods to compute 
the Newton direction.

It should be noted that in the computation of the predicted gap, 
it is assumed that a full step in the affine-scaling has been taken. 
Also, the affine-scaling predictor and Mehrotra's corrector direction 
contribute with equal weights to the final search direction. 
This argument will be considered again in Chapter~\ref{ch:Correctors}, 
where we study the use of a weighting strategy for the corrector
directions.

As mentioned above, Mehrotra's way of assessing the value of $\sigma$
and the computation of the second order term is an heuristic procedure: 
thus there are 
no global convergence results or polynomial complexity results. 
There is a local convergence result \cite{TapiaZhangSaltzmanWeiser}, 
according to which Mehrotra's method can be interpreted as a 
perturbed composite Newton method, but this result lies on strong 
assumptions (non degeneracy, strict complementarity).

Tapia et al. \cite{TapiaZhangSaltzmanWeiser} interpreted the Newton step 
produced by Mehrotra's predictor--corrector algorithm as a perturbed
composite Newton method and gave results on the order of convergence. 
They proved that a level-1 composite Newton method, when applied 
to the perturbed Karush-Kuhn-Tucker system, produces the same 
sequence of iterates as Mehrotra's predictor--corrector algorithm. 
While, in general, a level-$m$ composite Newton method has 
a $Q$-convergence rate of $m+2$ \cite{OrtegaRheinboldt},
the same result does not hold 
if the stepsize has to be damped to keep non-negativity of the iterates, 
as is necessary in an interior-point setting. However, under 
the additional assumptions of strict complementarity and nondegeneracy 
of the solution and feasibility of the starting point, Mehrotra's 
predictor--corrector method can be shown to have $Q$-cubic convergence
\cite{TapiaZhangSaltzmanWeiser}.


%
% Section
%
\section{Multiple centrality correctors}
\label{sec:MultipleCC}

Mehrotra's predictor--corrector, as it is implemented in optimization 
solvers \cite{LustigMarstenShanno,Mehrotra92}, is a very aggressive 
technique. It is based on the assumption, rarely satisfied, that a 
{\it full} step in the corrected direction will be achievable.
Moreover, an attempt to correct all complementarity products to the 
same value $\mu$ is also very demanding and occasionally
counterproductive. 
\fb{
Consider how Salahi et al. \cite{SalahiPengTerlaky} modify the centrality
term in the corrector.
}
Besides, practitioners noticed that this technique may sometimes 
behave erratically, especially when used for a predictor direction 
applied from highly infeasible and not well centered points. 
\fb{
Is there a reference for that?
}
Finally, Mehrotra's corrector does not provide CPU time savings 
when used recursively \cite{CarpenterLustigMulveyShanno}.

Trying to provide a remedy to the above considerations, Gondzio 
\cite{Gondzio96} introduced the multiple centrality corrector technique 
as an additional tool to complement those presented by Mehrotra. 
The idea behind this technique is to ``force'' an increase in the 
length of the stepsizes by correcting the centrality of Mehrotra's 
iterate.

These correctors can be described as ``less ambitious'' than Mehrotra's
corrector. Instead of attempting to correct for the whole second-order error,
they concentrate on improving the complementarity pairs which really seem 
to hinder the progress of the algorithm, ie. the complementarity products 
that are far from the average.

\fb{
Introduce the symmetric neighbourhood.

We assume that a long-step path-following algorithm is used, 
and therefore we work with the symmetric neighbourhood 
of the central path
%
\be \label{eq:N8hood}
N_\infty(\gamma) = \{ (x,y,s) : Ax = b,\: A^T y + s = c,\: (x,s) \!>\! 0, \, 
  \gamma \mu \leq x_j s_j \leq \mu / \gamma \;\; \forall j \}, 
\ee
%
where $0 < \gamma < 1$. 
In the authors' experience, such a neighbourhood best describes the desired 
properties of a ``well-centered'' interior point iterate.

This neighbourhood will be analysed in more detail in Chapter~\ref{ch:Correctors}.
}

%Like the previous one, this method also uses a composite direction 
%of the following form 
%\[
%  \Delta = \Delta_p + \Delta_{\Red{m}},
%\]
%where $\Delta_{p}$ and $\Delta_{m}$ are the predictor
%and corrector terms, respectively. The initial predictor is usually 
%chosen to be the affine-scaling direction although different choices 
%are also possible and, in certain special circumstances such as 
%for example warm-starting, may be justified.

In this context, Mehrotra's predictor--corrector direction 
(\ref{eq:CompositeDirection}) is considered to be a new predictor direction
$\Delta_p$ to which one or more centrality correctors can be applied. 
In the framework of multiple centrality correctors, we look for a 
centrality corrector $\Delta_m$ such that larger
steps will be made in the composite direction $\Delta = \Delta_p + \Delta_m$.

Assume that a predictor direction $\Delta_p$ is given and the corresponding
feasible stepsizes $\alpha_{P}$ and $\alpha_{D}$ 
in the primal and dual spaces are determined. 
We want to enlarge the stepsizes to 
%
\begin{eqnarray*} 
   \tilde{\alpha}_{P} = \min(\alpha_{P} \! + \! \delta, \,1) 
   \quad \mbox{ and } \quad
   \tilde{\alpha}_{D} = \min(\alpha_{D} \! + \! \delta, \,1), 
\end{eqnarray*}
%
for some fixed aspiration level $\delta \in(0,1)$. We compute a trial point
%
\[
  \tilde{x} = x + \tilde{\alpha}_{P} \Delta_{p} x, \quad 
  \tilde{s} = s + \tilde{\alpha}_{D} \Delta_{p} s,
\]
%
and the corresponding complementarity products 
$\tilde v = \tilde X \tilde S e \in \R^{n}$.
It is worth noting that this trial point is necessarily infeasible: 
this is not a drawback, as the trial point is used exclusively in
determining a target for the centrality corrector.

The products $\tilde v$ are very unlikely to align to the same value $\mu$.
Some of them are significantly smaller than $\mu$, 
including cases of negative components in $\tilde v$, 
and some exceed $\mu$. Instead of trying to correct 
them all to the value of $\mu$, we correct only the {\it outliers}. 
Namely, we try to move small products 
$(\tilde x_j \tilde s_j \leq \gamma \mu)$ to $\gamma \mu$ and move 
large products $(\tilde x_j \tilde s_j \geq \gamma^{-1} \mu)$ 
to $\gamma^{-1} \mu$, where $\, \gamma \in (0,1)$;
complementarity products 
which satisfy $\gamma \mu \leq x_j s_j \leq \gamma^{-1} \mu$ are
already reasonably close to their target values, and 
do not need to be changed. 

\fb{
In other words, we attempt to move the iterate inside the symmetric
neighbourhood.
}

Therefore, the corrector term $\Delta_m$ is computed by solving the usual 
system of equations (\ref{eq:NewtonSystem}) for a special right-hand side
$(0, \,0,\, t)^T$, where the target $t$ is defined as follows:
%
\begin{eqnarray} \label{eq:Target}
  t_j = \left\{
  \begin{array}{ll}
    \gamma \mu - \tilde x_j \tilde s_j  
    & \mbox{ if } \;\; \tilde x_j \tilde s_j \leq \gamma \mu  \\
    \gamma^{-1} \mu - \tilde x_j \tilde s_j  
    & \mbox{ if } \;\; \tilde x_j \tilde s_j \geq \gamma^{-1} \mu  \\
    0    
    & \mbox{ otherwise.}
  \end{array}
  \right.
\end{eqnarray}

\fb{
The target point is not on the central path, but in the symmetric
neighbourhood.
}

One important feature of multiple centrality correctors is that they
can be successfully applied recursively. 
The number of centrality correctors to be computed is determined 
heuristically, trying to balance the cost of additional backsolves to 
the savings in iteration count.

\fb{
Present the heuristic according to \cite{Gondzio96}.

This technique is applied 
recursively on the direction $\Delta_p := \Delta_p + \Delta_m$.
Indeed, we use it as long as the stepsizes increase 
at least by a fraction of the aspiration level $\delta$.
}

The computational experience presented in \cite{Gondzio96} showed 
that this strategy is effective, as the stepsizes in the primal and 
dual spaces computed for the composite direction are larger than 
those corresponding to the predictor direction. 
This leads to reductions in the number of iterations, which in turn 
translate into bigger CPU time savings the higher the factorization
cost.

Virtually all existing interior point codes implement such technique 
\cite[Appendix B]{ipm:Wright97}.

%
% Section
%
\section{Other issues}

In this section we discuss some remaining topics that were not
detailed above. These are relevant to any practical implementation
of interior point methods.

They concern the choice of the starting point, the termination
criteria. This selection has been made according to the relevance
of this thesis.
Therefore, other topics of extreme importance in practical algorithms
will not be discussed here. We refer the reader to the appropriate
references.

Detection of infeasibilities, linear algebra issues, use of iterative
methods.

\hrulefill

The initialisation of an interior point method consists of two
logically independent phases: the presolve (find duplicate rows or
columns, fixed variables, redundant constraints, bound tightening),
and the process of finding an initial iterate.

%
%
\subsection{Mehrotra's starting point heuristic}
\label{sec:StartingPoint}

The choice of an initial iterate for interior point methods is a
critical one. It challenges both the feasible and infeasible
algorithms, and the solutions proposed in the two contexts are
extremely different.

For the feasible algorithm, a starting iterate needs necessarily 
to be primal and dual feasible.
\fb{
What about centrality?
}

Solving the feasibility problem is an optimization problem in its
own right, which is as difficult as solving the original problem.
One important tool in this respect is the self-dual formulation.
\fb{
Add a citation!
}
The self-dual formulation wraps the optimization problem into one 
of larger dimension, but for which a feasible solution is known 
from the start.

\fb{
The use of a self-dual formulation is not attractive from a
computational viewpoint, particularly because of the need of one 
extra backsolve at each iteration.

Stability issues?
}

However, the homogeneous self-dual formulation has the very
appealing property of being able to detect infeasibility.

Turning our attention to infeasible algorithms, the major hurdle
of finding a feasible starting point is removed. 
However, the practical performance is very sensitive to the initial
iterate, so the use of arbitrary points is not recommended.
In particular, two requirements become important: the centrality 
of the point and the magnitude of the corresponding infeasibilities.
\fb{
Expand!
}

Mehrotra \cite{Mehrotra92} introduced a tool to find a starting point 
that attempts to fulfil the above requirements. In this
heuristic, we solve two least squares problems which aim to
satisfy the primal and dual constraints:
\begin{eqnarray*}
  \min_x    \!\! & x^Tx & \;\;\mbox{s.t. }\; Ax = b,      \\
  \min_{y,s}\!\! & s^Ts & \;\;\mbox{s.t. }\; A^Ty + s = c.
\end{eqnarray*}
The corresponding solution $(\tilde x, \tilde y, \tilde s)$ is further 
shifted inside the positive orthant, and the starting point is
\[
(x_0,y_0,s_0) = (\tilde x + \delta_x e,\, \tilde y,\, \tilde s + \delta_s e),
\]
where $\delta_x$ and $\delta_s$ are positive quantities. 
Their values depends on the distance of $\tilde x$ and $\tilde s$
to non-negativity, and an additional correction term to ensure
strict positivity.

The following variation is described in \cite{GondzioTerlaky}:
\begin{eqnarray*} 
  \min_x    \!\! & c^Tx + \rho x^Tx & \;\;\mbox{s.t. }\; Ax = b,      \\
  \min_{y,s}\!\! & b^Ty + \rho s^Ts & \;\;\mbox{s.t. }\; A^Ty + s = c,
\end{eqnarray*}
where the parameter $\rho$ is fixed to a predetermined value, in order
to compensate for the contribution of the primal and dual objectives.

A problem with Mehrotra's strategy is that it is scale dependent,
it is affected by the presence of redundant constraints,
and it does not guarantee to produce a well-centered iterate.

The procedure of finding an appropriate starting point will be discussed
again in Chapter~\ref{ch:Warmstart}, where a specialised technique for
the case of stochastic linear programming problems will be analysed.

%
%
\subsection{Termination criteria}

Present common termination criteria used in practice.

From \cite{GondzioTerlaky}:
Primal feasibility:
\[
\frac{\| Ax - b \|}{1 + \|x\|_\infty} \le10 ^{-p}
\]
Dual feasibility:
\[
\frac{\| A^Ty + s - c \|}{1 + \|s\|_\infty} \le10 ^{-p}
\]
Duality gap:
\[
\frac{| c^Tx - b^Ty |}{1 + | b^Ty |} \le10 ^{-p}
\]

The value of $p$ required depends on the specific application.
In the literature, it is common to use the value $p = 8$.

The third condition is usually the most important, as once that
is attained, also the 2 before are attained as well.

\fb{
Dependence on scaling.
}

%
%
\subsection{Choice of the step length}

Choice of step length: either use neighbourhoods or strict 
positivity of iterates. The latter works very efficiently 
in practice but does not ensure global convergence.

%
%
\subsection{Choice of penalty parameter}

In all predictor-corrector algorithms there is a crucial decision 
to be made at every iteration, namely the choice of the penalty 
parameter $\mu$ to be used in the correction.

The paper \cite{VillasBoasPerin} tries to answer this question. 
They build a polynomial function of $\mu$ and $\alpha$, and they 
use to determine what the optimal choices of these parameters are, 
under a suitably chosen measure.

By using this strategy they achieve a better iteration count on 
most of the problems in their experiment. This, however, is not 
supported by a corresponding reduction in computational time. 
The reason for this is that the postponing of the choice of $\mu$ 
requires the solution of additional systems. While it's true that 
the most expensive operation is the computation of the Cholesky 
factors, the actual cost of the backsolves is not negligible. 
In the results of this paper, the additional cost of the extra 
backsolves is bigger than the savings obtained by the decrease 
in number of iterations.
